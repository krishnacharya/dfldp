{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fPtWMrHIl5tu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df = adult.data.features \n",
    "y  = adult.data.targets.to_numpy() \n",
    "  \n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SKCsRg7xloiA",
    "outputId": "c2c6ac75-378f-4c31-a9ed-e66f82331ec5"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('adult.data')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "m1oyX0wIm4dn",
    "outputId": "53d6a4d4-60d2-4db3-e519-7cb2aa5cdcc5"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(df['charges'], kde=True)  # kde=True adds the Gaussian-like curve\n",
    "# plt.title('Distribution of Insurance Charges')\n",
    "# plt.xlabel('Charges')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# # Find and mark the median\n",
    "# median_charges = df['charges'].median()\n",
    "# plt.axvline(median_charges, color='red', linestyle='dashed', linewidth=2, label=f'Median: {median_charges:.2f}')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "QeZTeRpXnK4l",
    "outputId": "0a74dabf-9a37-4c41-d6b0-41f31854dad0"
   },
   "outputs": [],
   "source": [
    "# df['charges'] = df['charges'].apply(lambda x: 1 if x >= 10000 else 0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  \n",
       "0          2174             0              40  United-States  \n",
       "1             0             0              13  United-States  \n",
       "2             0             0              40  United-States  \n",
       "3             0             0              40  United-States  \n",
       "4             0             0              40           Cuba  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['<=50K', '<=50K.', '>50K', '>50K.'], dtype=object),\n",
       " array([24720, 12435,  7841,  3846]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(y)\n",
    "for i in range(len(y)):\n",
    "    if y[i] == '<=50K' or y[i] == '<=50K.':\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "        \n",
    "y = np.array(y)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64               NaN  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                NaN  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  income  \n",
       "0      United-States       0  \n",
       "1      United-States       0  \n",
       "2      United-States       0  \n",
       "3      United-States       0  \n",
       "4               Cuba       0  \n",
       "...              ...     ...  \n",
       "48837  United-States       0  \n",
       "48838  United-States       0  \n",
       "48839  United-States       0  \n",
       "48840  United-States       0  \n",
       "48841  United-States       1  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'capital-gain': 'capital gain', 'capital-loss': 'capital loss', 'native-country': 'country','hours-per-week': 'hours per week','marital-status': 'marital'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education-num        0\n",
       "marital              0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital gain         0\n",
       "capital loss         0\n",
       "hours per week       0\n",
       "country            583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isin(['?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will replace the special character to nan and then drop the columns \n",
    "df['country'] = df['country'].replace('?',np.nan)\n",
    "df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "#dropping the NaN rows now \n",
    "df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.crosstab(df.education,df.income).plot(kind='bar')\n",
    "plt.title('Purchase Frequency w.r.t Education')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['workclass', 'education', 'marital', \\\n",
    "            'occupation', 'relationship', 'race', 'sex', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ju78RIFTmF9y"
   },
   "outputs": [],
   "source": [
    "def one_hot(df, cols): # idk if sklearns one-hot encoder is similar\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame\n",
    "    param: cols a list of columns to encode\n",
    "    return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TMeE84WimVq_"
   },
   "outputs": [],
   "source": [
    "def numeric_scaler(df, cols):\n",
    "    '''\n",
    "    df: pandas dataframe\n",
    "    numeric_cols: (array of strings) column names for numeric variables\n",
    "\n",
    "    no return: does inplace operation\n",
    "    '''\n",
    "    df_new = df.copy()\n",
    "    mmscaler = MinMaxScaler()\n",
    "    df_new[cols] = mmscaler.fit_transform(df_new[cols])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital gain</th>\n",
       "      <th>capital loss</th>\n",
       "      <th>hours per week</th>\n",
       "      <th>country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "              marital         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital gain  capital loss  hours per week        country  income  \n",
       "0          2174             0              40  United-States       0  \n",
       "1             0             0              13  United-States       0  \n",
       "2             0             0              40  United-States       0  \n",
       "3             0             0              40  United-States       0  \n",
       "4             0             0              40           Cuba       0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "48836    0\n",
       "48837    0\n",
       "48839    0\n",
       "48840    0\n",
       "48841    1\n",
       "Name: income, Length: 45222, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['income']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qk7vt7Zfl2nV"
   },
   "outputs": [],
   "source": [
    "numeric_all = ['age', 'bmi', 'children', 'charges']\n",
    "# cat_all = ['sex', 'smoker', 'region']\n",
    "# df_medical_mm = numeric_scaler(df, numeric_all) # minmax scaling for all numeric columns, so all elements in [0,1]\n",
    "df_medical_mm_oh = one_hot(df, cat_cols)\n",
    "df_medical_mm_oh.drop(cat_cols, axis = 1, inplace=True) # drop categories that were used to one hot encode\n",
    "df_medical_mm_oh = df_medical_mm_oh * 1.0 # make bool true, false into 1.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital gain</th>\n",
       "      <th>capital loss</th>\n",
       "      <th>hours per week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Portugal</th>\n",
       "      <th>country_Puerto-Rico</th>\n",
       "      <th>country_Scotland</th>\n",
       "      <th>country_South</th>\n",
       "      <th>country_Taiwan</th>\n",
       "      <th>country_Thailand</th>\n",
       "      <th>country_Trinadad&amp;Tobago</th>\n",
       "      <th>country_United-States</th>\n",
       "      <th>country_Vietnam</th>\n",
       "      <th>country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    fnlwgt  education-num  capital gain  capital loss  hours per week  \\\n",
       "0  39.0   77516.0           13.0        2174.0           0.0            40.0   \n",
       "1  50.0   83311.0           13.0           0.0           0.0            13.0   \n",
       "2  38.0  215646.0            9.0           0.0           0.0            40.0   \n",
       "3  53.0  234721.0            7.0           0.0           0.0            40.0   \n",
       "4  28.0  338409.0           13.0           0.0           0.0            40.0   \n",
       "\n",
       "   income  workclass_Federal-gov  workclass_Local-gov  workclass_Private  ...  \\\n",
       "0     0.0                    0.0                  0.0                0.0  ...   \n",
       "1     0.0                    0.0                  0.0                0.0  ...   \n",
       "2     0.0                    0.0                  0.0                1.0  ...   \n",
       "3     0.0                    0.0                  0.0                1.0  ...   \n",
       "4     0.0                    0.0                  0.0                1.0  ...   \n",
       "\n",
       "   country_Portugal  country_Puerto-Rico  country_Scotland  country_South  \\\n",
       "0               0.0                  0.0               0.0            0.0   \n",
       "1               0.0                  0.0               0.0            0.0   \n",
       "2               0.0                  0.0               0.0            0.0   \n",
       "3               0.0                  0.0               0.0            0.0   \n",
       "4               0.0                  0.0               0.0            0.0   \n",
       "\n",
       "   country_Taiwan  country_Thailand  country_Trinadad&Tobago  \\\n",
       "0             0.0               0.0                      0.0   \n",
       "1             0.0               0.0                      0.0   \n",
       "2             0.0               0.0                      0.0   \n",
       "3             0.0               0.0                      0.0   \n",
       "4             0.0               0.0                      0.0   \n",
       "\n",
       "   country_United-States  country_Vietnam  country_Yugoslavia  \n",
       "0                    1.0              0.0                 0.0  \n",
       "1                    1.0              0.0                 0.0  \n",
       "2                    1.0              0.0                 0.0  \n",
       "3                    1.0              0.0                 0.0  \n",
       "4                    0.0              0.0                 0.0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_mm_oh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45222, 106)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_medical_mm_oh\n",
    "X['intercept'] = 1.0\n",
    "X = X.to_numpy() # now (n, d+1) dimensional, log regression in d+1 is affine in d\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.90000e+01, 7.75160e+04, 1.30000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [5.00000e+01, 8.33110e+04, 1.30000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [3.80000e+01, 2.15646e+05, 9.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       ...,\n",
       "       [3.80000e+01, 3.74983e+05, 1.30000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [4.40000e+01, 8.38910e+04, 1.30000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [3.50000e+01, 1.82148e+05, 1.30000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(X):\n",
    "    # Calculate the L2 norm of each column\n",
    "    col_norms = np.linalg.norm(X, ord=2, axis=0)\n",
    "    \n",
    "    # Find scaling factors where norm > 1\n",
    "    scaling_factors = np.maximum(col_norms, 1.0)  # Ensures norms <= 1\n",
    "    \n",
    "    # Scale columns with their respective factors\n",
    "    X_normalized = X / scaling_factors\n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00450039, 0.00167856, 0.00585806, ..., 0.        , 0.        ,\n",
       "        0.00470246],\n",
       "       [0.00576973, 0.00180404, 0.00585806, ..., 0.        , 0.        ,\n",
       "        0.00470246],\n",
       "       [0.004385  , 0.00466967, 0.00405558, ..., 0.        , 0.        ,\n",
       "        0.00470246],\n",
       "       ...,\n",
       "       [0.004385  , 0.00812   , 0.00585806, ..., 0.        , 0.        ,\n",
       "        0.00470246],\n",
       "       [0.00507737, 0.0018166 , 0.00585806, ..., 0.        , 0.        ,\n",
       "        0.00470246],\n",
       "       [0.00403881, 0.00394429, 0.00585806, ..., 0.        , 0.        ,\n",
       "        0.00470246]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize_columns(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X, ord=2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13191593 0.04920198 0.17171205 ... 0.         0.         0.13783895]\n",
      " [0.1989062  0.06219267 0.20195121 ... 0.         0.         0.16211292]\n",
      " [0.13781377 0.14676044 0.12746071 ... 0.         0.         0.14779112]\n",
      " ...\n",
      " [0.16762984 0.31041156 0.22394217 ... 0.         0.         0.17976579]\n",
      " [0.12720586 0.04551226 0.14676503 ... 0.         0.         0.11781314]\n",
      " [0.11151355 0.10890369 0.16174384 ... 0.         0.         0.12983713]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.6546496082341"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.65464960823218"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "48836    0\n",
       "48837    0\n",
       "48839    0\n",
       "48840    0\n",
       "48841    1\n",
       "Name: income, Length: 45222, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.where(y == '<=50K', 0, 1)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38zT1ksynd_u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYSSBvBthirg",
    "outputId": "595b80cd-8675-48a5-d5fb-f3fe758376fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data x, y shapes (40699, 106) (40699,)\n",
      "Test data x, y shapes (4523, 106) (4523,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=43)\n",
    "print(\"Training data x, y shapes\", X_train.shape, y_train.shape)\n",
    "print(\"Test data x, y shapes\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BKgjXpAXvYFI"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# ### Needs a check again\n",
    "\n",
    "# def sample_l2lap(eta:float, d:int) -> np.array:\n",
    "#     \"\"\"\n",
    "#         Returns\n",
    "#           d dimensional noise sampled from `L2 laplace'\n",
    "#           https://math.stackexchange.com/questions/3801271/sampling-from-a-exponentiated-multivariate-distribution-with-l2-norm\n",
    "#     \"\"\"\n",
    "#     R = np.random.gamma(d, scale = 1.0/eta)\n",
    "#     Z = np.random.normal(0, 1, size = d)\n",
    "#     return R  * (Z / np.linalg.norm(Z)) #shape is (d,) one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8zCAMbeC9cU",
    "outputId": "8c9ebab2-72fd-48ec-a92c-e7cf28bff3bb"
   },
   "outputs": [],
   "source": [
    "# dictt = {}\n",
    "# n = X_train.shape[0]\n",
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jCfjVfVJiSHs"
   },
   "outputs": [],
   "source": [
    "# n, input_dim = X_train.shape[0], X_train.shape[1]\n",
    "# output_dim = 1\n",
    "\n",
    "# #logistic regression class\n",
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "#         self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "#         self.sigmoid = torch.nn.Sigmoid()\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "\n",
    "#     #sigmoid transformation of the input\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.linear2(x)\n",
    "# #         x = self.relu(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6D0HM-ATDEQg"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Define the linear regression model\n",
    "# # class LinearRegression(nn.Module):\n",
    "# #     def __init__(self, input_dim):\n",
    "# #         super(LinearRegression, self).__init__()\n",
    "# #         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# def train_model(X_train, y_train, eps_p, epochs=1000):\n",
    "\n",
    "#     # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#     # Initialize the model, loss function, and optimizer\n",
    "# #     model = LogisticRegression(input_dim, output_dim)\n",
    "#     criterion = nn.BCELoss(reduction='sum') # Binary Cross Entropy Loss for binary classification\n",
    "    \n",
    "#     d = X_train.shape[1]\n",
    "#     lr = 1e-4\n",
    "    \n",
    "    \n",
    "#     theta_init = torch.randn((d,1),requires_grad=True)\n",
    "#     optimizer = optim.Adam([theta_init], lr=lr)\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "# #     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     b = np.random.gamma(d, scale=1.0 / eta, size=(theta_init, 1))\n",
    "#     b = torch.Tensor(b.reshape(1, -1))\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in tqdm(range(epochs), desc='Training'):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "\n",
    "#         for batch_X, batch_y in data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "# #             outputs = model(batch_X)\n",
    "# #             theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "            \n",
    "#             y_hat_init = torch.matmul(batch_X, theta_init.float())\n",
    "#             outputs = torch.nn.Sigmoid(y_hat_init)\n",
    "            \n",
    "            \n",
    "\n",
    "#             # Add perturbation\n",
    "#             pert = torch.dot(b.flatten(), theta.flatten())\n",
    "\n",
    "#             # Calculate the loss\n",
    "#             loss = (\n",
    "#                 criterion(outputs, batch_y)\n",
    "#                 + pert\n",
    "#                 + ((Lamb + Delta) * (torch.norm(theta_init, p=2) ** 2))\n",
    "#             )\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#             # Backward and optimize\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Logging training loss\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial, X, y):\n",
    "#     # Define the hyperparameter search space\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "# #     eta = trial.suggest_float('eta', 0.1, 10.0, log=True)\n",
    "#     epochs = trial.suggest_int('epochs', 1, 1000)\n",
    "    \n",
    "#     # Split data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "    \n",
    "#     try:\n",
    "#         # Train the model with suggested hyperparameters\n",
    "#         model = train_model(\n",
    "#             X_train=X_train,\n",
    "#             y_train=y_train,\n",
    "#             eps_p=np.inf,\n",
    "#             epochs=epochs\n",
    "#         )\n",
    "        \n",
    "#         # Update the model's optimizer with suggested learning rate and weight decay\n",
    "#         model.optimizer = torch.optim.Adam(\n",
    "#             model.parameters(),\n",
    "#             lr=learning_rate\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         with torch.no_grad():\n",
    "#             X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "#             val_outputs = model(X_val_tensor)\n",
    "#             val_predictions = (val_outputs >= 0.5).float().numpy()\n",
    "#             accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "#         return accuracy\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         # Return a very low score if training fails\n",
    "#         print(f\"Trial failed with error: {str(e)}\")\n",
    "#         return float('-inf')\n",
    "\n",
    "# def optimize_hyperparameters(X, y, n_trials=100):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "#     # Create a partial function with fixed X and y\n",
    "#     objective_with_data = lambda trial: objective(trial, X, y)\n",
    "    \n",
    "#     # Run the optimization\n",
    "#     study.optimize(objective_with_data, n_trials=n_trials)\n",
    "    \n",
    "#     print(\"Best hyperparameters:\", study.best_params)\n",
    "#     print(\"Best accuracy:\", study.best_value)\n",
    "    \n",
    "#     return study.best_params, study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Lamb = n\n",
    "\n",
    "# best_params, best_trial = optimize_hyperparameters(X_train, y_train, n_trials=100)\n",
    "\n",
    "# # Train final model with best parameters\n",
    "# final_model = train_model(\n",
    "#     X_train=X,\n",
    "#     y_train=y,\n",
    "#     eps_p=np.inf,\n",
    "#     epochs=best_params['epochs']\n",
    "# )\n",
    "# final_model.optimizer = torch.optim.Adam(\n",
    "#     final_model.parameters(),\n",
    "#     lr=best_params['learning_rate']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Define the linear regression model\n",
    "# # class LinearRegression(nn.Module):\n",
    "# #     def __init__(self, input_dim):\n",
    "# #         super(LinearRegression, self).__init__()\n",
    "# #         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# def twostg_train_model(X_train, y_train, eps_p, \\\n",
    "#                        lr, weight_decay, epochs):\n",
    "\n",
    "#     # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#     # Initialize the model, loss function, and optimizer\n",
    "#     model = LogisticRegression(input_dim, output_dim)\n",
    "#     criterion = nn.BCELoss(reduction='sum') # Binary Cross Entropy Loss for binary classification\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "\n",
    "# #     patience = 10\n",
    "# #     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=1e-3, patience=patience, \\\n",
    "# #                                 verbose=True)\n",
    "\n",
    "\n",
    "#     # Training loop\n",
    "    \n",
    "#     #   b, Lamb, Delta = get_noise_vector(eps_dash_p)\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# #     print(\"n = \", X_train.shape[0])\n",
    "\n",
    "#     d = X_train.shape[1]\n",
    "#     b = np.random.gamma(d, scale=1.0/eta, size=(theta_d,1))\n",
    "#     #     sample_l2lap(eta, X_train.shape[1])\n",
    "# #     print(b)\n",
    "#     b = b.reshape(1, -1)    \n",
    "\n",
    "\n",
    "#     for epoch in tqdm(range(epochs), desc='Training'):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_train_tensor)\n",
    "#         theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "# #         theta = theta.reshape(1, -1)\n",
    "#         b = torch.Tensor(b)\n",
    "# #         print(b.shape)\n",
    "#         #       print(theta.shape)\n",
    "#         pert = torch.dot(b.flatten(), theta.flatten())\n",
    "#         #       print(pert)\n",
    "#         # print(pert[0,0])\n",
    "#         # print(pert.shape)\n",
    "#         # if eps_p == 100:\n",
    "#         #   print(pert[0,0])\n",
    "#         # Calculate the loss\n",
    "# #         print(criterion(outputs, y_train_tensor))\n",
    "#         loss = criterion(outputs, y_train_tensor) \\\n",
    "#             + pert \\\n",
    "#             + ((Lamb + Delta) * (torch.norm(theta, p=2)**2))\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (epoch+1) % 20 == 0:\n",
    "# #             print(criterion(outputs, y_train_tensor))\n",
    "# #             print((Lamb + Delta) * (torch.norm(theta, p=2)**2))\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "#         self.sigmoid = torch.nn.Sigmoid()  # Define sigmoid as a class member\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.sigmoid(x)  # Apply sigmoid to bound outputs between 0 and 1\n",
    "#         return x\n",
    "    \n",
    "#     def get_parameters(self):\n",
    "#         return next(self.parameters())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogReg(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(LogReg, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "#         self.sigmoid = torch.nn.Sigmoid()  # Define sigmoid as a class member\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.sigmoid(x)  # Apply sigmoid to bound outputs between 0 and 1\n",
    "#         return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# Define the linear regression model\n",
    "# class LinearRegression(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(LinearRegression, self).__init__()\n",
    "#         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "def dfl_train_model(X_train_tensor, y_train_tensor, theta_init, Lamb, b, c, batch_size=128):\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Create TensorDataset\n",
    "    #   dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    #   # Create DataLoader\n",
    "    #   batch_size = 100\n",
    "    #   train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "#     dfl_model = LogisticRegression(input_dim, output_dim)\n",
    "    \n",
    "    \n",
    "    n, d = X_train_tensor.shape[0], X_train_tensor.shape[1]\n",
    "    epochs = 1000\n",
    "#     b_dfl, Lamb = dfl_get_noise_vector(eps_p)\n",
    "\n",
    "    \n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Initialize model\n",
    "#     model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "    # Extract all model parameters into a single 1D tensor (theta_init)\n",
    "#     theta_init = torch.cat([\n",
    "#         model.linear1.weight.data.flatten(),\n",
    "#         model.linear1.bias.data.flatten(),\n",
    "#         model.linear2.weight.data.flatten(),\n",
    "#         model.linear2.bias.data.flatten()\n",
    "#     ])\n",
    "    \n",
    "\n",
    "#     theta_init = torch.randn((d,1),requires_grad=True)\n",
    "#     print(theta_init.shape)\n",
    "\n",
    "    #   optimizer = optim.Adam(dfl_model.parameters(), lr=0.001)\n",
    "#     optimizer = torch.optim.SGD([theta_init], lr=lr)\n",
    "    optimizer = optim.Adam([theta_init], lr=1e-3)\n",
    "#     optimizer = optim.Adam(dfl_model.parameters(), lr=lr)\n",
    "    \n",
    "#     patience = 1\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=1e-4, patience=patience, \\\n",
    "#                                 verbose=True)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "#     func = torch.nn.Linear(X_train.shape[0], 1, bias=False)\n",
    "#     z = torch.cat([p.flatten() for p in func.parameters()])\n",
    "#     print(z.shape)\n",
    "#     print(z.shape[0])\n",
    "\n",
    "#     print(Q)\n",
    "#     print(torch.linalg.inv(Q))\n",
    "\n",
    "#     nineq = 100\n",
    "#     G_init = Parameter(torch.randn(nineq,128))\n",
    "    \n",
    "    newn = 128\n",
    "    o, p = np.identity(newn), -np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "\n",
    "    k = 1\n",
    "    gamma_star = k * torch.ones(G.shape[0])\n",
    "    \n",
    "    gamma_G = (gamma_star @ G).float()\n",
    "    \n",
    "    print(gamma_G.shape)\n",
    "\n",
    "    \n",
    "#     print(gamma_star.shape)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Training'):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in data_loader:\n",
    "        \n",
    "\n",
    "            # Forward pass\n",
    "            y_hat_init = torch.matmul(batch_X, theta_init.float())\n",
    "    #         print(y_hat_init.shape)\n",
    "    #         torch.matmul(X_train_tensor, theta_init.float())\n",
    "    #         print(X_train_tensor.shape)\n",
    "    #         print(y_hat_init.shape)\n",
    "\n",
    "    #         theta_init = torch.cat([\n",
    "    #             model.linear1.weight.data.flatten(),\n",
    "    #             model.linear1.bias.data.flatten(),\n",
    "    #             model.linear2.weight.data.flatten(),\n",
    "    #             model.linear2.bias.data.flatten()\n",
    "    #         ])\n",
    "    #         print(theta_init)\n",
    "\n",
    "\n",
    "    #     theta_init = torch.ones((d,1),requires_grad=True)\n",
    "    #         print(theta_init.shape)\n",
    "\n",
    "    #         z_star = QPFunction(verbose=-1)(Q, y_hat_init.T, G_init, h, e, e)\n",
    "    #         print(z_star)\n",
    "            \n",
    "#             print(gamma_G)\n",
    "#             print(gamma_G.shape)\n",
    "    #         print(\"gamma_G.shape\", gamma_G.shape)\n",
    "\n",
    "    #         print(y_hat_init)\n",
    "    \n",
    "            Q = c * torch.eye(batch_X.shape[0])\n",
    "\n",
    "            z_star = torch.matmul(torch.linalg.inv(Q), (y_hat_init.flatten() - gamma_G.flatten()))\n",
    "#             print(z_star.shape)\n",
    "    #         print(\"z_star\", z_star)\n",
    "\n",
    "    #         print(y_train_tensor.shape)\n",
    "\n",
    "    #         print(b.T.shape)\n",
    "    #         print(theta_init.shape)\n",
    "\n",
    "            pert = torch.dot(b.flatten(), theta_init.flatten())\n",
    "    #         print(pert)\n",
    "\n",
    "    #         print(torch.dot(y_train_tensor.flatten(), z_star.flatten()))\n",
    "\n",
    "\n",
    "            obj = - torch.dot(batch_y.flatten(), z_star.flatten()) \\\n",
    "                + pert \\\n",
    "                + (Lamb * (torch.norm(theta_init)**2))\n",
    "        \n",
    "            epoch_loss += obj.item()\n",
    "\n",
    "#             print(obj.item())\n",
    "\n",
    "            obj.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], 'f'Loss: {epoch_loss:.4f}')\n",
    "            \n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "# #                 model.eval()\n",
    "#                 train_pred = torch.matmul(X_train_tensor, theta_init.float())\n",
    "# #                 (torch.matmul(X_train_tensor, theta_init.float()) >= 0.5).float()\n",
    "# #                 test_pred = (model(X_test_tensor) >= 0.5).float()\n",
    "#                 train_pred_cls = train_pred.round()\n",
    "#                 train_acc = (train_pred_cls == y_train_tensor).float().mean()\n",
    "# #                 test_acc = (test_pred == y_test_tensor).float().mean()\n",
    "#                 print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "#                       f'Loss: {loss.item():.4f}, '\n",
    "#                       f'Train Acc: {train_acc:.4f}, ')\n",
    "# #                       f'Test Acc: {test_acc:.4f}')\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "    #         grad_theta = torch.autograd.grad(obj, theta_init)\n",
    "    #         print(grad_theta)\n",
    "\n",
    "    #         with torch.no_grad():\n",
    "    # #             clipped_grad = torch.clamp(grad_theta[0], 0, 1)\n",
    "    #             theta_init = theta_init - (lr * grad_theta[0])\n",
    "    #             theta_init = theta_init.detach()  # Detach from computation graph\n",
    "    #             theta_init.requires_grad_()  # Re-enable gradients for next iteration\n",
    "\n",
    "            # Zero gradients for next iteration\n",
    "    #         if theta_init.grad is not None:\n",
    "    #             theta_init.grad.zero_()\n",
    "\n",
    "    return theta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def theta_closed_form(X_train, y_train, Lamb, b, c):\n",
    "    \n",
    "    \n",
    "#     Q = c * torch.eye(X_train.shape[0])\n",
    "    \n",
    "#     if isinstance(y_train, np.ndarray):\n",
    "#         y_train = torch.from_numpy(y_train)\n",
    "    \n",
    "#     term = (b + torch.matmul(X_train.T, torch.matmul(torch.linalg.inv(Q).T, y_train.flatten())))\n",
    "# #     print(term.shape)\n",
    "# #     print(term)\n",
    "#     theta_star = - (1 / (2 * Lamb)) * term\n",
    "#     print(theta_star)\n",
    "#     return theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def theta_closed_form_batched(X_train, y_train, Lamb, b, c, batch_size=128):\n",
    "#     \"\"\"\n",
    "#     Compute theta_star using batched computation\n",
    "    \n",
    "#     Args:\n",
    "#         X_train: Training features\n",
    "#         y_train: Training labels\n",
    "#         Lamb: Lambda parameter\n",
    "#         b: b parameter\n",
    "#         c: c parameter\n",
    "#         batch_size: Size of each batch\n",
    "#     \"\"\"\n",
    "#     # Convert numpy arrays to torch tensors if needed\n",
    "#     if isinstance(y_train, np.ndarray):\n",
    "#         y_train = torch.from_numpy(y_train)\n",
    "    \n",
    "#     n_samples = X_train.shape[0]\n",
    "#     n_batches = (n_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "#     # Initialize accumulator for the summation terms\n",
    "#     accumulated_term = torch.zeros_like(X_train[0])\n",
    "    \n",
    "#     for i in range(n_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, n_samples)\n",
    "        \n",
    "#         # Get current batch\n",
    "#         X_batch = X_train[start_idx:end_idx]\n",
    "#         y_batch = y_train[start_idx:end_idx]\n",
    "        \n",
    "#         # Compute Q matrix for current batch\n",
    "#         Q_batch = c * torch.eye(end_idx - start_idx)\n",
    "        \n",
    "#         # Compute batch term\n",
    "#         batch_term = torch.matmul(X_batch.T, \n",
    "#                                 torch.matmul(torch.linalg.inv(Q_batch).T, \n",
    "#                                            y_batch.flatten()))\n",
    "        \n",
    "#         # Accumulate the term\n",
    "#         accumulated_term += batch_term\n",
    "    \n",
    "#     # Add the b term and compute final theta_star\n",
    "#     final_term = (b + accumulated_term)\n",
    "#     theta_star = (1 / (2 * Lamb)) * final_term\n",
    "    \n",
    "#     print(theta_star)\n",
    "#     return theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "def run_opt(y_hat):\n",
    "\n",
    "#     y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "    newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "    print(newn)\n",
    "\n",
    "    model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "    # Create variables z\n",
    "    z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "#     abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "\n",
    "    # Define the auxiliary variables t\n",
    "#     t = model.addMVar(newn, lb=0, ub=GRB.INFINITY, name=\"t\")\n",
    "\n",
    "#     # Add constraints to enforce t_i >= |z_i|\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(t[i] >= z[i], name=f\"abs_constr1_{i}\")\n",
    "#         model.addConstr(t[i] >= -z[i], name=f\"abs_constr2_{i}\")\n",
    "\n",
    "#     # Add the budget constraint\n",
    "    B = newn // 2  # Replace with your desired budget value\n",
    "#     model.addConstr(t.sum() <= B, name=\"L1_norm_constraint\")\n",
    "\n",
    "    model.addConstr(z.sum() <= B, name=\"sum_constraint\")\n",
    "    \n",
    "    \n",
    "    # h = h.flatten()\n",
    "    # print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "    # print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "    o, p = np.identity(newn), -np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    # h = np.ones(G.shape[0])\n",
    "    # h = h.reshape(-1,1)\n",
    "\n",
    "    h_1, h_2 = np.ones(newn), np.zeros(newn)\n",
    "    h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "    h = np.concatenate((h_1, h_2), axis=0)\n",
    "#     print(h.shape)\n",
    "\n",
    "\n",
    "    print(G.shape)\n",
    "    print(z.shape)\n",
    "    print(h.shape)\n",
    "    \n",
    "    z = z.reshape(-1, 1)\n",
    "    \n",
    "    model.addConstr(G @ z <= h)\n",
    "\n",
    "#     print(y_hat.shape)\n",
    "#     print(z.shape)\n",
    "    y_hat = y_hat.flatten()\n",
    "    \n",
    "    dot_product = gp.quicksum(y_hat[i] * z[i] for i in range(len(y_hat)))\n",
    "    \n",
    "\n",
    "    model.setObjective(dot_product, GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(abs_z[i] <= z[i], name=f\"abs_z_pos_{i}\")\n",
    "#         model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "#     model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "    # model.addConstr(G_1 @ z <= h_1)\n",
    "    # model.addConstr(G_2 @ z <= h_2)\n",
    "    # model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        z_star_test = z.X\n",
    "        \n",
    "    return z_star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:43<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -213322938624.0000\n",
      "1000\n",
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2585225\n",
      "Academic license - for non-commercial use only - expires 2025-11-15\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x82574ce3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e+02, 1e+03]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x2fb6931f\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e+02, 1e+03]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "{'4.0699000000000005': [0.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:47<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -9347769626.0000\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xd330e8d8\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e+02, 9e+02]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x93f721d7\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e+02, 9e+02]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "{'40.699': [0.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:53<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -90464018.1562\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x60fa0ec6\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e+01, 9e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x999d0683\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [3e+01, 9e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "{'406.99': [0.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:47<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -809016.8794\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xda007213\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [3e+00, 8e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xe5d9565f\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [3e+00, 8e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "{'4069.9': [0.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:55<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -27696.5891\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x80fa1cbc\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -0.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -0.000000000e+00\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xdc752bf0\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e-03, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.1950888e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.195088807e+00\n",
      "{'20349.5': [22.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:44<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -7882.3125\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x07bc567d\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.9700000e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.970000000e+02\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xf8ac6a5e\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [3e-04, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2000 rows and 318 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 1 rows, 682 columns, 682 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    5.0629914e+02   4.990000e+02   0.000000e+00      0s\n",
      "       1    2.3047603e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 1 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective  2.304760291e+02\n",
      "{'40699': [211.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:41<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -5474.6407\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xab4545e5\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    5.6400000e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective  5.640000000e+02\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x896c3ed3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e-03, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2000 rows and 1 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 1 rows, 999 columns, 999 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    8.7663758e+02   4.990000e+02   0.000000e+00      0s\n",
      "       1    5.6549953e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 1 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  5.654995333e+02\n",
      "{'203495': [205.0]}\n",
      "Lamb = 4.0699000000000005\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:43<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/1000], Loss: -6773.0175\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xfe2951cb\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    6.3100000e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective  6.310000000e+02\n",
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0xbc4ee34a\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [7e-02, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2000 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 1 rows, 1000 columns, 1000 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    9.6997774e+02   4.990000e+02   0.000000e+00      0s\n",
      "       1    6.4658997e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 1 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  6.465899710e+02\n",
      "{'inf': [205.0]}\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "dqs, accs, cQs_accs, cQs_dQs = {}, {}, {}, {}\n",
    "preds_list, binary_preds_list = [], []\n",
    "# 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5,\n",
    "# 0.1, 0.5, 1, 3, 5, 10, \n",
    "n, d = X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "c = 1\n",
    "Lamb = 0.0001 * n\n",
    "\n",
    "for eps_p in [0.0001*n, 0.001*n, 0.01*n, 0.1*n, 0.5*n, 1*n, 5*n, np.inf]:\n",
    "    y_predicted_list, y_predicted_cls_list = [], [] \n",
    "    \n",
    "    dq_cls_1_runs, dq_1_runs = {}, {}\n",
    "    print(f\"Lamb = {Lamb}\")\n",
    "    dq_list, dq_cls_list = [], []\n",
    "    for _ in range(1):\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        theta_init = torch.randn((d,1), requires_grad=True)\n",
    "        eta = (c * eps_p) / (4 * math.sqrt(n))\n",
    "\n",
    "        b = np.random.gamma(d, scale=1.0/eta, size=(d,1))\n",
    "        #     sample_l2lap(eta, X_train.shape[1])\n",
    "    #     print(b)\n",
    "        b = b.reshape(1, -1) \n",
    "        b = torch.Tensor(b)\n",
    "\n",
    "        theta_priv = dfl_train_model(X_train_tensor, y_train_tensor, theta_init, Lamb, b, c)\n",
    "#             theta_star = theta_closed_form(X_train_tensor, y_train_tensor, Lamb, b, c)\n",
    "#             theta_star_batched = theta_closed_form_batched(X_train_tensor, y_train_tensor, Lamb, b, c)\n",
    "\n",
    "#             print(theta_priv - theta_star)\n",
    "#             print(theta_star)\n",
    "\n",
    "\n",
    "#         print(theta_priv)\n",
    "        # Evaluation\n",
    "\n",
    "        X_test, y_test = X_test[:1000], y_test[:1000]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "\n",
    "\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "            acc_list = []\n",
    "            y_predicted_list, y_predicted_cls_list = [], []\n",
    "\n",
    "            for batch_X, batch_y in test_loader:\n",
    "\n",
    "\n",
    "\n",
    "                y_predicted = torch.matmul(batch_X, theta_priv.flatten().float())\n",
    "#                 print(f\"y_predicted : {y_predicted}\")\n",
    "        #         print(theta.float())\n",
    "        #         print(y_predicted)\n",
    "                y_predicted_cls = y_predicted.round()\n",
    "#                 print(y_test.shape)\n",
    "#                 print(y_test)\n",
    "#                 print(y_predicted @ y_test)\n",
    "                acc = y_predicted_cls.eq(batch_y).sum() / float(batch_y.shape[0])\n",
    "                acc_list.append(acc)\n",
    "#                     print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "                y_predicted_list.append(y_predicted)\n",
    "                y_predicted_cls_list.append(y_predicted_cls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "                # preds_list.append(np.array(predicted_labels))\n",
    "#                 binary_preds_list.append(np.array(y_predicted_cls))\n",
    "#                 preds_list.append(y_predicted)\n",
    "\n",
    "\n",
    "#                     z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "#                     dq = np.dot(z_star.flatten(), y_test.numpy())\n",
    "#                     dq_list.append(dq)\n",
    "\n",
    "#                 print(f'accuracy: {acc.item() / y_test_tensor.shape[0]:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "            y_pred = list(chain(*y_predicted_list))\n",
    "            y_pred = np.array(y_pred)\n",
    "\n",
    "            y_pred_cls = list(chain(*y_predicted_cls_list))\n",
    "            y_pred_cls = np.array(y_pred_cls)\n",
    "\n",
    "            z_star_cls = run_opt(np.array(y_pred_cls))\n",
    "            z_star = run_opt(np.array(y_pred))\n",
    "\n",
    "            dq_cls = np.dot(z_star_cls.flatten(), y_test)\n",
    "            dq = np.dot(z_star.flatten(), y_test)\n",
    "\n",
    "            dq_cls_list.append(dq_cls)\n",
    "            dq_list.append(dq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dq_1_runs[f'{eps_p}'] = dq_list\n",
    "    dq_cls_1_runs[f'{eps_p}'] = dq_cls_list\n",
    "\n",
    "    print(dq_1_runs)\n",
    "\n",
    "\n",
    "#         dqs[f'{eps_p}'] = dq_list\n",
    "#         accs[f'{eps_p}'] = np.mean(acc_list)\n",
    "#         cQs_dQs[f'{c}'] = dqs[f'{eps_p}']\n",
    "#         cQs_accs[f'{c}'] = accs[f'{eps_p}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_3_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "                \n",
    "                \n",
    "                \n",
    "#         X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "#         y_test = np.array(y_test)\n",
    "#         y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "#         test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "#         acc_list = []\n",
    "#         y_predicted_list = []\n",
    "\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "\n",
    "\n",
    "\n",
    "#             y_predicted = torch.matmul(batch_X, theta_priv.flatten().float())\n",
    "# #                 print(f\"y_predicted : {y_predicted}\")\n",
    "#     #         print(theta.float())\n",
    "#     #         print(y_predicted)\n",
    "#             y_predicted_cls = y_predicted.round()\n",
    "# #                 print(y_test.shape)\n",
    "# #                 print(y_test)\n",
    "# #                 print(y_predicted @ y_test)\n",
    "#             acc = y_predicted_cls.eq(batch_y).sum() / float(batch_y.shape[0])\n",
    "#             acc_list.append(acc)\n",
    "# #                     print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "#             y_predicted_list.append(y_predicted)\n",
    "#             y_predicted_cls_list.append(y_predicted_cls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "#             # preds_list.append(np.array(predicted_labels))\n",
    "# #                 binary_preds_list.append(np.array(y_predicted_cls))\n",
    "# #                 preds_list.append(y_predicted)\n",
    "\n",
    "\n",
    "# #                     z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "# #                     dq = np.dot(z_star.flatten(), y_test.numpy())\n",
    "# #                     dq_list.append(dq)\n",
    "\n",
    "#         print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_5_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dq_cls_5_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dq_means, dq_stds = {}, {}\n",
    "for key in dq_5_runs:\n",
    "    dq_means[key] = np.mean(dq_5_runs[key])\n",
    "    dq_stds[key] = np.std(dq_5_runs[key])\n",
    "    \n",
    "dq_means, dq_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "y_pred = list(chain(*y_predicted_cls_list))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_star_test = run_opt(np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z_star_test.flatten() - y_test_tensor.flatten().numpy()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = np.dot(z_star_test.flatten(), y_test)\n",
    "dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH THETA_STAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "#                 y_test = np.array(y_test)\n",
    "#                 y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "                \n",
    "#                 test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "#                 test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=True)\n",
    "                \n",
    "                \n",
    "#                 acc_list = []\n",
    "#                 y_predicted_list = []\n",
    "                \n",
    "#                 for batch_X, batch_y in test_loader:\n",
    "                \n",
    "                \n",
    "                    \n",
    "#                     y_predicted = torch.matmul(batch_X, theta_star.flatten().float())\n",
    "#     #                 print(f\"y_predicted : {y_predicted}\")\n",
    "#             #         print(theta.float())\n",
    "#             #         print(y_predicted)\n",
    "#                     y_predicted_cls = y_predicted.round()\n",
    "#     #                 print(y_test.shape)\n",
    "#     #                 print(y_test)\n",
    "#     #                 print(y_predicted @ y_test)\n",
    "#                     acc = y_predicted_cls.eq(batch_y).sum() / float(batch_y.shape[0])\n",
    "#                     acc_list.append(acc)\n",
    "# #                     print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "#                     y_predicted_list.append(y_predicted)\n",
    "#                     y_predicted_cls_list.append(y_predicted_cls)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "#                     # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "#                     # preds_list.append(np.array(predicted_labels))\n",
    "#     #                 binary_preds_list.append(np.array(y_predicted_cls))\n",
    "#     #                 preds_list.append(y_predicted)\n",
    "\n",
    "\n",
    "# #                     z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "# #                     dq = np.dot(z_star.flatten(), y_test.numpy())\n",
    "# #                     dq_list.append(dq)\n",
    "\n",
    "#                 print(f'accuracy: {acc.item() / y_test_tensor.shape[0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# y_pred = list(chain(*y_predicted_cls_list))\n",
    "# y_pred = np.array(y_pred)\n",
    "# y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_star_test = run_opt(np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(z_star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (z_star_test.flatten() - y_test_tensor.flatten().numpy()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dq = np.dot(z_star_test.flatten(), y_test)\n",
    "# dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_loss(linear_output, X_train, y_train, Lamb):\n",
    "\n",
    "    eps_p = np.inf\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    b = np.random.gamma(d, scale=1.0 / eta, size=(d, 1))\n",
    "    b = torch.Tensor(b.reshape(1, -1))\n",
    "    \n",
    "\n",
    "#     z = torch.dot(w.T,x_train.T)\n",
    "#     y_head = sigmoid(z)\n",
    "    #     print(y_head.shape)\n",
    "    pert = torch.dot(b.flatten(), linear_output.flatten())\n",
    "    \n",
    "    log_likelihood = - y_train * torch.log(sigmoid(linear_output)) - (1 - y_train) * torch.log(1 - sigmoid(linear_output))\n",
    "    l2_reg = (lambda_reg + delta_reg) * torch.sum(model.linear.weight ** 2)\n",
    "    \n",
    "    return log_likelihood + l2_reg + pert_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "Lamb = n\n",
    "\n",
    "model = LogisticRegression(d,1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    linear_output = model(X_train_tensor)\n",
    "\n",
    "\n",
    "    loss = two_stage_loss(linear_output, X_train_tensor, y_train_tensor, Lamb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(x_train,y_train,eps_p=np.inf):\n",
    "    \n",
    "    \n",
    "    n, d = x_train.shape[0], x_train.shape[1]\n",
    "    \n",
    "    w = torch.randn(size=(d,1))\n",
    "    \n",
    "    k = 1\n",
    "    Lamb = k * n\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "    b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "    # forward propagation\n",
    "#     b = torch.FloatTensor(b)\n",
    "    \n",
    "    z = torch.dot(w.T,x_train.T)\n",
    "    y_head = sigmoid(z)\n",
    "#     print(y_head.shape)\n",
    "    pert = np.dot(b.T, w)\n",
    "#     print(pert.shape)\n",
    "    \n",
    "#     print(y_head.shape)\n",
    "\n",
    "    loss_term = np.dot(y_train, np.log(y_head.T)) + np.dot((1-y_train), np.log(1-y_head.T))\n",
    "    print(loss_term)\n",
    "    \n",
    "    loss = loss_term \\\n",
    "            + pert \\\n",
    "            + ((Lamb + Delta) * (np.linalg.norm(w, ord=2)**2))\n",
    "            \n",
    "    cost = (np.sum(loss)) #     /x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    ce_grad = np.dot(x_train.T,((y_head-y_train)).T) / n\n",
    "#     print(ce_grad)\n",
    "    b_grad = b\n",
    "#     print(b_grad.shape)\n",
    "    reg_grad = 2 * (Lamb + Delta) * w\n",
    "\n",
    "#     print(reg_grad)\n",
    "#     print(reg_grad.shape)\n",
    "    total_grads = ce_grad + b_grad + reg_grad # x_train.shape[1]  is for scaling\n",
    "#     derivative_bias = np.sum(y_head-y_train)/x_train.shape[0]    # x_train.shape[1]  is for scaling\n",
    "#     print(grads)\n",
    "    \n",
    "    if np.any(total_grads):  # Check if any non-zero gradients\n",
    "        grad_min = total_grads.min()\n",
    "        grad_max = total_grads.max()\n",
    "\n",
    "        # Avoid division by zero in scaling\n",
    "        if grad_max != grad_min:\n",
    "            scaled_gradient = (total_grads - grad_min) / (grad_max - grad_min)\n",
    "        else:\n",
    "            scaled_gradient = np.zeros_like(total_grads)\n",
    "    else:\n",
    "        scaled_gradient = total_grads\n",
    "    \n",
    "#     total_grads = np.clip(total_grads, 0, 1)\n",
    "#     print(total_grads)\n",
    "    gradients = {\"gradients\": scaled_gradient}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,x_train,y_train,eps_p=np.inf):\n",
    "    \n",
    "    \n",
    "    n, d = x_train.shape[0], x_train.shape[1]\n",
    "    \n",
    "    k = 1\n",
    "    Lamb = k * n\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "    b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train.T)\n",
    "    y_head = sigmoid(z)\n",
    "#     print(y_head.shape)\n",
    "    pert = np.dot(b.T, w)\n",
    "#     print(pert.shape)\n",
    "    \n",
    "#     print(y_head.shape)\n",
    "\n",
    "    loss_term = np.dot(y_train, np.log(y_head.T)) + np.dot((1-y_train), np.log(1-y_head.T))\n",
    "    print(loss_term)\n",
    "    \n",
    "    loss = - loss_term \\\n",
    "            + pert \\\n",
    "            + ((Lamb + Delta) * (np.linalg.norm(w, ord=2)**2))\n",
    "            \n",
    "    cost = (np.sum(loss)) #     /x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    ce_grad = np.dot(x_train.T,((y_head-y_train)).T) / n\n",
    "#     print(ce_grad)\n",
    "    b_grad = b\n",
    "#     print(b_grad.shape)\n",
    "    reg_grad = 2 * (Lamb + Delta) * w\n",
    "\n",
    "#     print(reg_grad)\n",
    "#     print(reg_grad.shape)\n",
    "    total_grads = ce_grad + b_grad + reg_grad # x_train.shape[1]  is for scaling\n",
    "#     derivative_bias = np.sum(y_head-y_train)/x_train.shape[0]    # x_train.shape[1]  is for scaling\n",
    "#     print(grads)\n",
    "    \n",
    "    if np.any(total_grads):  # Check if any non-zero gradients\n",
    "        grad_min = total_grads.min()\n",
    "        grad_max = total_grads.max()\n",
    "\n",
    "        # Avoid division by zero in scaling\n",
    "        if grad_max != grad_min:\n",
    "            scaled_gradient = (total_grads - grad_min) / (grad_max - grad_min)\n",
    "        else:\n",
    "            scaled_gradient = np.zeros_like(total_grads)\n",
    "    else:\n",
    "        scaled_gradient = total_grads\n",
    "    \n",
    "#     total_grads = np.clip(total_grads, 0, 1)\n",
    "#     print(total_grads)\n",
    "    gradients = {\"gradients\": scaled_gradient}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w, x_train, y_train, learning_rate,number_of_iterations):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterations):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - (learning_rate * gradients[\"gradients\"])\n",
    "#         print(gradients['gradients'])\n",
    "#         print(learning_rate * gradients[\"gradients\"])\n",
    "#         b = b - (learning_rate * gradients[\"derivative_bias\"])\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test.T))\n",
    "    Y_prediction = np.zeros((1,x_test.shape[0]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "#     b = 0.0\n",
    "    return w\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[1] \n",
    "    w = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(X_train, y_train, X_test, y_test,learning_rate = 1e-2, num_iterations = 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def theta_closed_form(X_train, y_train):\n",
    "    \n",
    "#     k = 1\n",
    "#     Lamb = k * n\n",
    "    \n",
    "#     n, d = X_train.shape[0], X_train.shape[1]\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "#     b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "#     # forward propagation\n",
    "#     term = np.dot(y_train - , X_train)\n",
    "    \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # Compute sigmoid: 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def twostg_train_model(X_train, y_train, eps_p, lr, epochs, Lamb, batch_size=128):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    d = X_train.shape[1]\n",
    "    output_dim = 1  # Binary classification\n",
    "#     model = LogisticRegression(d, output_dim)\n",
    "    criterion = nn.BCELoss(reduction='sum')  # Binary Cross Entropy Loss\n",
    "    \n",
    "    \n",
    "    theta_init = torch.randn((d,1),requires_grad=True)\n",
    "    optimizer = optim.Adam([theta_init], lr=lr)\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    b = np.random.gamma(d, scale=1.0 / eta, size=(d, 1))\n",
    "    b = torch.Tensor(b.reshape(1, -1))\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc='Training'):\n",
    "#         model.train()\n",
    "        \n",
    "        for (X_batch, y_batch) in data_loader:\n",
    "        \n",
    "    #         for batch_X, batch_y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "    #             outputs = model(batch_X)\n",
    "\n",
    "            y_hat_init = torch.matmul(X_batch, theta_init.float())\n",
    "    # #             print(y_hat_init.shape)\n",
    "            outputs = sigmoid(y_hat_init.flatten())\n",
    "#             outputs = model(X_batch)\n",
    "\n",
    "    #         theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "#             theta = model.get_parameters().view(-1)  # Get current parameters\n",
    "            theta = theta_init\n",
    "\n",
    "    #         print(theta)\n",
    "\n",
    "\n",
    "\n",
    "            # Add perturbation\n",
    "            pert = torch.dot(b.flatten(), theta.flatten())\n",
    "    #         print(pert)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = (\n",
    "                criterion(outputs.flatten(), y_batch.flatten())\n",
    "                + pert\n",
    "                + ((Lamb + Delta) * (torch.norm(theta, p=2) ** 2))\n",
    "            )\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging training loss\n",
    "    #         if (epoch + 1) % 10 == 0:\n",
    "    #             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "    #                 model.eval()\n",
    "                    train_pred = (torch.matmul(X_batch, theta_init.float()) >= 0.5).float()\n",
    "    #                 test_pred = (model(X_test_tensor) >= 0.5).float()\n",
    "                    train_acc = (train_pred == y_batch).float().mean()\n",
    "    #                 test_acc = (test_pred == y_test_tensor).float().mean()\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                          f'Loss: {loss.item():.4f}, '\n",
    "                          f'Train Acc: {train_acc:.4f}, ')\n",
    "    #                       f'Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return theta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fC26Q-Eljl8E"
   },
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "def run_opt(y_hat):\n",
    "\n",
    "#     y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "    newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "    print(newn)\n",
    "\n",
    "    model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "    # Create variables z\n",
    "    z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "#     abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "\n",
    "    # Define the auxiliary variables t\n",
    "#     t = model.addMVar(newn, lb=0, ub=GRB.INFINITY, name=\"t\")\n",
    "\n",
    "#     # Add constraints to enforce t_i >= |z_i|\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(t[i] >= z[i], name=f\"abs_constr1_{i}\")\n",
    "#         model.addConstr(t[i] >= -z[i], name=f\"abs_constr2_{i}\")\n",
    "\n",
    "#     # Add the budget constraint\n",
    "    B = newn // 2  # Replace with your desired budget value\n",
    "#     model.addConstr(t.sum() <= B, name=\"L1_norm_constraint\")\n",
    "\n",
    "    model.addConstr(z.sum() <= B, name=\"sum_constraint\")\n",
    "    \n",
    "    \n",
    "    # h = h.flatten()\n",
    "    # print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "    # print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "    o, p = -np.identity(newn), np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    # h = np.ones(G.shape[0])\n",
    "    # h = h.reshape(-1,1)\n",
    "\n",
    "    h_1, h_2 = np.zeros(newn), np.ones(newn)\n",
    "    h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "    h = np.concatenate((h_1, h_2), axis=0)\n",
    "#     print(h.shape)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model.addConstr(G @ z <= h)\n",
    "\n",
    "#     print(y_hat.shape)\n",
    "#     print(z.shape)\n",
    "\n",
    "    z = z.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    model.setObjective(y_hat.T @ z, GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(abs_z[i] <= z[i], name=f\"abs_z_pos_{i}\")\n",
    "#         model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "#     model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "    # model.addConstr(G_1 @ z <= h_1)\n",
    "    # model.addConstr(G_2 @ z <= h_2)\n",
    "    # model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        z_star_test = z.X\n",
    "        return z_star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150, fit_intercept=False)\n",
    "print(\"test accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HhZYotVrtNo",
    "outputId": "3fa1b047-0296-4b5f-b0c0-3bf19f047271"
   },
   "outputs": [],
   "source": [
    "dqs, accs = {}, {}\n",
    "float_preds_list, preds_list = [], []\n",
    "# 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5,\n",
    "n = X_train.shape[0]\n",
    "k = 1\n",
    "Lamb = k * n\n",
    "# [0.1, 0.3, 0.5, 0.7, 0.9, 1, 2, 3, 5, 10]\n",
    "# 0.1, 0.5, 1, 3, 5, 10, \n",
    "for eps_p in [np.inf]:\n",
    "    dq_list, acc_list = [], []\n",
    "    for _ in range(1):\n",
    "\n",
    "\n",
    "#         model = twostg_train_model(X_train, y_train, eps_p, \\\n",
    "#                                    lr=best_params['learning_rate'], \\\n",
    "#                                    weight_decay=best_params['weight_decay'], \\\n",
    "#                                    epochs=100)\n",
    "\n",
    "        theta_trained = twostg_train_model(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                eps_p=eps_p,\n",
    "                lr=1e-1,\n",
    "                epochs=150,\n",
    "                Lamb=Lamb,\n",
    "            )\n",
    "    \n",
    "        X_test, y_test = X_test[:1000], y_test[:1000]\n",
    "        print(np.unique(y_test))\n",
    "\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "#         g = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "#         print(g)\n",
    "#         print(g.shape)\n",
    "        \n",
    "#         model.eval()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            # predicted = model(X_test_tensor)\n",
    "            # predicted_labels = (predicted > 0.5).float() # Convert probabilities to binary predictions\n",
    "            # accuracy = (predicted_labels == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            # print(f'Eps_p = {eps_p} -- Accuracy on test set: {accuracy:.4f}')\n",
    "\n",
    "            y_predicted = torch.matmul(X_test_tensor, theta_trained.float())\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            print(np.unique(y_predicted_cls))\n",
    "            acc = y_predicted_cls.eq(y_test_tensor).sum() / float(y_test_tensor.shape[0])\n",
    "            acc_list.append(acc)\n",
    "            print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "\n",
    "#         print(y_test_tensor)\n",
    "        # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "        preds_list.append(np.array(y_predicted_cls))\n",
    "        float_preds_list.append(np.array(y_predicted))\n",
    "\n",
    "        z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "        dq = np.dot(z_star.flatten(), y_test)\n",
    "        dq_list.append(dq)\n",
    "        \n",
    "        \n",
    "    dqs[f'{eps_p}'] = np.mean(dq_list)\n",
    "    accs[f'{eps_p}'] = np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(dq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CmPeZ9cr-EQ",
    "outputId": "0e830129-9238-4232-dd23-f8f4c6bc2e02"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, preds_list[5])\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, preds_list[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-wQL9nJo5ji",
    "outputId": "c15e4eba-11cb-4480-9f03-ae339408b9db"
   },
   "outputs": [],
   "source": [
    "test_n = X_test.shape[0]\n",
    "test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJCKDEK9tX_x",
    "outputId": "6192f221-72b1-4f38-f126-d65026337f3b"
   },
   "outputs": [],
   "source": [
    "# o, p = np.identity(test_n), -np.identity(test_n)\n",
    "\n",
    "# G = np.concatenate((o, p), axis=0)\n",
    "# G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_1 = np.identity(test_n)\n",
    "G_2 = -np.identity(test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qk3GicOEu8HM",
    "outputId": "93701894-e5d2-4ce7-989a-abb8dbe6f4a4"
   },
   "outputs": [],
   "source": [
    "G_1.shape, G_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o, p = np.ones(test_n), np.ones(test_n)\n",
    "\n",
    "# h = np.concatenate((o, p), axis=0)\n",
    "# h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK4pYh-Rt9kf",
    "outputId": "320715bf-b336-48d1-ecb8-d383079ddbda"
   },
   "outputs": [],
   "source": [
    "# h_1, h_2 = np.ones(test_n), np.ones(test_n)\n",
    "# h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "# h_1.shape, h_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "y_hat = float_preds_list[0]  # Predicted values (1D array or list)\n",
    "newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "\n",
    "model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "# Create variables z\n",
    "z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "z = z.reshape(-1, 1)\n",
    "\n",
    "print(y_hat.shape)\n",
    "print(z.shape)\n",
    "\n",
    "model.setObjective(y_hat.T @ z, GRB.MAXIMIZE)\n",
    "\n",
    "# h = h.flatten()\n",
    "# print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "# print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "o, p = np.identity(newn), -np.identity(newn)\n",
    "G = np.concatenate((o, p), axis=0)\n",
    "# h = np.ones(G.shape[0])\n",
    "# h = h.reshape(-1,1)\n",
    "\n",
    "h_1, h_2 = np.ones(test_n), np.zeros(test_n)\n",
    "h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "h = np.concatenate((h_1, h_2), axis=0)\n",
    "print(h.shape)\n",
    "\n",
    "B = newn // 2\n",
    "\n",
    "model.addConstr(G @ z <= h)\n",
    "\n",
    "\n",
    "for i in range(newn):\n",
    "    model.addConstr(abs_z[i] >= z[i], name=f\"abs_z_pos_{i}\")\n",
    "    model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "# model.addConstr(G_1 @ z <= h_1)\n",
    "# model.addConstr(G_2 @ z <= h_2)\n",
    "# model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "model.optimize()\n",
    "\n",
    "# Check if the model has a feasible solution\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Extract the optimal solution for z\n",
    "    z_star_test = z.X\n",
    "    print(\"Optimal solution z_star:\", z_star_test)\n",
    "else:\n",
    "    print(\"No optimal solution found. Status:\", model.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = np.dot(z_star_test.flatten(), y_test)\n",
    "dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(z_star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test) # no. of label=\"1\" in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(abs(z_star.flatten() - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "# Assuming G, h, preds_list, and y_test are already defined.\n",
    "y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "\n",
    "# Convert y_hat to a NumPy array for consistency and then to a list\n",
    "if isinstance(y_hat, np.ndarray):\n",
    "    y_hat = y_hat.astype(float).tolist()  # Ensure all elements are floats\n",
    "\n",
    "# Create a Gurobi model\n",
    "model = gp.Model(\"robust_optimization\")\n",
    "\n",
    "# Create variables z with no bounds (default is unbounded)\n",
    "z = model.addVars(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")\n",
    "\n",
    "# Set the objective function: Minimize y_hat^T * z\n",
    "objective = gp.quicksum(y_hat[i] * z[i] for i in range(newn))  # y_hat[i] is numeric, z[i] is a Gurobi variable\n",
    "model.setObjective(objective, GRB.MINIMIZE)\n",
    "\n",
    "# Debug: Print shapes of G and h\n",
    "print(\"G shape:\", G.shape)\n",
    "print(\"z dimensions:\", len(z))\n",
    "print(\"h shape:\", h.shape if hasattr(h, 'shape') else len(h))\n",
    "\n",
    "# Ensure h is a 1D array with the correct length\n",
    "if isinstance(h, np.ndarray):\n",
    "    if len(h.shape) == 2 and h.shape[1] == 1:\n",
    "        h = h.flatten()  # Convert h from (n, 1) to (n,)\n",
    "    h = h.tolist()  # Convert h to a Python list for compatibility\n",
    "\n",
    "# Ensure h's length matches the number of rows in G\n",
    "if len(h) != G.shape[0]:\n",
    "    raise ValueError(f\"The length of h ({len(h)}) does not match the number of rows in G ({G.shape[0]}).\")\n",
    "\n",
    "# Add inequality constraints: Gz ≤ h\n",
    "for i in range(G.shape[0]):\n",
    "    model.addConstr(\n",
    "        gp.quicksum(G[i, j] * z[j] for j in range(G.shape[1])) <= h[i],\n",
    "        name=f\"ineq_constr_{i}\"\n",
    "    )\n",
    "\n",
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# Check if the model has a feasible solution\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Extract the optimal solution for z\n",
    "    z_star = np.array([z[i].X for i in range(newn)])  # Convert to a NumPy array\n",
    "    print(\"Optimal solution z_star:\", z_star)\n",
    "\n",
    "    # Optional: Compute and append result to TWOSTG_SOLQ_LIST\n",
    "    TWOSTG_SOLQ_LIST = []\n",
    "    TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))  # Dot product with y_test\n",
    "    print(\"TWOSTG_SOLQ_LIST:\", TWOSTG_SOLQ_LIST)\n",
    "else:\n",
    "    print(\"No optimal solution found. Status:\", model.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-w37BV6pS1E"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define Q as an identity matrix\n",
    "# Q = np.eye(n)\n",
    "\n",
    "TWOSTG_SOLQ_LIST = []\n",
    "\n",
    "y_hat = preds_list[0]\n",
    "\n",
    "# Number of variables : batch_size\n",
    "\n",
    "# Create a variable for z\n",
    "newn = y_hat.shape[0]\n",
    "print(newn)\n",
    "z = cp.Variable(newn)\n",
    "\n",
    "# Set the objective function\n",
    "# objective = (0.5 * cp.quad_form(z, Q)) + (y_hat.T @ z)\n",
    "objective = (y_hat.T @ z)\n",
    "\n",
    "# Add inequality constraints -1 <= z <= 1\n",
    "# o, p = np.identity(newn), -np.identity(newn)\n",
    "# G = np.concatenate((o, p), axis=0)\n",
    "# h = np.ones(newn)\n",
    "# h = h.reshape(-1,1)\n",
    "\n",
    "print(h.shape)\n",
    "constraints = [G @ z <= h]\n",
    "\n",
    "problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "problem.solve()\n",
    "\n",
    "z_star = z.value\n",
    "\n",
    "print(\"Optimal solution z_star:\", z_star)\n",
    "print(z_star.shape)\n",
    "\n",
    "TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZuwUTJSsYvo"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define Q as an identity matrix\n",
    "# Q = np.eye(n)\n",
    "\n",
    "TWOSTG_SOLQ_LIST = []\n",
    "\n",
    "batch_size = 100\n",
    "B = h.shape[0] // 100\n",
    "\n",
    "for j in range(B):\n",
    "\n",
    "  for i in range(len(preds_list)):\n",
    "    y_hat = preds_list[i]\n",
    "\n",
    "    # Number of variables : batch_size\n",
    "\n",
    "    # Create a variable for z\n",
    "    newn = y_hat.shape[0]\n",
    "    print(newn)\n",
    "    z = cp.Variable(newn)\n",
    "\n",
    "    # Set the objective function\n",
    "    # objective = (0.5 * cp.quad_form(z, Q)) + (y_hat.T @ z)\n",
    "    objective = (y_hat.T @ z)\n",
    "\n",
    "    # Add inequality constraints -1 <= z <= 1\n",
    "    o, p = np.identity(newn), -np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    h = np.ones(newn)\n",
    "    h = h.reshape(-1,1)\n",
    "    print(h.shape)\n",
    "    constraints = [G @ z <= h]\n",
    "\n",
    "    problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    z_star = z.value\n",
    "\n",
    "    print(\"Optimal solution z_star:\", z_star)\n",
    "    print(z_star.shape)\n",
    "\n",
    "    TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ZcgQDbwkvGx6",
    "outputId": "99828f11-856f-4700-f799-ff9eae810e00"
   },
   "outputs": [],
   "source": [
    "# # z_star = z_star.reshape(-1, 1)\n",
    "\n",
    "# # Decision quality\n",
    "np.dot(TWOSTG_SOLQ_LIST[5].reshape(-1,1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.gamma((12,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
