{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "fPtWMrHIl5tu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df = adult.data.features \n",
    "y  = adult.data.targets.to_numpy() \n",
    "  \n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SKCsRg7xloiA",
    "outputId": "c2c6ac75-378f-4c31-a9ed-e66f82331ec5"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('adult.data')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "m1oyX0wIm4dn",
    "outputId": "53d6a4d4-60d2-4db3-e519-7cb2aa5cdcc5"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(df['charges'], kde=True)  # kde=True adds the Gaussian-like curve\n",
    "# plt.title('Distribution of Insurance Charges')\n",
    "# plt.xlabel('Charges')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# # Find and mark the median\n",
    "# median_charges = df['charges'].median()\n",
    "# plt.axvline(median_charges, color='red', linestyle='dashed', linewidth=2, label=f'Median: {median_charges:.2f}')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "QeZTeRpXnK4l",
    "outputId": "0a74dabf-9a37-4c41-d6b0-41f31854dad0"
   },
   "outputs": [],
   "source": [
    "# df['charges'] = df['charges'].apply(lambda x: 1 if x >= 10000 else 0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  \n",
       "0          2174             0              40  United-States  \n",
       "1             0             0              13  United-States  \n",
       "2             0             0              40  United-States  \n",
       "3             0             0              40  United-States  \n",
       "4             0             0              40           Cuba  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['<=50K', '<=50K.', '>50K', '>50K.'], dtype=object),\n",
       " array([24720, 12435,  7841,  3846]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(y)\n",
    "for i in range(len(y)):\n",
    "    if y[i] == '<=50K' or y[i] == '<=50K.':\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "        \n",
    "y = np.array(y)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64               NaN  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                NaN  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  income  \n",
       "0      United-States       0  \n",
       "1      United-States       0  \n",
       "2      United-States       0  \n",
       "3      United-States       0  \n",
       "4               Cuba       0  \n",
       "...              ...     ...  \n",
       "48837  United-States       0  \n",
       "48838  United-States       0  \n",
       "48839  United-States       0  \n",
       "48840  United-States       0  \n",
       "48841  United-States       1  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'capital-gain': 'capital gain', 'capital-loss': 'capital loss', 'native-country': 'country','hours-per-week': 'hours per week','marital-status': 'marital'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education-num        0\n",
       "marital              0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital gain         0\n",
       "capital loss         0\n",
       "hours per week       0\n",
       "country            583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isin(['?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will replace the special character to nan and then drop the columns \n",
    "df['country'] = df['country'].replace('?',np.nan)\n",
    "df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "#dropping the NaN rows now \n",
    "df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.crosstab(df.education,df.income).plot(kind='bar')\n",
    "plt.title('Purchase Frequency w.r.t Education')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['workclass', 'education', 'marital', \\\n",
    "            'occupation', 'relationship', 'race', 'sex', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ju78RIFTmF9y"
   },
   "outputs": [],
   "source": [
    "def one_hot(df, cols): # idk if sklearns one-hot encoder is similar\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame\n",
    "    param: cols a list of columns to encode\n",
    "    return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "TMeE84WimVq_"
   },
   "outputs": [],
   "source": [
    "def numeric_scaler(df, cols):\n",
    "    '''\n",
    "    df: pandas dataframe\n",
    "    numeric_cols: (array of strings) column names for numeric variables\n",
    "\n",
    "    no return: does inplace operation\n",
    "    '''\n",
    "    df_new = df.copy()\n",
    "    mmscaler = MinMaxScaler()\n",
    "    df_new[cols] = mmscaler.fit_transform(df_new[cols])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital gain</th>\n",
       "      <th>capital loss</th>\n",
       "      <th>hours per week</th>\n",
       "      <th>country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "              marital         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital gain  capital loss  hours per week        country  income  \n",
       "0          2174             0              40  United-States       0  \n",
       "1             0             0              13  United-States       0  \n",
       "2             0             0              40  United-States       0  \n",
       "3             0             0              40  United-States       0  \n",
       "4             0             0              40           Cuba       0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "48836    0\n",
       "48837    0\n",
       "48839    0\n",
       "48840    0\n",
       "48841    1\n",
       "Name: income, Length: 45222, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['income']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "qk7vt7Zfl2nV"
   },
   "outputs": [],
   "source": [
    "numeric_all = ['age', 'bmi', 'children', 'charges']\n",
    "# cat_all = ['sex', 'smoker', 'region']\n",
    "# df_medical_mm = numeric_scaler(df, numeric_all) # minmax scaling for all numeric columns, so all elements in [0,1]\n",
    "df_medical_mm_oh = one_hot(df, cat_cols)\n",
    "df_medical_mm_oh.drop(cat_cols, axis = 1, inplace=True) # drop categories that were used to one hot encode\n",
    "df_medical_mm_oh = df_medical_mm_oh * 1.0 # make bool true, false into 1.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital gain</th>\n",
       "      <th>capital loss</th>\n",
       "      <th>hours per week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Portugal</th>\n",
       "      <th>country_Puerto-Rico</th>\n",
       "      <th>country_Scotland</th>\n",
       "      <th>country_South</th>\n",
       "      <th>country_Taiwan</th>\n",
       "      <th>country_Thailand</th>\n",
       "      <th>country_Trinadad&amp;Tobago</th>\n",
       "      <th>country_United-States</th>\n",
       "      <th>country_Vietnam</th>\n",
       "      <th>country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    fnlwgt  education-num  capital gain  capital loss  hours per week  \\\n",
       "0  39.0   77516.0           13.0        2174.0           0.0            40.0   \n",
       "1  50.0   83311.0           13.0           0.0           0.0            13.0   \n",
       "2  38.0  215646.0            9.0           0.0           0.0            40.0   \n",
       "3  53.0  234721.0            7.0           0.0           0.0            40.0   \n",
       "4  28.0  338409.0           13.0           0.0           0.0            40.0   \n",
       "\n",
       "   income  workclass_Federal-gov  workclass_Local-gov  workclass_Private  ...  \\\n",
       "0     0.0                    0.0                  0.0                0.0  ...   \n",
       "1     0.0                    0.0                  0.0                0.0  ...   \n",
       "2     0.0                    0.0                  0.0                1.0  ...   \n",
       "3     0.0                    0.0                  0.0                1.0  ...   \n",
       "4     0.0                    0.0                  0.0                1.0  ...   \n",
       "\n",
       "   country_Portugal  country_Puerto-Rico  country_Scotland  country_South  \\\n",
       "0               0.0                  0.0               0.0            0.0   \n",
       "1               0.0                  0.0               0.0            0.0   \n",
       "2               0.0                  0.0               0.0            0.0   \n",
       "3               0.0                  0.0               0.0            0.0   \n",
       "4               0.0                  0.0               0.0            0.0   \n",
       "\n",
       "   country_Taiwan  country_Thailand  country_Trinadad&Tobago  \\\n",
       "0             0.0               0.0                      0.0   \n",
       "1             0.0               0.0                      0.0   \n",
       "2             0.0               0.0                      0.0   \n",
       "3             0.0               0.0                      0.0   \n",
       "4             0.0               0.0                      0.0   \n",
       "\n",
       "   country_United-States  country_Vietnam  country_Yugoslavia  \n",
       "0                    1.0              0.0                 0.0  \n",
       "1                    1.0              0.0                 0.0  \n",
       "2                    1.0              0.0                 0.0  \n",
       "3                    1.0              0.0                 0.0  \n",
       "4                    0.0              0.0                 0.0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_mm_oh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45222, 105)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_medical_mm_oh\n",
    "# X['intercept'] = 1.0\n",
    "X = X.to_numpy() # now (n, d+1) dimensional, log regression in d+1 is affine in d\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.90000e+01, 7.75160e+04, 1.30000e+01, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [5.00000e+01, 8.33110e+04, 1.30000e+01, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [3.80000e+01, 2.15646e+05, 9.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       ...,\n",
       "       [3.80000e+01, 3.74983e+05, 1.30000e+01, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [4.40000e+01, 8.38910e+04, 1.30000e+01, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [3.50000e+01, 1.82148e+05, 1.30000e+01, ..., 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(X):\n",
    "    # Calculate the L2 norm of each column\n",
    "    col_norms = np.linalg.norm(X, ord=2, axis=0)\n",
    "    \n",
    "    # Find scaling factors where norm > 1\n",
    "    scaling_factors = np.maximum(col_norms, 1.0)  # Ensures norms <= 1\n",
    "    \n",
    "    # Scale columns with their respective factors\n",
    "    X_normalized = X / scaling_factors\n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00450039, 0.00167856, 0.00585806, ..., 0.00492115, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00576973, 0.00180404, 0.00585806, ..., 0.00492115, 0.        ,\n",
       "        0.        ],\n",
       "       [0.004385  , 0.00466967, 0.00405558, ..., 0.00492115, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.004385  , 0.00812   , 0.00585806, ..., 0.00492115, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00507737, 0.0018166 , 0.00585806, ..., 0.00492115, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00403881, 0.00394429, 0.00585806, ..., 0.00492115, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize_columns(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X, ord=2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13318725 0.04967615 0.1733669  ... 0.14563953 0.         0.        ]\n",
      " [0.20157255 0.06302637 0.20465839 ... 0.17192643 0.         0.        ]\n",
      " [0.13934396 0.14838997 0.12887595 ... 0.15638165 0.         0.        ]\n",
      " ...\n",
      " [0.17040585 0.31555208 0.22765073 ... 0.1912415  0.         0.        ]\n",
      " [0.12809796 0.04583144 0.1477943  ... 0.12415687 0.         0.        ]\n",
      " [0.11246553 0.10983339 0.16312463 ... 0.13703536 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.65464960823303"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.65464960823218"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "48836    0\n",
       "48837    0\n",
       "48839    0\n",
       "48840    0\n",
       "48841    1\n",
       "Name: income, Length: 45222, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.where(y == '<=50K', 0, 1)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38zT1ksynd_u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYSSBvBthirg",
    "outputId": "595b80cd-8675-48a5-d5fb-f3fe758376fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data x, y shapes (40699, 105) (40699,)\n",
      "Test data x, y shapes (4523, 105) (4523,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=43)\n",
    "print(\"Training data x, y shapes\", X_train.shape, y_train.shape)\n",
    "print(\"Test data x, y shapes\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "BKgjXpAXvYFI"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# ### Needs a check again\n",
    "\n",
    "# def sample_l2lap(eta:float, d:int) -> np.array:\n",
    "#     \"\"\"\n",
    "#         Returns\n",
    "#           d dimensional noise sampled from `L2 laplace'\n",
    "#           https://math.stackexchange.com/questions/3801271/sampling-from-a-exponentiated-multivariate-distribution-with-l2-norm\n",
    "#     \"\"\"\n",
    "#     R = np.random.gamma(d, scale = 1.0/eta)\n",
    "#     Z = np.random.normal(0, 1, size = d)\n",
    "#     return R  * (Z / np.linalg.norm(Z)) #shape is (d,) one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8zCAMbeC9cU",
    "outputId": "8c9ebab2-72fd-48ec-a92c-e7cf28bff3bb"
   },
   "outputs": [],
   "source": [
    "# dictt = {}\n",
    "# n = X_train.shape[0]\n",
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "jCfjVfVJiSHs"
   },
   "outputs": [],
   "source": [
    "n, input_dim = X_train.shape[0], X_train.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "#logistic regression class\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    #sigmoid transformation of the input\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "#         x = self.relu(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "6D0HM-ATDEQg"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Define the linear regression model\n",
    "# # class LinearRegression(nn.Module):\n",
    "# #     def __init__(self, input_dim):\n",
    "# #         super(LinearRegression, self).__init__()\n",
    "# #         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# def train_model(X_train, y_train, eps_p, epochs=1000):\n",
    "\n",
    "#     # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#     # Initialize the model, loss function, and optimizer\n",
    "# #     model = LogisticRegression(input_dim, output_dim)\n",
    "#     criterion = nn.BCELoss(reduction='sum') # Binary Cross Entropy Loss for binary classification\n",
    "    \n",
    "#     d = X_train.shape[1]\n",
    "#     lr = 1e-4\n",
    "    \n",
    "    \n",
    "#     theta_init = torch.randn((d,1),requires_grad=True)\n",
    "#     optimizer = optim.Adam([theta_init], lr=lr)\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "# #     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     b = np.random.gamma(d, scale=1.0 / eta, size=(theta_init, 1))\n",
    "#     b = torch.Tensor(b.reshape(1, -1))\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in tqdm(range(epochs), desc='Training'):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "\n",
    "#         for batch_X, batch_y in data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "# #             outputs = model(batch_X)\n",
    "# #             theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "            \n",
    "#             y_hat_init = torch.matmul(batch_X, theta_init.float())\n",
    "#             outputs = torch.nn.Sigmoid(y_hat_init)\n",
    "            \n",
    "            \n",
    "\n",
    "#             # Add perturbation\n",
    "#             pert = torch.dot(b.flatten(), theta.flatten())\n",
    "\n",
    "#             # Calculate the loss\n",
    "#             loss = (\n",
    "#                 criterion(outputs, batch_y)\n",
    "#                 + pert\n",
    "#                 + ((Lamb + Delta) * (torch.norm(theta_init, p=2) ** 2))\n",
    "#             )\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#             # Backward and optimize\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Logging training loss\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial, X, y):\n",
    "#     # Define the hyperparameter search space\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "# #     eta = trial.suggest_float('eta', 0.1, 10.0, log=True)\n",
    "#     epochs = trial.suggest_int('epochs', 1, 1000)\n",
    "    \n",
    "#     # Split data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "    \n",
    "#     try:\n",
    "#         # Train the model with suggested hyperparameters\n",
    "#         model = train_model(\n",
    "#             X_train=X_train,\n",
    "#             y_train=y_train,\n",
    "#             eps_p=np.inf,\n",
    "#             epochs=epochs\n",
    "#         )\n",
    "        \n",
    "#         # Update the model's optimizer with suggested learning rate and weight decay\n",
    "#         model.optimizer = torch.optim.Adam(\n",
    "#             model.parameters(),\n",
    "#             lr=learning_rate\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         with torch.no_grad():\n",
    "#             X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "#             val_outputs = model(X_val_tensor)\n",
    "#             val_predictions = (val_outputs >= 0.5).float().numpy()\n",
    "#             accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "#         return accuracy\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         # Return a very low score if training fails\n",
    "#         print(f\"Trial failed with error: {str(e)}\")\n",
    "#         return float('-inf')\n",
    "\n",
    "# def optimize_hyperparameters(X, y, n_trials=100):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "#     # Create a partial function with fixed X and y\n",
    "#     objective_with_data = lambda trial: objective(trial, X, y)\n",
    "    \n",
    "#     # Run the optimization\n",
    "#     study.optimize(objective_with_data, n_trials=n_trials)\n",
    "    \n",
    "#     print(\"Best hyperparameters:\", study.best_params)\n",
    "#     print(\"Best accuracy:\", study.best_value)\n",
    "    \n",
    "#     return study.best_params, study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Lamb = n\n",
    "\n",
    "# best_params, best_trial = optimize_hyperparameters(X_train, y_train, n_trials=100)\n",
    "\n",
    "# # Train final model with best parameters\n",
    "# final_model = train_model(\n",
    "#     X_train=X,\n",
    "#     y_train=y,\n",
    "#     eps_p=np.inf,\n",
    "#     epochs=best_params['epochs']\n",
    "# )\n",
    "# final_model.optimizer = torch.optim.Adam(\n",
    "#     final_model.parameters(),\n",
    "#     lr=best_params['learning_rate']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Define the linear regression model\n",
    "# # class LinearRegression(nn.Module):\n",
    "# #     def __init__(self, input_dim):\n",
    "# #         super(LinearRegression, self).__init__()\n",
    "# #         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# def twostg_train_model(X_train, y_train, eps_p, \\\n",
    "#                        lr, weight_decay, epochs):\n",
    "\n",
    "#     # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#     # Initialize the model, loss function, and optimizer\n",
    "#     model = LogisticRegression(input_dim, output_dim)\n",
    "#     criterion = nn.BCELoss(reduction='sum') # Binary Cross Entropy Loss for binary classification\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "\n",
    "# #     patience = 10\n",
    "# #     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=1e-3, patience=patience, \\\n",
    "# #                                 verbose=True)\n",
    "\n",
    "\n",
    "#     # Training loop\n",
    "    \n",
    "#     #   b, Lamb, Delta = get_noise_vector(eps_dash_p)\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# #     print(\"n = \", X_train.shape[0])\n",
    "\n",
    "#     d = X_train.shape[1]\n",
    "#     b = np.random.gamma(d, scale=1.0/eta, size=(theta_d,1))\n",
    "#     #     sample_l2lap(eta, X_train.shape[1])\n",
    "# #     print(b)\n",
    "#     b = b.reshape(1, -1)    \n",
    "\n",
    "\n",
    "#     for epoch in tqdm(range(epochs), desc='Training'):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_train_tensor)\n",
    "#         theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "# #         theta = theta.reshape(1, -1)\n",
    "#         b = torch.Tensor(b)\n",
    "# #         print(b.shape)\n",
    "#         #       print(theta.shape)\n",
    "#         pert = torch.dot(b.flatten(), theta.flatten())\n",
    "#         #       print(pert)\n",
    "#         # print(pert[0,0])\n",
    "#         # print(pert.shape)\n",
    "#         # if eps_p == 100:\n",
    "#         #   print(pert[0,0])\n",
    "#         # Calculate the loss\n",
    "# #         print(criterion(outputs, y_train_tensor))\n",
    "#         loss = criterion(outputs, y_train_tensor) \\\n",
    "#             + pert \\\n",
    "#             + ((Lamb + Delta) * (torch.norm(theta, p=2)**2))\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (epoch+1) % 20 == 0:\n",
    "# #             print(criterion(outputs, y_train_tensor))\n",
    "# #             print((Lamb + Delta) * (torch.norm(theta, p=2)**2))\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "#         self.sigmoid = torch.nn.Sigmoid()  # Define sigmoid as a class member\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.sigmoid(x)  # Apply sigmoid to bound outputs between 0 and 1\n",
    "#         return x\n",
    "    \n",
    "#     def get_parameters(self):\n",
    "#         return next(self.parameters())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()  # Define sigmoid as a class member\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)  # Apply sigmoid to bound outputs between 0 and 1\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # Compute sigmoid: 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def twostg_train_model(X_train, y_train, eps_p, lr, epochs, Lamb, batch_size=128):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "#     dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#     data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    n, d =  X_train.shape[0], X_train.shape[1]\n",
    "    \n",
    "    model = LogReg(d, 1)\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    \n",
    "    \n",
    "    theta_init = torch.randn((d,1),requires_grad=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1 / (n * Lamb))))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / (6 * math.sqrt(n))\n",
    "\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    b = np.random.gamma(d, scale=1.0 / eta, size=(d+1, 1))\n",
    "    b = torch.Tensor(b.reshape(1, -1))\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc='Training'):\n",
    "        model.train()\n",
    "        \n",
    "   #         for batch_X, batch_y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "#             outputs = model(batch_X)\n",
    "\n",
    "#         y_hat_init = \n",
    "        outputs = model(X_train_tensor)\n",
    "# #             print(y_hat_init.shape)\n",
    "#         outputs = sigmoid(y_hat_init.flatten())\n",
    "#             outputs = model(X_batch)\n",
    "\n",
    "        theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "#         theta = model.get_parameters().view(-1)  # Get current parameters\n",
    "#         theta = theta_init\n",
    "\n",
    "#         print(theta)\n",
    "\n",
    "\n",
    "\n",
    "        # Add perturbation\n",
    "        pert = (1 / n) * torch.dot(b.flatten(), theta.flatten())\n",
    "#         print(pert)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = (\n",
    "            criterion(outputs.flatten(), y_train_tensor.flatten())\n",
    "            + pert\n",
    "            + ((Lamb + Delta) * (torch.norm(theta, p=2) ** 2))\n",
    "        )\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging training loss\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "#                 model.eval()\n",
    "                train_pred = model(X_train_tensor)\n",
    "#                 (torch.matmul(X_train_tensor, theta_init.float()) >= 0.5).float()\n",
    "#                 test_pred = (model(X_test_tensor) >= 0.5).float()\n",
    "                train_pred_cls = train_pred.round()\n",
    "                train_acc = (train_pred_cls == y_train_tensor).float().mean()\n",
    "#                 test_acc = (test_pred == y_test_tensor).float().mean()\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Train Acc: {train_acc:.4f}, ')\n",
    "#                       f'Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏                            | 70/10000 [00:00<00:34, 286.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 0.7050, Train Acc: 0.3084, \n",
      "Epoch [20/10000], Loss: 0.6946, Train Acc: 0.5074, \n",
      "Epoch [30/10000], Loss: 0.6846, Train Acc: 0.6746, \n",
      "Epoch [40/10000], Loss: 0.6750, Train Acc: 0.7404, \n",
      "Epoch [50/10000], Loss: 0.6658, Train Acc: 0.7651, \n",
      "Epoch [60/10000], Loss: 0.6571, Train Acc: 0.7708, \n",
      "Epoch [70/10000], Loss: 0.6488, Train Acc: 0.7709, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▍                           | 139/10000 [00:00<00:30, 321.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/10000], Loss: 0.6408, Train Acc: 0.7696, \n",
      "Epoch [90/10000], Loss: 0.6332, Train Acc: 0.7680, \n",
      "Epoch [100/10000], Loss: 0.6259, Train Acc: 0.7672, \n",
      "Epoch [110/10000], Loss: 0.6189, Train Acc: 0.7665, \n",
      "Epoch [120/10000], Loss: 0.6122, Train Acc: 0.7664, \n",
      "Epoch [130/10000], Loss: 0.6057, Train Acc: 0.7660, \n",
      "Epoch [140/10000], Loss: 0.5995, Train Acc: 0.7658, \n",
      "Epoch [150/10000], Loss: 0.5936, Train Acc: 0.7657, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▋                           | 227/10000 [00:00<00:25, 385.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/10000], Loss: 0.5878, Train Acc: 0.7660, \n",
      "Epoch [170/10000], Loss: 0.5822, Train Acc: 0.7662, \n",
      "Epoch [180/10000], Loss: 0.5769, Train Acc: 0.7667, \n",
      "Epoch [190/10000], Loss: 0.5716, Train Acc: 0.7670, \n",
      "Epoch [200/10000], Loss: 0.5666, Train Acc: 0.7675, \n",
      "Epoch [210/10000], Loss: 0.5617, Train Acc: 0.7679, \n",
      "Epoch [220/10000], Loss: 0.5570, Train Acc: 0.7684, \n",
      "Epoch [230/10000], Loss: 0.5524, Train Acc: 0.7691, \n",
      "Epoch [240/10000], Loss: 0.5480, Train Acc: 0.7700, \n",
      "Epoch [250/10000], Loss: 0.5437, Train Acc: 0.7706, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▉                           | 313/10000 [00:00<00:23, 405.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/10000], Loss: 0.5395, Train Acc: 0.7720, \n",
      "Epoch [270/10000], Loss: 0.5354, Train Acc: 0.7738, \n",
      "Epoch [280/10000], Loss: 0.5315, Train Acc: 0.7758, \n",
      "Epoch [290/10000], Loss: 0.5276, Train Acc: 0.7776, \n",
      "Epoch [300/10000], Loss: 0.5239, Train Acc: 0.7789, \n",
      "Epoch [310/10000], Loss: 0.5203, Train Acc: 0.7802, \n",
      "Epoch [320/10000], Loss: 0.5167, Train Acc: 0.7813, \n",
      "Epoch [330/10000], Loss: 0.5133, Train Acc: 0.7819, \n",
      "Epoch [340/10000], Loss: 0.5100, Train Acc: 0.7828, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|█                           | 401/10000 [00:01<00:22, 419.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/10000], Loss: 0.5067, Train Acc: 0.7836, \n",
      "Epoch [360/10000], Loss: 0.5035, Train Acc: 0.7848, \n",
      "Epoch [370/10000], Loss: 0.5005, Train Acc: 0.7857, \n",
      "Epoch [380/10000], Loss: 0.4974, Train Acc: 0.7865, \n",
      "Epoch [390/10000], Loss: 0.4945, Train Acc: 0.7873, \n",
      "Epoch [400/10000], Loss: 0.4916, Train Acc: 0.7881, \n",
      "Epoch [410/10000], Loss: 0.4888, Train Acc: 0.7891, \n",
      "Epoch [420/10000], Loss: 0.4861, Train Acc: 0.7902, \n",
      "Epoch [430/10000], Loss: 0.4835, Train Acc: 0.7915, \n",
      "Epoch [440/10000], Loss: 0.4809, Train Acc: 0.7926, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▎                          | 491/10000 [00:01<00:27, 347.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [450/10000], Loss: 0.4783, Train Acc: 0.7936, \n",
      "Epoch [460/10000], Loss: 0.4758, Train Acc: 0.7943, \n",
      "Epoch [470/10000], Loss: 0.4734, Train Acc: 0.7947, \n",
      "Epoch [480/10000], Loss: 0.4710, Train Acc: 0.7957, \n",
      "Epoch [490/10000], Loss: 0.4687, Train Acc: 0.7968, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   5%|█▍                          | 529/10000 [00:01<00:30, 314.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/10000], Loss: 0.4665, Train Acc: 0.7980, \n",
      "Epoch [510/10000], Loss: 0.4642, Train Acc: 0.7990, \n",
      "Epoch [520/10000], Loss: 0.4621, Train Acc: 0.8001, \n",
      "Epoch [530/10000], Loss: 0.4599, Train Acc: 0.8010, \n",
      "Epoch [540/10000], Loss: 0.4578, Train Acc: 0.8025, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|█▋                          | 610/10000 [00:01<00:27, 342.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [550/10000], Loss: 0.4558, Train Acc: 0.8038, \n",
      "Epoch [560/10000], Loss: 0.4538, Train Acc: 0.8051, \n",
      "Epoch [570/10000], Loss: 0.4518, Train Acc: 0.8068, \n",
      "Epoch [580/10000], Loss: 0.4499, Train Acc: 0.8083, \n",
      "Epoch [590/10000], Loss: 0.4480, Train Acc: 0.8093, \n",
      "Epoch [600/10000], Loss: 0.4462, Train Acc: 0.8105, \n",
      "Epoch [610/10000], Loss: 0.4444, Train Acc: 0.8113, \n",
      "Epoch [620/10000], Loss: 0.4426, Train Acc: 0.8121, \n",
      "Epoch [630/10000], Loss: 0.4408, Train Acc: 0.8128, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|█▉                          | 692/10000 [00:01<00:25, 370.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [640/10000], Loss: 0.4391, Train Acc: 0.8137, \n",
      "Epoch [650/10000], Loss: 0.4374, Train Acc: 0.8142, \n",
      "Epoch [660/10000], Loss: 0.4358, Train Acc: 0.8150, \n",
      "Epoch [670/10000], Loss: 0.4342, Train Acc: 0.8156, \n",
      "Epoch [680/10000], Loss: 0.4326, Train Acc: 0.8159, \n",
      "Epoch [690/10000], Loss: 0.4310, Train Acc: 0.8166, \n",
      "Epoch [700/10000], Loss: 0.4294, Train Acc: 0.8174, \n",
      "Epoch [710/10000], Loss: 0.4279, Train Acc: 0.8182, \n",
      "Epoch [720/10000], Loss: 0.4264, Train Acc: 0.8191, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|██▏                         | 775/10000 [00:02<00:23, 386.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [730/10000], Loss: 0.4250, Train Acc: 0.8196, \n",
      "Epoch [740/10000], Loss: 0.4235, Train Acc: 0.8201, \n",
      "Epoch [750/10000], Loss: 0.4221, Train Acc: 0.8209, \n",
      "Epoch [760/10000], Loss: 0.4207, Train Acc: 0.8216, \n",
      "Epoch [770/10000], Loss: 0.4193, Train Acc: 0.8226, \n",
      "Epoch [780/10000], Loss: 0.4179, Train Acc: 0.8228, \n",
      "Epoch [790/10000], Loss: 0.4166, Train Acc: 0.8232, \n",
      "Epoch [800/10000], Loss: 0.4153, Train Acc: 0.8241, \n",
      "Epoch [810/10000], Loss: 0.4140, Train Acc: 0.8246, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██▍                         | 865/10000 [00:02<00:22, 412.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [820/10000], Loss: 0.4127, Train Acc: 0.8253, \n",
      "Epoch [830/10000], Loss: 0.4115, Train Acc: 0.8256, \n",
      "Epoch [840/10000], Loss: 0.4102, Train Acc: 0.8260, \n",
      "Epoch [850/10000], Loss: 0.4090, Train Acc: 0.8266, \n",
      "Epoch [860/10000], Loss: 0.4078, Train Acc: 0.8271, \n",
      "Epoch [870/10000], Loss: 0.4066, Train Acc: 0.8276, \n",
      "Epoch [880/10000], Loss: 0.4054, Train Acc: 0.8281, \n",
      "Epoch [890/10000], Loss: 0.4043, Train Acc: 0.8283, \n",
      "Epoch [900/10000], Loss: 0.4031, Train Acc: 0.8287, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██▋                        | 1002/10000 [00:02<00:20, 438.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [910/10000], Loss: 0.4020, Train Acc: 0.8292, \n",
      "Epoch [920/10000], Loss: 0.4009, Train Acc: 0.8300, \n",
      "Epoch [930/10000], Loss: 0.3998, Train Acc: 0.8305, \n",
      "Epoch [940/10000], Loss: 0.3987, Train Acc: 0.8312, \n",
      "Epoch [950/10000], Loss: 0.3977, Train Acc: 0.8317, \n",
      "Epoch [960/10000], Loss: 0.3966, Train Acc: 0.8322, \n",
      "Epoch [970/10000], Loss: 0.3956, Train Acc: 0.8329, \n",
      "Epoch [980/10000], Loss: 0.3946, Train Acc: 0.8333, \n",
      "Epoch [990/10000], Loss: 0.3935, Train Acc: 0.8338, \n",
      "Epoch [1000/10000], Loss: 0.3925, Train Acc: 0.8344, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|██▉                        | 1100/10000 [00:02<00:19, 460.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1010/10000], Loss: 0.3915, Train Acc: 0.8349, \n",
      "Epoch [1020/10000], Loss: 0.3906, Train Acc: 0.8352, \n",
      "Epoch [1030/10000], Loss: 0.3896, Train Acc: 0.8356, \n",
      "Epoch [1040/10000], Loss: 0.3887, Train Acc: 0.8359, \n",
      "Epoch [1050/10000], Loss: 0.3877, Train Acc: 0.8362, \n",
      "Epoch [1060/10000], Loss: 0.3868, Train Acc: 0.8366, \n",
      "Epoch [1070/10000], Loss: 0.3859, Train Acc: 0.8368, \n",
      "Epoch [1080/10000], Loss: 0.3850, Train Acc: 0.8372, \n",
      "Epoch [1090/10000], Loss: 0.3841, Train Acc: 0.8375, \n",
      "Epoch [1100/10000], Loss: 0.3832, Train Acc: 0.8378, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███▏                       | 1198/10000 [00:03<00:18, 472.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1110/10000], Loss: 0.3823, Train Acc: 0.8380, \n",
      "Epoch [1120/10000], Loss: 0.3814, Train Acc: 0.8383, \n",
      "Epoch [1130/10000], Loss: 0.3806, Train Acc: 0.8385, \n",
      "Epoch [1140/10000], Loss: 0.3797, Train Acc: 0.8388, \n",
      "Epoch [1150/10000], Loss: 0.3789, Train Acc: 0.8394, \n",
      "Epoch [1160/10000], Loss: 0.3780, Train Acc: 0.8398, \n",
      "Epoch [1170/10000], Loss: 0.3772, Train Acc: 0.8403, \n",
      "Epoch [1180/10000], Loss: 0.3764, Train Acc: 0.8408, \n",
      "Epoch [1190/10000], Loss: 0.3756, Train Acc: 0.8412, \n",
      "Epoch [1200/10000], Loss: 0.3748, Train Acc: 0.8418, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|███▍                       | 1293/10000 [00:03<00:18, 465.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1210/10000], Loss: 0.3740, Train Acc: 0.8425, \n",
      "Epoch [1220/10000], Loss: 0.3732, Train Acc: 0.8427, \n",
      "Epoch [1230/10000], Loss: 0.3725, Train Acc: 0.8431, \n",
      "Epoch [1240/10000], Loss: 0.3717, Train Acc: 0.8435, \n",
      "Epoch [1250/10000], Loss: 0.3709, Train Acc: 0.8441, \n",
      "Epoch [1260/10000], Loss: 0.3702, Train Acc: 0.8447, \n",
      "Epoch [1270/10000], Loss: 0.3694, Train Acc: 0.8454, \n",
      "Epoch [1280/10000], Loss: 0.3687, Train Acc: 0.8457, \n",
      "Epoch [1290/10000], Loss: 0.3680, Train Acc: 0.8459, \n",
      "Epoch [1300/10000], Loss: 0.3672, Train Acc: 0.8465, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|███▋                       | 1387/10000 [00:03<00:18, 462.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1310/10000], Loss: 0.3665, Train Acc: 0.8469, \n",
      "Epoch [1320/10000], Loss: 0.3658, Train Acc: 0.8472, \n",
      "Epoch [1330/10000], Loss: 0.3651, Train Acc: 0.8477, \n",
      "Epoch [1340/10000], Loss: 0.3644, Train Acc: 0.8479, \n",
      "Epoch [1350/10000], Loss: 0.3637, Train Acc: 0.8484, \n",
      "Epoch [1360/10000], Loss: 0.3631, Train Acc: 0.8490, \n",
      "Epoch [1370/10000], Loss: 0.3624, Train Acc: 0.8494, \n",
      "Epoch [1380/10000], Loss: 0.3617, Train Acc: 0.8498, \n",
      "Epoch [1390/10000], Loss: 0.3610, Train Acc: 0.8501, \n",
      "Epoch [1400/10000], Loss: 0.3604, Train Acc: 0.8506, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|████                       | 1482/10000 [00:03<00:18, 460.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1410/10000], Loss: 0.3597, Train Acc: 0.8510, \n",
      "Epoch [1420/10000], Loss: 0.3591, Train Acc: 0.8515, \n",
      "Epoch [1430/10000], Loss: 0.3584, Train Acc: 0.8520, \n",
      "Epoch [1440/10000], Loss: 0.3578, Train Acc: 0.8523, \n",
      "Epoch [1450/10000], Loss: 0.3572, Train Acc: 0.8525, \n",
      "Epoch [1460/10000], Loss: 0.3566, Train Acc: 0.8528, \n",
      "Epoch [1470/10000], Loss: 0.3559, Train Acc: 0.8531, \n",
      "Epoch [1480/10000], Loss: 0.3553, Train Acc: 0.8533, \n",
      "Epoch [1490/10000], Loss: 0.3547, Train Acc: 0.8536, \n",
      "Epoch [1500/10000], Loss: 0.3541, Train Acc: 0.8539, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|████▎                      | 1580/10000 [00:03<00:17, 472.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1510/10000], Loss: 0.3535, Train Acc: 0.8541, \n",
      "Epoch [1520/10000], Loss: 0.3529, Train Acc: 0.8543, \n",
      "Epoch [1530/10000], Loss: 0.3523, Train Acc: 0.8545, \n",
      "Epoch [1540/10000], Loss: 0.3517, Train Acc: 0.8547, \n",
      "Epoch [1550/10000], Loss: 0.3511, Train Acc: 0.8549, \n",
      "Epoch [1560/10000], Loss: 0.3506, Train Acc: 0.8553, \n",
      "Epoch [1570/10000], Loss: 0.3500, Train Acc: 0.8555, \n",
      "Epoch [1580/10000], Loss: 0.3494, Train Acc: 0.8558, \n",
      "Epoch [1590/10000], Loss: 0.3489, Train Acc: 0.8560, \n",
      "Epoch [1600/10000], Loss: 0.3483, Train Acc: 0.8562, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|████▌                      | 1677/10000 [00:04<00:17, 476.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1610/10000], Loss: 0.3478, Train Acc: 0.8563, \n",
      "Epoch [1620/10000], Loss: 0.3472, Train Acc: 0.8567, \n",
      "Epoch [1630/10000], Loss: 0.3467, Train Acc: 0.8571, \n",
      "Epoch [1640/10000], Loss: 0.3461, Train Acc: 0.8572, \n",
      "Epoch [1650/10000], Loss: 0.3456, Train Acc: 0.8576, \n",
      "Epoch [1660/10000], Loss: 0.3450, Train Acc: 0.8579, \n",
      "Epoch [1670/10000], Loss: 0.3445, Train Acc: 0.8581, \n",
      "Epoch [1680/10000], Loss: 0.3440, Train Acc: 0.8584, \n",
      "Epoch [1690/10000], Loss: 0.3435, Train Acc: 0.8586, \n",
      "Epoch [1700/10000], Loss: 0.3429, Train Acc: 0.8587, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|████▊                      | 1773/10000 [00:04<00:17, 473.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1710/10000], Loss: 0.3424, Train Acc: 0.8589, \n",
      "Epoch [1720/10000], Loss: 0.3419, Train Acc: 0.8592, \n",
      "Epoch [1730/10000], Loss: 0.3414, Train Acc: 0.8595, \n",
      "Epoch [1740/10000], Loss: 0.3409, Train Acc: 0.8597, \n",
      "Epoch [1750/10000], Loss: 0.3404, Train Acc: 0.8599, \n",
      "Epoch [1760/10000], Loss: 0.3399, Train Acc: 0.8602, \n",
      "Epoch [1770/10000], Loss: 0.3394, Train Acc: 0.8605, \n",
      "Epoch [1780/10000], Loss: 0.3389, Train Acc: 0.8608, \n",
      "Epoch [1790/10000], Loss: 0.3384, Train Acc: 0.8611, \n",
      "Epoch [1800/10000], Loss: 0.3379, Train Acc: 0.8612, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:  18%|████▉                      | 1821/10000 [00:04<00:19, 422.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1810/10000], Loss: 0.3374, Train Acc: 0.8614, \n",
      "Epoch [1820/10000], Loss: 0.3370, Train Acc: 0.8617, \n",
      "Epoch [1830/10000], Loss: 0.3365, Train Acc: 0.8618, \n",
      "Epoch [1840/10000], Loss: 0.3360, Train Acc: 0.8620, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████▏                     | 1912/10000 [00:04<00:21, 380.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1850/10000], Loss: 0.3355, Train Acc: 0.8622, \n",
      "Epoch [1860/10000], Loss: 0.3351, Train Acc: 0.8625, \n",
      "Epoch [1870/10000], Loss: 0.3346, Train Acc: 0.8626, \n",
      "Epoch [1880/10000], Loss: 0.3341, Train Acc: 0.8628, \n",
      "Epoch [1890/10000], Loss: 0.3337, Train Acc: 0.8630, \n",
      "Epoch [1900/10000], Loss: 0.3332, Train Acc: 0.8632, \n",
      "Epoch [1910/10000], Loss: 0.3328, Train Acc: 0.8634, \n",
      "Epoch [1920/10000], Loss: 0.3323, Train Acc: 0.8637, \n",
      "Epoch [1930/10000], Loss: 0.3319, Train Acc: 0.8639, \n",
      "Epoch [1940/10000], Loss: 0.3314, Train Acc: 0.8639, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█████▍                     | 2008/10000 [00:04<00:18, 424.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1950/10000], Loss: 0.3310, Train Acc: 0.8642, \n",
      "Epoch [1960/10000], Loss: 0.3305, Train Acc: 0.8643, \n",
      "Epoch [1970/10000], Loss: 0.3301, Train Acc: 0.8644, \n",
      "Epoch [1980/10000], Loss: 0.3297, Train Acc: 0.8646, \n",
      "Epoch [1990/10000], Loss: 0.3292, Train Acc: 0.8648, \n",
      "Epoch [2000/10000], Loss: 0.3288, Train Acc: 0.8648, \n",
      "Epoch [2010/10000], Loss: 0.3284, Train Acc: 0.8650, \n",
      "Epoch [2020/10000], Loss: 0.3279, Train Acc: 0.8651, \n",
      "Epoch [2030/10000], Loss: 0.3275, Train Acc: 0.8652, \n",
      "Epoch [2040/10000], Loss: 0.3271, Train Acc: 0.8655, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|█████▋                     | 2106/10000 [00:05<00:17, 453.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2050/10000], Loss: 0.3267, Train Acc: 0.8657, \n",
      "Epoch [2060/10000], Loss: 0.3263, Train Acc: 0.8658, \n",
      "Epoch [2070/10000], Loss: 0.3258, Train Acc: 0.8659, \n",
      "Epoch [2080/10000], Loss: 0.3254, Train Acc: 0.8662, \n",
      "Epoch [2090/10000], Loss: 0.3250, Train Acc: 0.8663, \n",
      "Epoch [2100/10000], Loss: 0.3246, Train Acc: 0.8665, \n",
      "Epoch [2110/10000], Loss: 0.3242, Train Acc: 0.8668, \n",
      "Epoch [2120/10000], Loss: 0.3238, Train Acc: 0.8669, \n",
      "Epoch [2130/10000], Loss: 0.3234, Train Acc: 0.8670, \n",
      "Epoch [2140/10000], Loss: 0.3230, Train Acc: 0.8671, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|█████▉                     | 2203/10000 [00:05<00:16, 467.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2150/10000], Loss: 0.3226, Train Acc: 0.8671, \n",
      "Epoch [2160/10000], Loss: 0.3222, Train Acc: 0.8673, \n",
      "Epoch [2170/10000], Loss: 0.3218, Train Acc: 0.8676, \n",
      "Epoch [2180/10000], Loss: 0.3214, Train Acc: 0.8678, \n",
      "Epoch [2190/10000], Loss: 0.3210, Train Acc: 0.8679, \n",
      "Epoch [2200/10000], Loss: 0.3206, Train Acc: 0.8681, \n",
      "Epoch [2210/10000], Loss: 0.3202, Train Acc: 0.8685, \n",
      "Epoch [2220/10000], Loss: 0.3199, Train Acc: 0.8687, \n",
      "Epoch [2230/10000], Loss: 0.3195, Train Acc: 0.8687, \n",
      "Epoch [2240/10000], Loss: 0.3191, Train Acc: 0.8688, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██████▏                    | 2301/10000 [00:05<00:16, 475.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2250/10000], Loss: 0.3187, Train Acc: 0.8691, \n",
      "Epoch [2260/10000], Loss: 0.3183, Train Acc: 0.8693, \n",
      "Epoch [2270/10000], Loss: 0.3180, Train Acc: 0.8695, \n",
      "Epoch [2280/10000], Loss: 0.3176, Train Acc: 0.8698, \n",
      "Epoch [2290/10000], Loss: 0.3172, Train Acc: 0.8699, \n",
      "Epoch [2300/10000], Loss: 0.3168, Train Acc: 0.8701, \n",
      "Epoch [2310/10000], Loss: 0.3165, Train Acc: 0.8703, \n",
      "Epoch [2320/10000], Loss: 0.3161, Train Acc: 0.8708, \n",
      "Epoch [2330/10000], Loss: 0.3157, Train Acc: 0.8708, \n",
      "Epoch [2340/10000], Loss: 0.3154, Train Acc: 0.8711, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██████▍                    | 2399/10000 [00:05<00:15, 479.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2350/10000], Loss: 0.3150, Train Acc: 0.8713, \n",
      "Epoch [2360/10000], Loss: 0.3146, Train Acc: 0.8715, \n",
      "Epoch [2370/10000], Loss: 0.3143, Train Acc: 0.8718, \n",
      "Epoch [2380/10000], Loss: 0.3139, Train Acc: 0.8719, \n",
      "Epoch [2390/10000], Loss: 0.3136, Train Acc: 0.8722, \n",
      "Epoch [2400/10000], Loss: 0.3132, Train Acc: 0.8724, \n",
      "Epoch [2410/10000], Loss: 0.3128, Train Acc: 0.8728, \n",
      "Epoch [2420/10000], Loss: 0.3125, Train Acc: 0.8731, \n",
      "Epoch [2430/10000], Loss: 0.3121, Train Acc: 0.8734, \n",
      "Epoch [2440/10000], Loss: 0.3118, Train Acc: 0.8736, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██████▋                    | 2498/10000 [00:05<00:15, 480.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2450/10000], Loss: 0.3114, Train Acc: 0.8739, \n",
      "Epoch [2460/10000], Loss: 0.3111, Train Acc: 0.8741, \n",
      "Epoch [2470/10000], Loss: 0.3107, Train Acc: 0.8745, \n",
      "Epoch [2480/10000], Loss: 0.3104, Train Acc: 0.8747, \n",
      "Epoch [2490/10000], Loss: 0.3100, Train Acc: 0.8751, \n",
      "Epoch [2500/10000], Loss: 0.3097, Train Acc: 0.8754, \n",
      "Epoch [2510/10000], Loss: 0.3094, Train Acc: 0.8757, \n",
      "Epoch [2520/10000], Loss: 0.3090, Train Acc: 0.8761, \n",
      "Epoch [2530/10000], Loss: 0.3087, Train Acc: 0.8765, \n",
      "Epoch [2540/10000], Loss: 0.3083, Train Acc: 0.8766, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|███████▏                   | 2648/10000 [00:06<00:14, 490.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2550/10000], Loss: 0.3080, Train Acc: 0.8770, \n",
      "Epoch [2560/10000], Loss: 0.3077, Train Acc: 0.8772, \n",
      "Epoch [2570/10000], Loss: 0.3073, Train Acc: 0.8775, \n",
      "Epoch [2580/10000], Loss: 0.3070, Train Acc: 0.8777, \n",
      "Epoch [2590/10000], Loss: 0.3067, Train Acc: 0.8779, \n",
      "Epoch [2600/10000], Loss: 0.3063, Train Acc: 0.8782, \n",
      "Epoch [2610/10000], Loss: 0.3060, Train Acc: 0.8786, \n",
      "Epoch [2620/10000], Loss: 0.3057, Train Acc: 0.8788, \n",
      "Epoch [2630/10000], Loss: 0.3053, Train Acc: 0.8789, \n",
      "Epoch [2640/10000], Loss: 0.3050, Train Acc: 0.8791, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|███████▍                   | 2748/10000 [00:06<00:14, 489.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2650/10000], Loss: 0.3047, Train Acc: 0.8794, \n",
      "Epoch [2660/10000], Loss: 0.3044, Train Acc: 0.8796, \n",
      "Epoch [2670/10000], Loss: 0.3040, Train Acc: 0.8799, \n",
      "Epoch [2680/10000], Loss: 0.3037, Train Acc: 0.8802, \n",
      "Epoch [2690/10000], Loss: 0.3034, Train Acc: 0.8805, \n",
      "Epoch [2700/10000], Loss: 0.3031, Train Acc: 0.8808, \n",
      "Epoch [2710/10000], Loss: 0.3027, Train Acc: 0.8811, \n",
      "Epoch [2720/10000], Loss: 0.3024, Train Acc: 0.8812, \n",
      "Epoch [2730/10000], Loss: 0.3021, Train Acc: 0.8815, \n",
      "Epoch [2740/10000], Loss: 0.3018, Train Acc: 0.8816, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|███████▋                   | 2847/10000 [00:06<00:14, 488.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2750/10000], Loss: 0.3015, Train Acc: 0.8817, \n",
      "Epoch [2760/10000], Loss: 0.3012, Train Acc: 0.8819, \n",
      "Epoch [2770/10000], Loss: 0.3008, Train Acc: 0.8821, \n",
      "Epoch [2780/10000], Loss: 0.3005, Train Acc: 0.8823, \n",
      "Epoch [2790/10000], Loss: 0.3002, Train Acc: 0.8825, \n",
      "Epoch [2800/10000], Loss: 0.2999, Train Acc: 0.8827, \n",
      "Epoch [2810/10000], Loss: 0.2996, Train Acc: 0.8828, \n",
      "Epoch [2820/10000], Loss: 0.2993, Train Acc: 0.8830, \n",
      "Epoch [2830/10000], Loss: 0.2990, Train Acc: 0.8833, \n",
      "Epoch [2840/10000], Loss: 0.2987, Train Acc: 0.8835, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|███████▉                   | 2945/10000 [00:06<00:14, 487.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2850/10000], Loss: 0.2984, Train Acc: 0.8838, \n",
      "Epoch [2860/10000], Loss: 0.2980, Train Acc: 0.8841, \n",
      "Epoch [2870/10000], Loss: 0.2977, Train Acc: 0.8843, \n",
      "Epoch [2880/10000], Loss: 0.2974, Train Acc: 0.8844, \n",
      "Epoch [2890/10000], Loss: 0.2971, Train Acc: 0.8846, \n",
      "Epoch [2900/10000], Loss: 0.2968, Train Acc: 0.8847, \n",
      "Epoch [2910/10000], Loss: 0.2965, Train Acc: 0.8848, \n",
      "Epoch [2920/10000], Loss: 0.2962, Train Acc: 0.8849, \n",
      "Epoch [2930/10000], Loss: 0.2959, Train Acc: 0.8850, \n",
      "Epoch [2940/10000], Loss: 0.2956, Train Acc: 0.8851, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|████████▏                  | 3044/10000 [00:07<00:14, 488.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2950/10000], Loss: 0.2953, Train Acc: 0.8853, \n",
      "Epoch [2960/10000], Loss: 0.2950, Train Acc: 0.8856, \n",
      "Epoch [2970/10000], Loss: 0.2947, Train Acc: 0.8858, \n",
      "Epoch [2980/10000], Loss: 0.2944, Train Acc: 0.8859, \n",
      "Epoch [2990/10000], Loss: 0.2941, Train Acc: 0.8861, \n",
      "Epoch [3000/10000], Loss: 0.2938, Train Acc: 0.8863, \n",
      "Epoch [3010/10000], Loss: 0.2935, Train Acc: 0.8866, \n",
      "Epoch [3020/10000], Loss: 0.2932, Train Acc: 0.8867, \n",
      "Epoch [3030/10000], Loss: 0.2929, Train Acc: 0.8869, \n",
      "Epoch [3040/10000], Loss: 0.2926, Train Acc: 0.8871, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|████████▍                  | 3142/10000 [00:07<00:14, 486.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3050/10000], Loss: 0.2924, Train Acc: 0.8873, \n",
      "Epoch [3060/10000], Loss: 0.2921, Train Acc: 0.8874, \n",
      "Epoch [3070/10000], Loss: 0.2918, Train Acc: 0.8877, \n",
      "Epoch [3080/10000], Loss: 0.2915, Train Acc: 0.8881, \n",
      "Epoch [3090/10000], Loss: 0.2912, Train Acc: 0.8885, \n",
      "Epoch [3100/10000], Loss: 0.2909, Train Acc: 0.8888, \n",
      "Epoch [3110/10000], Loss: 0.2906, Train Acc: 0.8890, \n",
      "Epoch [3120/10000], Loss: 0.2903, Train Acc: 0.8893, \n",
      "Epoch [3130/10000], Loss: 0.2900, Train Acc: 0.8894, \n",
      "Epoch [3140/10000], Loss: 0.2897, Train Acc: 0.8897, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|████████▊                  | 3241/10000 [00:07<00:13, 484.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3150/10000], Loss: 0.2895, Train Acc: 0.8899, \n",
      "Epoch [3160/10000], Loss: 0.2892, Train Acc: 0.8901, \n",
      "Epoch [3170/10000], Loss: 0.2889, Train Acc: 0.8904, \n",
      "Epoch [3180/10000], Loss: 0.2886, Train Acc: 0.8906, \n",
      "Epoch [3190/10000], Loss: 0.2883, Train Acc: 0.8908, \n",
      "Epoch [3200/10000], Loss: 0.2880, Train Acc: 0.8910, \n",
      "Epoch [3210/10000], Loss: 0.2877, Train Acc: 0.8912, \n",
      "Epoch [3220/10000], Loss: 0.2875, Train Acc: 0.8914, \n",
      "Epoch [3230/10000], Loss: 0.2872, Train Acc: 0.8916, \n",
      "Epoch [3240/10000], Loss: 0.2869, Train Acc: 0.8919, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|█████████                  | 3339/10000 [00:07<00:13, 481.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3250/10000], Loss: 0.2866, Train Acc: 0.8920, \n",
      "Epoch [3260/10000], Loss: 0.2863, Train Acc: 0.8922, \n",
      "Epoch [3270/10000], Loss: 0.2861, Train Acc: 0.8925, \n",
      "Epoch [3280/10000], Loss: 0.2858, Train Acc: 0.8927, \n",
      "Epoch [3290/10000], Loss: 0.2855, Train Acc: 0.8928, \n",
      "Epoch [3300/10000], Loss: 0.2852, Train Acc: 0.8931, \n",
      "Epoch [3310/10000], Loss: 0.2849, Train Acc: 0.8933, \n",
      "Epoch [3320/10000], Loss: 0.2847, Train Acc: 0.8935, \n",
      "Epoch [3330/10000], Loss: 0.2844, Train Acc: 0.8938, \n",
      "Epoch [3340/10000], Loss: 0.2841, Train Acc: 0.8941, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|█████████▎                 | 3438/10000 [00:07<00:13, 483.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3350/10000], Loss: 0.2838, Train Acc: 0.8944, \n",
      "Epoch [3360/10000], Loss: 0.2836, Train Acc: 0.8947, \n",
      "Epoch [3370/10000], Loss: 0.2833, Train Acc: 0.8949, \n",
      "Epoch [3380/10000], Loss: 0.2830, Train Acc: 0.8952, \n",
      "Epoch [3390/10000], Loss: 0.2827, Train Acc: 0.8953, \n",
      "Epoch [3400/10000], Loss: 0.2825, Train Acc: 0.8956, \n",
      "Epoch [3410/10000], Loss: 0.2822, Train Acc: 0.8958, \n",
      "Epoch [3420/10000], Loss: 0.2819, Train Acc: 0.8960, \n",
      "Epoch [3430/10000], Loss: 0.2816, Train Acc: 0.8962, \n",
      "Epoch [3440/10000], Loss: 0.2814, Train Acc: 0.8965, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|█████████▌                 | 3537/10000 [00:08<00:13, 485.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3450/10000], Loss: 0.2811, Train Acc: 0.8969, \n",
      "Epoch [3460/10000], Loss: 0.2808, Train Acc: 0.8970, \n",
      "Epoch [3470/10000], Loss: 0.2806, Train Acc: 0.8972, \n",
      "Epoch [3480/10000], Loss: 0.2803, Train Acc: 0.8975, \n",
      "Epoch [3490/10000], Loss: 0.2800, Train Acc: 0.8977, \n",
      "Epoch [3500/10000], Loss: 0.2798, Train Acc: 0.8979, \n",
      "Epoch [3510/10000], Loss: 0.2795, Train Acc: 0.8982, \n",
      "Epoch [3520/10000], Loss: 0.2792, Train Acc: 0.8984, \n",
      "Epoch [3530/10000], Loss: 0.2790, Train Acc: 0.8986, \n",
      "Epoch [3540/10000], Loss: 0.2787, Train Acc: 0.8988, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|█████████▊                 | 3635/10000 [00:08<00:13, 483.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3550/10000], Loss: 0.2784, Train Acc: 0.8989, \n",
      "Epoch [3560/10000], Loss: 0.2782, Train Acc: 0.8991, \n",
      "Epoch [3570/10000], Loss: 0.2779, Train Acc: 0.8992, \n",
      "Epoch [3580/10000], Loss: 0.2776, Train Acc: 0.8995, \n",
      "Epoch [3590/10000], Loss: 0.2774, Train Acc: 0.8998, \n",
      "Epoch [3600/10000], Loss: 0.2771, Train Acc: 0.9000, \n",
      "Epoch [3610/10000], Loss: 0.2768, Train Acc: 0.9004, \n",
      "Epoch [3620/10000], Loss: 0.2766, Train Acc: 0.9007, \n",
      "Epoch [3630/10000], Loss: 0.2763, Train Acc: 0.9009, \n",
      "Epoch [3640/10000], Loss: 0.2760, Train Acc: 0.9011, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|██████████                 | 3733/10000 [00:08<00:12, 485.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3650/10000], Loss: 0.2758, Train Acc: 0.9014, \n",
      "Epoch [3660/10000], Loss: 0.2755, Train Acc: 0.9017, \n",
      "Epoch [3670/10000], Loss: 0.2753, Train Acc: 0.9020, \n",
      "Epoch [3680/10000], Loss: 0.2750, Train Acc: 0.9023, \n",
      "Epoch [3690/10000], Loss: 0.2747, Train Acc: 0.9027, \n",
      "Epoch [3700/10000], Loss: 0.2745, Train Acc: 0.9030, \n",
      "Epoch [3710/10000], Loss: 0.2742, Train Acc: 0.9033, \n",
      "Epoch [3720/10000], Loss: 0.2740, Train Acc: 0.9036, \n",
      "Epoch [3730/10000], Loss: 0.2737, Train Acc: 0.9041, \n",
      "Epoch [3740/10000], Loss: 0.2734, Train Acc: 0.9044, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████▎                | 3831/10000 [00:08<00:12, 485.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3750/10000], Loss: 0.2732, Train Acc: 0.9046, \n",
      "Epoch [3760/10000], Loss: 0.2729, Train Acc: 0.9050, \n",
      "Epoch [3770/10000], Loss: 0.2727, Train Acc: 0.9053, \n",
      "Epoch [3780/10000], Loss: 0.2724, Train Acc: 0.9056, \n",
      "Epoch [3790/10000], Loss: 0.2722, Train Acc: 0.9060, \n",
      "Epoch [3800/10000], Loss: 0.2719, Train Acc: 0.9062, \n",
      "Epoch [3810/10000], Loss: 0.2717, Train Acc: 0.9065, \n",
      "Epoch [3820/10000], Loss: 0.2714, Train Acc: 0.9068, \n",
      "Epoch [3830/10000], Loss: 0.2711, Train Acc: 0.9070, \n",
      "Epoch [3840/10000], Loss: 0.2709, Train Acc: 0.9073, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|██████████▌                | 3930/10000 [00:08<00:12, 485.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3850/10000], Loss: 0.2706, Train Acc: 0.9075, \n",
      "Epoch [3860/10000], Loss: 0.2704, Train Acc: 0.9076, \n",
      "Epoch [3870/10000], Loss: 0.2701, Train Acc: 0.9079, \n",
      "Epoch [3880/10000], Loss: 0.2699, Train Acc: 0.9081, \n",
      "Epoch [3890/10000], Loss: 0.2696, Train Acc: 0.9083, \n",
      "Epoch [3900/10000], Loss: 0.2694, Train Acc: 0.9085, \n",
      "Epoch [3910/10000], Loss: 0.2691, Train Acc: 0.9088, \n",
      "Epoch [3920/10000], Loss: 0.2689, Train Acc: 0.9091, \n",
      "Epoch [3930/10000], Loss: 0.2686, Train Acc: 0.9092, \n",
      "Epoch [3940/10000], Loss: 0.2684, Train Acc: 0.9095, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████████▉                | 4029/10000 [00:09<00:12, 486.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3950/10000], Loss: 0.2681, Train Acc: 0.9097, \n",
      "Epoch [3960/10000], Loss: 0.2679, Train Acc: 0.9099, \n",
      "Epoch [3970/10000], Loss: 0.2676, Train Acc: 0.9100, \n",
      "Epoch [3980/10000], Loss: 0.2674, Train Acc: 0.9101, \n",
      "Epoch [3990/10000], Loss: 0.2671, Train Acc: 0.9103, \n",
      "Epoch [4000/10000], Loss: 0.2669, Train Acc: 0.9105, \n",
      "Epoch [4010/10000], Loss: 0.2666, Train Acc: 0.9107, \n",
      "Epoch [4020/10000], Loss: 0.2664, Train Acc: 0.9109, \n",
      "Epoch [4030/10000], Loss: 0.2661, Train Acc: 0.9110, \n",
      "Epoch [4040/10000], Loss: 0.2659, Train Acc: 0.9111, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|███████████▏               | 4128/10000 [00:09<00:12, 487.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4050/10000], Loss: 0.2656, Train Acc: 0.9113, \n",
      "Epoch [4060/10000], Loss: 0.2654, Train Acc: 0.9115, \n",
      "Epoch [4070/10000], Loss: 0.2651, Train Acc: 0.9118, \n",
      "Epoch [4080/10000], Loss: 0.2649, Train Acc: 0.9120, \n",
      "Epoch [4090/10000], Loss: 0.2646, Train Acc: 0.9122, \n",
      "Epoch [4100/10000], Loss: 0.2644, Train Acc: 0.9123, \n",
      "Epoch [4110/10000], Loss: 0.2642, Train Acc: 0.9125, \n",
      "Epoch [4120/10000], Loss: 0.2639, Train Acc: 0.9128, \n",
      "Epoch [4130/10000], Loss: 0.2637, Train Acc: 0.9130, \n",
      "Epoch [4140/10000], Loss: 0.2634, Train Acc: 0.9132, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|███████████▍               | 4226/10000 [00:09<00:11, 482.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4150/10000], Loss: 0.2632, Train Acc: 0.9133, \n",
      "Epoch [4160/10000], Loss: 0.2629, Train Acc: 0.9135, \n",
      "Epoch [4170/10000], Loss: 0.2627, Train Acc: 0.9136, \n",
      "Epoch [4180/10000], Loss: 0.2625, Train Acc: 0.9138, \n",
      "Epoch [4190/10000], Loss: 0.2622, Train Acc: 0.9140, \n",
      "Epoch [4200/10000], Loss: 0.2620, Train Acc: 0.9141, \n",
      "Epoch [4210/10000], Loss: 0.2617, Train Acc: 0.9141, \n",
      "Epoch [4220/10000], Loss: 0.2615, Train Acc: 0.9142, \n",
      "Epoch [4230/10000], Loss: 0.2612, Train Acc: 0.9145, \n",
      "Epoch [4240/10000], Loss: 0.2610, Train Acc: 0.9148, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|███████████▋               | 4324/10000 [00:09<00:12, 469.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4250/10000], Loss: 0.2608, Train Acc: 0.9149, \n",
      "Epoch [4260/10000], Loss: 0.2605, Train Acc: 0.9151, \n",
      "Epoch [4270/10000], Loss: 0.2603, Train Acc: 0.9154, \n",
      "Epoch [4280/10000], Loss: 0.2601, Train Acc: 0.9155, \n",
      "Epoch [4290/10000], Loss: 0.2598, Train Acc: 0.9156, \n",
      "Epoch [4300/10000], Loss: 0.2596, Train Acc: 0.9157, \n",
      "Epoch [4310/10000], Loss: 0.2593, Train Acc: 0.9159, \n",
      "Epoch [4320/10000], Loss: 0.2591, Train Acc: 0.9159, \n",
      "Epoch [4330/10000], Loss: 0.2589, Train Acc: 0.9161, \n",
      "Epoch [4340/10000], Loss: 0.2586, Train Acc: 0.9165, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████████▉               | 4421/10000 [00:09<00:11, 474.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4350/10000], Loss: 0.2584, Train Acc: 0.9167, \n",
      "Epoch [4360/10000], Loss: 0.2582, Train Acc: 0.9169, \n",
      "Epoch [4370/10000], Loss: 0.2579, Train Acc: 0.9171, \n",
      "Epoch [4380/10000], Loss: 0.2577, Train Acc: 0.9173, \n",
      "Epoch [4390/10000], Loss: 0.2574, Train Acc: 0.9175, \n",
      "Epoch [4400/10000], Loss: 0.2572, Train Acc: 0.9176, \n",
      "Epoch [4410/10000], Loss: 0.2570, Train Acc: 0.9178, \n",
      "Epoch [4420/10000], Loss: 0.2567, Train Acc: 0.9180, \n",
      "Epoch [4430/10000], Loss: 0.2565, Train Acc: 0.9182, \n",
      "Epoch [4440/10000], Loss: 0.2563, Train Acc: 0.9184, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████████████▏              | 4518/10000 [00:10<00:11, 476.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4450/10000], Loss: 0.2560, Train Acc: 0.9186, \n",
      "Epoch [4460/10000], Loss: 0.2558, Train Acc: 0.9188, \n",
      "Epoch [4470/10000], Loss: 0.2556, Train Acc: 0.9190, \n",
      "Epoch [4480/10000], Loss: 0.2553, Train Acc: 0.9191, \n",
      "Epoch [4490/10000], Loss: 0.2551, Train Acc: 0.9193, \n",
      "Epoch [4500/10000], Loss: 0.2549, Train Acc: 0.9195, \n",
      "Epoch [4510/10000], Loss: 0.2546, Train Acc: 0.9196, \n",
      "Epoch [4520/10000], Loss: 0.2544, Train Acc: 0.9198, \n",
      "Epoch [4530/10000], Loss: 0.2542, Train Acc: 0.9199, \n",
      "Epoch [4540/10000], Loss: 0.2540, Train Acc: 0.9200, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████████████▍              | 4617/10000 [00:10<00:11, 483.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4550/10000], Loss: 0.2537, Train Acc: 0.9202, \n",
      "Epoch [4560/10000], Loss: 0.2535, Train Acc: 0.9206, \n",
      "Epoch [4570/10000], Loss: 0.2533, Train Acc: 0.9208, \n",
      "Epoch [4580/10000], Loss: 0.2530, Train Acc: 0.9210, \n",
      "Epoch [4590/10000], Loss: 0.2528, Train Acc: 0.9211, \n",
      "Epoch [4600/10000], Loss: 0.2526, Train Acc: 0.9214, \n",
      "Epoch [4610/10000], Loss: 0.2523, Train Acc: 0.9215, \n",
      "Epoch [4620/10000], Loss: 0.2521, Train Acc: 0.9217, \n",
      "Epoch [4630/10000], Loss: 0.2519, Train Acc: 0.9220, \n",
      "Epoch [4640/10000], Loss: 0.2517, Train Acc: 0.9224, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████████████▋              | 4715/10000 [00:10<00:10, 482.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4650/10000], Loss: 0.2514, Train Acc: 0.9226, \n",
      "Epoch [4660/10000], Loss: 0.2512, Train Acc: 0.9228, \n",
      "Epoch [4670/10000], Loss: 0.2510, Train Acc: 0.9230, \n",
      "Epoch [4680/10000], Loss: 0.2508, Train Acc: 0.9233, \n",
      "Epoch [4690/10000], Loss: 0.2505, Train Acc: 0.9234, \n",
      "Epoch [4700/10000], Loss: 0.2503, Train Acc: 0.9236, \n",
      "Epoch [4710/10000], Loss: 0.2501, Train Acc: 0.9239, \n",
      "Epoch [4720/10000], Loss: 0.2499, Train Acc: 0.9242, \n",
      "Epoch [4730/10000], Loss: 0.2496, Train Acc: 0.9244, \n",
      "Epoch [4740/10000], Loss: 0.2494, Train Acc: 0.9245, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████████▉              | 4813/10000 [00:10<00:10, 482.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4750/10000], Loss: 0.2492, Train Acc: 0.9247, \n",
      "Epoch [4760/10000], Loss: 0.2490, Train Acc: 0.9249, \n",
      "Epoch [4770/10000], Loss: 0.2487, Train Acc: 0.9252, \n",
      "Epoch [4780/10000], Loss: 0.2485, Train Acc: 0.9255, \n",
      "Epoch [4790/10000], Loss: 0.2483, Train Acc: 0.9257, \n",
      "Epoch [4800/10000], Loss: 0.2481, Train Acc: 0.9259, \n",
      "Epoch [4810/10000], Loss: 0.2478, Train Acc: 0.9261, \n",
      "Epoch [4820/10000], Loss: 0.2476, Train Acc: 0.9266, \n",
      "Epoch [4830/10000], Loss: 0.2474, Train Acc: 0.9268, \n",
      "Epoch [4840/10000], Loss: 0.2472, Train Acc: 0.9270, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|█████████████▎             | 4911/10000 [00:10<00:10, 479.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4850/10000], Loss: 0.2470, Train Acc: 0.9272, \n",
      "Epoch [4860/10000], Loss: 0.2467, Train Acc: 0.9275, \n",
      "Epoch [4870/10000], Loss: 0.2465, Train Acc: 0.9277, \n",
      "Epoch [4880/10000], Loss: 0.2463, Train Acc: 0.9279, \n",
      "Epoch [4890/10000], Loss: 0.2461, Train Acc: 0.9281, \n",
      "Epoch [4900/10000], Loss: 0.2459, Train Acc: 0.9282, \n",
      "Epoch [4910/10000], Loss: 0.2456, Train Acc: 0.9283, \n",
      "Epoch [4920/10000], Loss: 0.2454, Train Acc: 0.9284, \n",
      "Epoch [4930/10000], Loss: 0.2452, Train Acc: 0.9286, \n",
      "Epoch [4940/10000], Loss: 0.2450, Train Acc: 0.9288, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████████████▌             | 5008/10000 [00:11<00:10, 480.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4950/10000], Loss: 0.2448, Train Acc: 0.9290, \n",
      "Epoch [4960/10000], Loss: 0.2445, Train Acc: 0.9292, \n",
      "Epoch [4970/10000], Loss: 0.2443, Train Acc: 0.9294, \n",
      "Epoch [4980/10000], Loss: 0.2441, Train Acc: 0.9296, \n",
      "Epoch [4990/10000], Loss: 0.2439, Train Acc: 0.9297, \n",
      "Epoch [5000/10000], Loss: 0.2437, Train Acc: 0.9298, \n",
      "Epoch [5010/10000], Loss: 0.2435, Train Acc: 0.9299, \n",
      "Epoch [5020/10000], Loss: 0.2432, Train Acc: 0.9301, \n",
      "Epoch [5030/10000], Loss: 0.2430, Train Acc: 0.9302, \n",
      "Epoch [5040/10000], Loss: 0.2428, Train Acc: 0.9304, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████████████▊             | 5105/10000 [00:11<00:10, 472.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5050/10000], Loss: 0.2426, Train Acc: 0.9305, \n",
      "Epoch [5060/10000], Loss: 0.2424, Train Acc: 0.9307, \n",
      "Epoch [5070/10000], Loss: 0.2422, Train Acc: 0.9310, \n",
      "Epoch [5080/10000], Loss: 0.2420, Train Acc: 0.9310, \n",
      "Epoch [5090/10000], Loss: 0.2417, Train Acc: 0.9311, \n",
      "Epoch [5100/10000], Loss: 0.2415, Train Acc: 0.9313, \n",
      "Epoch [5110/10000], Loss: 0.2413, Train Acc: 0.9313, \n",
      "Epoch [5120/10000], Loss: 0.2411, Train Acc: 0.9315, \n",
      "Epoch [5130/10000], Loss: 0.2409, Train Acc: 0.9316, \n",
      "Epoch [5140/10000], Loss: 0.2407, Train Acc: 0.9318, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|██████████████             | 5202/10000 [00:11<00:10, 474.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5150/10000], Loss: 0.2405, Train Acc: 0.9319, \n",
      "Epoch [5160/10000], Loss: 0.2402, Train Acc: 0.9320, \n",
      "Epoch [5170/10000], Loss: 0.2400, Train Acc: 0.9321, \n",
      "Epoch [5180/10000], Loss: 0.2398, Train Acc: 0.9323, \n",
      "Epoch [5190/10000], Loss: 0.2396, Train Acc: 0.9323, \n",
      "Epoch [5200/10000], Loss: 0.2394, Train Acc: 0.9325, \n",
      "Epoch [5210/10000], Loss: 0.2392, Train Acc: 0.9326, \n",
      "Epoch [5220/10000], Loss: 0.2390, Train Acc: 0.9328, \n",
      "Epoch [5230/10000], Loss: 0.2388, Train Acc: 0.9329, \n",
      "Epoch [5240/10000], Loss: 0.2386, Train Acc: 0.9330, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|██████████████▎            | 5300/10000 [00:11<00:09, 478.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5250/10000], Loss: 0.2384, Train Acc: 0.9331, \n",
      "Epoch [5260/10000], Loss: 0.2381, Train Acc: 0.9333, \n",
      "Epoch [5270/10000], Loss: 0.2379, Train Acc: 0.9334, \n",
      "Epoch [5280/10000], Loss: 0.2377, Train Acc: 0.9335, \n",
      "Epoch [5290/10000], Loss: 0.2375, Train Acc: 0.9336, \n",
      "Epoch [5300/10000], Loss: 0.2373, Train Acc: 0.9338, \n",
      "Epoch [5310/10000], Loss: 0.2371, Train Acc: 0.9338, \n",
      "Epoch [5320/10000], Loss: 0.2369, Train Acc: 0.9340, \n",
      "Epoch [5330/10000], Loss: 0.2367, Train Acc: 0.9340, \n",
      "Epoch [5340/10000], Loss: 0.2365, Train Acc: 0.9342, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|██████████████▌            | 5399/10000 [00:11<00:09, 472.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5350/10000], Loss: 0.2363, Train Acc: 0.9343, \n",
      "Epoch [5360/10000], Loss: 0.2361, Train Acc: 0.9344, \n",
      "Epoch [5370/10000], Loss: 0.2359, Train Acc: 0.9345, \n",
      "Epoch [5380/10000], Loss: 0.2357, Train Acc: 0.9345, \n",
      "Epoch [5390/10000], Loss: 0.2354, Train Acc: 0.9346, \n",
      "Epoch [5400/10000], Loss: 0.2352, Train Acc: 0.9346, \n",
      "Epoch [5410/10000], Loss: 0.2350, Train Acc: 0.9347, \n",
      "Epoch [5420/10000], Loss: 0.2348, Train Acc: 0.9348, \n",
      "Epoch [5430/10000], Loss: 0.2346, Train Acc: 0.9349, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|██████████████▊            | 5494/10000 [00:12<00:10, 445.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5440/10000], Loss: 0.2344, Train Acc: 0.9350, \n",
      "Epoch [5450/10000], Loss: 0.2342, Train Acc: 0.9351, \n",
      "Epoch [5460/10000], Loss: 0.2340, Train Acc: 0.9352, \n",
      "Epoch [5470/10000], Loss: 0.2338, Train Acc: 0.9352, \n",
      "Epoch [5480/10000], Loss: 0.2336, Train Acc: 0.9353, \n",
      "Epoch [5490/10000], Loss: 0.2334, Train Acc: 0.9354, \n",
      "Epoch [5500/10000], Loss: 0.2332, Train Acc: 0.9355, \n",
      "Epoch [5510/10000], Loss: 0.2330, Train Acc: 0.9356, \n",
      "Epoch [5520/10000], Loss: 0.2328, Train Acc: 0.9356, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|███████████████            | 5590/10000 [00:12<00:09, 460.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5530/10000], Loss: 0.2326, Train Acc: 0.9356, \n",
      "Epoch [5540/10000], Loss: 0.2324, Train Acc: 0.9357, \n",
      "Epoch [5550/10000], Loss: 0.2322, Train Acc: 0.9358, \n",
      "Epoch [5560/10000], Loss: 0.2320, Train Acc: 0.9358, \n",
      "Epoch [5570/10000], Loss: 0.2318, Train Acc: 0.9359, \n",
      "Epoch [5580/10000], Loss: 0.2316, Train Acc: 0.9360, \n",
      "Epoch [5590/10000], Loss: 0.2314, Train Acc: 0.9361, \n",
      "Epoch [5600/10000], Loss: 0.2312, Train Acc: 0.9362, \n",
      "Epoch [5610/10000], Loss: 0.2310, Train Acc: 0.9363, \n",
      "Epoch [5620/10000], Loss: 0.2308, Train Acc: 0.9364, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|███████████████▎           | 5684/10000 [00:12<00:09, 458.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5630/10000], Loss: 0.2306, Train Acc: 0.9365, \n",
      "Epoch [5640/10000], Loss: 0.2304, Train Acc: 0.9367, \n",
      "Epoch [5650/10000], Loss: 0.2302, Train Acc: 0.9368, \n",
      "Epoch [5660/10000], Loss: 0.2300, Train Acc: 0.9368, \n",
      "Epoch [5670/10000], Loss: 0.2298, Train Acc: 0.9369, \n",
      "Epoch [5680/10000], Loss: 0.2296, Train Acc: 0.9369, \n",
      "Epoch [5690/10000], Loss: 0.2294, Train Acc: 0.9371, \n",
      "Epoch [5700/10000], Loss: 0.2292, Train Acc: 0.9371, \n",
      "Epoch [5710/10000], Loss: 0.2290, Train Acc: 0.9372, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|███████████████▌           | 5776/10000 [00:12<00:09, 448.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5720/10000], Loss: 0.2288, Train Acc: 0.9373, \n",
      "Epoch [5730/10000], Loss: 0.2286, Train Acc: 0.9374, \n",
      "Epoch [5740/10000], Loss: 0.2284, Train Acc: 0.9375, \n",
      "Epoch [5750/10000], Loss: 0.2282, Train Acc: 0.9376, \n",
      "Epoch [5760/10000], Loss: 0.2280, Train Acc: 0.9378, \n",
      "Epoch [5770/10000], Loss: 0.2278, Train Acc: 0.9380, \n",
      "Epoch [5780/10000], Loss: 0.2276, Train Acc: 0.9381, \n",
      "Epoch [5790/10000], Loss: 0.2274, Train Acc: 0.9382, \n",
      "Epoch [5800/10000], Loss: 0.2272, Train Acc: 0.9385, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|███████████████▊           | 5866/10000 [00:12<00:09, 447.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5810/10000], Loss: 0.2270, Train Acc: 0.9386, \n",
      "Epoch [5820/10000], Loss: 0.2269, Train Acc: 0.9388, \n",
      "Epoch [5830/10000], Loss: 0.2267, Train Acc: 0.9389, \n",
      "Epoch [5840/10000], Loss: 0.2265, Train Acc: 0.9391, \n",
      "Epoch [5850/10000], Loss: 0.2263, Train Acc: 0.9393, \n",
      "Epoch [5860/10000], Loss: 0.2261, Train Acc: 0.9394, \n",
      "Epoch [5870/10000], Loss: 0.2259, Train Acc: 0.9395, \n",
      "Epoch [5880/10000], Loss: 0.2257, Train Acc: 0.9398, \n",
      "Epoch [5890/10000], Loss: 0.2255, Train Acc: 0.9400, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|████████████████           | 5955/10000 [00:13<00:09, 422.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5900/10000], Loss: 0.2253, Train Acc: 0.9401, \n",
      "Epoch [5910/10000], Loss: 0.2251, Train Acc: 0.9403, \n",
      "Epoch [5920/10000], Loss: 0.2249, Train Acc: 0.9407, \n",
      "Epoch [5930/10000], Loss: 0.2247, Train Acc: 0.9408, \n",
      "Epoch [5940/10000], Loss: 0.2245, Train Acc: 0.9410, \n",
      "Epoch [5950/10000], Loss: 0.2244, Train Acc: 0.9412, \n",
      "Epoch [5960/10000], Loss: 0.2242, Train Acc: 0.9415, \n",
      "Epoch [5970/10000], Loss: 0.2240, Train Acc: 0.9418, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|████████████████▎          | 6038/10000 [00:13<00:10, 361.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5980/10000], Loss: 0.2238, Train Acc: 0.9419, \n",
      "Epoch [5990/10000], Loss: 0.2236, Train Acc: 0.9422, \n",
      "Epoch [6000/10000], Loss: 0.2234, Train Acc: 0.9423, \n",
      "Epoch [6010/10000], Loss: 0.2232, Train Acc: 0.9426, \n",
      "Epoch [6020/10000], Loss: 0.2230, Train Acc: 0.9428, \n",
      "Epoch [6030/10000], Loss: 0.2228, Train Acc: 0.9431, \n",
      "Epoch [6040/10000], Loss: 0.2226, Train Acc: 0.9433, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|████████████████▌          | 6120/10000 [00:13<00:10, 383.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6050/10000], Loss: 0.2225, Train Acc: 0.9434, \n",
      "Epoch [6060/10000], Loss: 0.2223, Train Acc: 0.9436, \n",
      "Epoch [6070/10000], Loss: 0.2221, Train Acc: 0.9440, \n",
      "Epoch [6080/10000], Loss: 0.2219, Train Acc: 0.9442, \n",
      "Epoch [6090/10000], Loss: 0.2217, Train Acc: 0.9444, \n",
      "Epoch [6100/10000], Loss: 0.2215, Train Acc: 0.9447, \n",
      "Epoch [6110/10000], Loss: 0.2213, Train Acc: 0.9450, \n",
      "Epoch [6120/10000], Loss: 0.2211, Train Acc: 0.9451, \n",
      "Epoch [6130/10000], Loss: 0.2210, Train Acc: 0.9452, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|████████████████▊          | 6210/10000 [00:13<00:09, 414.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6140/10000], Loss: 0.2208, Train Acc: 0.9454, \n",
      "Epoch [6150/10000], Loss: 0.2206, Train Acc: 0.9455, \n",
      "Epoch [6160/10000], Loss: 0.2204, Train Acc: 0.9457, \n",
      "Epoch [6170/10000], Loss: 0.2202, Train Acc: 0.9457, \n",
      "Epoch [6180/10000], Loss: 0.2200, Train Acc: 0.9458, \n",
      "Epoch [6190/10000], Loss: 0.2198, Train Acc: 0.9460, \n",
      "Epoch [6200/10000], Loss: 0.2197, Train Acc: 0.9462, \n",
      "Epoch [6210/10000], Loss: 0.2195, Train Acc: 0.9462, \n",
      "Epoch [6220/10000], Loss: 0.2193, Train Acc: 0.9464, \n",
      "Epoch [6230/10000], Loss: 0.2191, Train Acc: 0.9466, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|█████████████████          | 6300/10000 [00:14<00:08, 428.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6240/10000], Loss: 0.2189, Train Acc: 0.9466, \n",
      "Epoch [6250/10000], Loss: 0.2187, Train Acc: 0.9468, \n",
      "Epoch [6260/10000], Loss: 0.2186, Train Acc: 0.9469, \n",
      "Epoch [6270/10000], Loss: 0.2184, Train Acc: 0.9471, \n",
      "Epoch [6280/10000], Loss: 0.2182, Train Acc: 0.9471, \n",
      "Epoch [6290/10000], Loss: 0.2180, Train Acc: 0.9472, \n",
      "Epoch [6300/10000], Loss: 0.2178, Train Acc: 0.9473, \n",
      "Epoch [6310/10000], Loss: 0.2176, Train Acc: 0.9474, \n",
      "Epoch [6320/10000], Loss: 0.2175, Train Acc: 0.9474, \n",
      "Epoch [6330/10000], Loss: 0.2173, Train Acc: 0.9475, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|█████████████████▎         | 6389/10000 [00:14<00:08, 434.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6340/10000], Loss: 0.2171, Train Acc: 0.9476, \n",
      "Epoch [6350/10000], Loss: 0.2169, Train Acc: 0.9477, \n",
      "Epoch [6360/10000], Loss: 0.2167, Train Acc: 0.9478, \n",
      "Epoch [6370/10000], Loss: 0.2165, Train Acc: 0.9478, \n",
      "Epoch [6380/10000], Loss: 0.2164, Train Acc: 0.9479, \n",
      "Epoch [6390/10000], Loss: 0.2162, Train Acc: 0.9480, \n",
      "Epoch [6400/10000], Loss: 0.2160, Train Acc: 0.9481, \n",
      "Epoch [6410/10000], Loss: 0.2158, Train Acc: 0.9481, \n",
      "Epoch [6420/10000], Loss: 0.2156, Train Acc: 0.9482, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████▍         | 6477/10000 [00:14<00:08, 433.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6430/10000], Loss: 0.2155, Train Acc: 0.9484, \n",
      "Epoch [6440/10000], Loss: 0.2153, Train Acc: 0.9484, \n",
      "Epoch [6450/10000], Loss: 0.2151, Train Acc: 0.9485, \n",
      "Epoch [6460/10000], Loss: 0.2149, Train Acc: 0.9485, \n",
      "Epoch [6470/10000], Loss: 0.2148, Train Acc: 0.9486, \n",
      "Epoch [6480/10000], Loss: 0.2146, Train Acc: 0.9486, \n",
      "Epoch [6490/10000], Loss: 0.2144, Train Acc: 0.9487, \n",
      "Epoch [6500/10000], Loss: 0.2142, Train Acc: 0.9487, \n",
      "Epoch [6510/10000], Loss: 0.2140, Train Acc: 0.9488, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|█████████████████▋         | 6567/10000 [00:14<00:07, 439.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6520/10000], Loss: 0.2139, Train Acc: 0.9489, \n",
      "Epoch [6530/10000], Loss: 0.2137, Train Acc: 0.9489, \n",
      "Epoch [6540/10000], Loss: 0.2135, Train Acc: 0.9490, \n",
      "Epoch [6550/10000], Loss: 0.2133, Train Acc: 0.9490, \n",
      "Epoch [6560/10000], Loss: 0.2132, Train Acc: 0.9491, \n",
      "Epoch [6570/10000], Loss: 0.2130, Train Acc: 0.9491, \n",
      "Epoch [6580/10000], Loss: 0.2128, Train Acc: 0.9492, \n",
      "Epoch [6590/10000], Loss: 0.2126, Train Acc: 0.9492, \n",
      "Epoch [6600/10000], Loss: 0.2125, Train Acc: 0.9493, \n",
      "Epoch [6610/10000], Loss: 0.2123, Train Acc: 0.9494, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████████████████         | 6706/10000 [00:14<00:07, 454.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6620/10000], Loss: 0.2121, Train Acc: 0.9495, \n",
      "Epoch [6630/10000], Loss: 0.2119, Train Acc: 0.9496, \n",
      "Epoch [6640/10000], Loss: 0.2118, Train Acc: 0.9497, \n",
      "Epoch [6650/10000], Loss: 0.2116, Train Acc: 0.9498, \n",
      "Epoch [6660/10000], Loss: 0.2114, Train Acc: 0.9499, \n",
      "Epoch [6670/10000], Loss: 0.2112, Train Acc: 0.9500, \n",
      "Epoch [6680/10000], Loss: 0.2111, Train Acc: 0.9502, \n",
      "Epoch [6690/10000], Loss: 0.2109, Train Acc: 0.9503, \n",
      "Epoch [6700/10000], Loss: 0.2107, Train Acc: 0.9504, \n",
      "Epoch [6710/10000], Loss: 0.2105, Train Acc: 0.9506, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████████████▎        | 6801/10000 [00:15<00:06, 465.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6720/10000], Loss: 0.2104, Train Acc: 0.9506, \n",
      "Epoch [6730/10000], Loss: 0.2102, Train Acc: 0.9507, \n",
      "Epoch [6740/10000], Loss: 0.2100, Train Acc: 0.9508, \n",
      "Epoch [6750/10000], Loss: 0.2098, Train Acc: 0.9509, \n",
      "Epoch [6760/10000], Loss: 0.2097, Train Acc: 0.9510, \n",
      "Epoch [6770/10000], Loss: 0.2095, Train Acc: 0.9510, \n",
      "Epoch [6780/10000], Loss: 0.2093, Train Acc: 0.9511, \n",
      "Epoch [6790/10000], Loss: 0.2092, Train Acc: 0.9512, \n",
      "Epoch [6800/10000], Loss: 0.2090, Train Acc: 0.9512, \n",
      "Epoch [6810/10000], Loss: 0.2088, Train Acc: 0.9513, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████████████████▌        | 6895/10000 [00:15<00:06, 461.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6820/10000], Loss: 0.2086, Train Acc: 0.9514, \n",
      "Epoch [6830/10000], Loss: 0.2085, Train Acc: 0.9514, \n",
      "Epoch [6840/10000], Loss: 0.2083, Train Acc: 0.9515, \n",
      "Epoch [6850/10000], Loss: 0.2081, Train Acc: 0.9517, \n",
      "Epoch [6860/10000], Loss: 0.2080, Train Acc: 0.9518, \n",
      "Epoch [6870/10000], Loss: 0.2078, Train Acc: 0.9520, \n",
      "Epoch [6880/10000], Loss: 0.2076, Train Acc: 0.9521, \n",
      "Epoch [6890/10000], Loss: 0.2075, Train Acc: 0.9523, \n",
      "Epoch [6900/10000], Loss: 0.2073, Train Acc: 0.9523, \n",
      "Epoch [6910/10000], Loss: 0.2071, Train Acc: 0.9523, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████████████████▊        | 6989/10000 [00:15<00:06, 460.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6920/10000], Loss: 0.2069, Train Acc: 0.9524, \n",
      "Epoch [6930/10000], Loss: 0.2068, Train Acc: 0.9525, \n",
      "Epoch [6940/10000], Loss: 0.2066, Train Acc: 0.9525, \n",
      "Epoch [6950/10000], Loss: 0.2064, Train Acc: 0.9526, \n",
      "Epoch [6960/10000], Loss: 0.2063, Train Acc: 0.9526, \n",
      "Epoch [6970/10000], Loss: 0.2061, Train Acc: 0.9528, \n",
      "Epoch [6980/10000], Loss: 0.2059, Train Acc: 0.9528, \n",
      "Epoch [6990/10000], Loss: 0.2058, Train Acc: 0.9530, \n",
      "Epoch [7000/10000], Loss: 0.2056, Train Acc: 0.9531, \n",
      "Epoch [7010/10000], Loss: 0.2054, Train Acc: 0.9531, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████████████████        | 7083/10000 [00:15<00:06, 458.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7020/10000], Loss: 0.2053, Train Acc: 0.9533, \n",
      "Epoch [7030/10000], Loss: 0.2051, Train Acc: 0.9533, \n",
      "Epoch [7040/10000], Loss: 0.2049, Train Acc: 0.9534, \n",
      "Epoch [7050/10000], Loss: 0.2048, Train Acc: 0.9535, \n",
      "Epoch [7060/10000], Loss: 0.2046, Train Acc: 0.9535, \n",
      "Epoch [7070/10000], Loss: 0.2044, Train Acc: 0.9535, \n",
      "Epoch [7080/10000], Loss: 0.2043, Train Acc: 0.9535, \n",
      "Epoch [7090/10000], Loss: 0.2041, Train Acc: 0.9536, \n",
      "Epoch [7100/10000], Loss: 0.2039, Train Acc: 0.9537, \n",
      "Epoch [7110/10000], Loss: 0.2038, Train Acc: 0.9538, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████████████▍       | 7179/10000 [00:15<00:06, 468.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7120/10000], Loss: 0.2036, Train Acc: 0.9538, \n",
      "Epoch [7130/10000], Loss: 0.2035, Train Acc: 0.9539, \n",
      "Epoch [7140/10000], Loss: 0.2033, Train Acc: 0.9541, \n",
      "Epoch [7150/10000], Loss: 0.2031, Train Acc: 0.9542, \n",
      "Epoch [7160/10000], Loss: 0.2030, Train Acc: 0.9542, \n",
      "Epoch [7170/10000], Loss: 0.2028, Train Acc: 0.9542, \n",
      "Epoch [7180/10000], Loss: 0.2026, Train Acc: 0.9542, \n",
      "Epoch [7190/10000], Loss: 0.2025, Train Acc: 0.9543, \n",
      "Epoch [7200/10000], Loss: 0.2023, Train Acc: 0.9543, \n",
      "Epoch [7210/10000], Loss: 0.2021, Train Acc: 0.9544, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████████████████▋       | 7273/10000 [00:16<00:05, 464.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7220/10000], Loss: 0.2020, Train Acc: 0.9545, \n",
      "Epoch [7230/10000], Loss: 0.2018, Train Acc: 0.9546, \n",
      "Epoch [7240/10000], Loss: 0.2017, Train Acc: 0.9546, \n",
      "Epoch [7250/10000], Loss: 0.2015, Train Acc: 0.9547, \n",
      "Epoch [7260/10000], Loss: 0.2013, Train Acc: 0.9548, \n",
      "Epoch [7270/10000], Loss: 0.2012, Train Acc: 0.9548, \n",
      "Epoch [7280/10000], Loss: 0.2010, Train Acc: 0.9549, \n",
      "Epoch [7290/10000], Loss: 0.2008, Train Acc: 0.9550, \n",
      "Epoch [7300/10000], Loss: 0.2007, Train Acc: 0.9550, \n",
      "Epoch [7310/10000], Loss: 0.2005, Train Acc: 0.9550, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████████████████▉       | 7368/10000 [00:16<00:05, 467.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7320/10000], Loss: 0.2004, Train Acc: 0.9551, \n",
      "Epoch [7330/10000], Loss: 0.2002, Train Acc: 0.9551, \n",
      "Epoch [7340/10000], Loss: 0.2000, Train Acc: 0.9552, \n",
      "Epoch [7350/10000], Loss: 0.1999, Train Acc: 0.9553, \n",
      "Epoch [7360/10000], Loss: 0.1997, Train Acc: 0.9554, \n",
      "Epoch [7370/10000], Loss: 0.1996, Train Acc: 0.9555, \n",
      "Epoch [7380/10000], Loss: 0.1994, Train Acc: 0.9555, \n",
      "Epoch [7390/10000], Loss: 0.1992, Train Acc: 0.9555, \n",
      "Epoch [7400/10000], Loss: 0.1991, Train Acc: 0.9555, \n",
      "Epoch [7410/10000], Loss: 0.1989, Train Acc: 0.9556, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|████████████████████▎      | 7513/10000 [00:16<00:05, 472.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7420/10000], Loss: 0.1988, Train Acc: 0.9557, \n",
      "Epoch [7430/10000], Loss: 0.1986, Train Acc: 0.9557, \n",
      "Epoch [7440/10000], Loss: 0.1985, Train Acc: 0.9558, \n",
      "Epoch [7450/10000], Loss: 0.1983, Train Acc: 0.9559, \n",
      "Epoch [7460/10000], Loss: 0.1981, Train Acc: 0.9560, \n",
      "Epoch [7470/10000], Loss: 0.1980, Train Acc: 0.9561, \n",
      "Epoch [7480/10000], Loss: 0.1978, Train Acc: 0.9562, \n",
      "Epoch [7490/10000], Loss: 0.1977, Train Acc: 0.9562, \n",
      "Epoch [7500/10000], Loss: 0.1975, Train Acc: 0.9563, \n",
      "Epoch [7510/10000], Loss: 0.1974, Train Acc: 0.9563, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████████████▌      | 7610/10000 [00:16<00:05, 472.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7520/10000], Loss: 0.1972, Train Acc: 0.9564, \n",
      "Epoch [7530/10000], Loss: 0.1970, Train Acc: 0.9564, \n",
      "Epoch [7540/10000], Loss: 0.1969, Train Acc: 0.9565, \n",
      "Epoch [7550/10000], Loss: 0.1967, Train Acc: 0.9566, \n",
      "Epoch [7560/10000], Loss: 0.1966, Train Acc: 0.9567, \n",
      "Epoch [7570/10000], Loss: 0.1964, Train Acc: 0.9567, \n",
      "Epoch [7580/10000], Loss: 0.1963, Train Acc: 0.9568, \n",
      "Epoch [7590/10000], Loss: 0.1961, Train Acc: 0.9568, \n",
      "Epoch [7600/10000], Loss: 0.1959, Train Acc: 0.9569, \n",
      "Epoch [7610/10000], Loss: 0.1958, Train Acc: 0.9570, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|████████████████████▊      | 7706/10000 [00:17<00:04, 472.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7620/10000], Loss: 0.1956, Train Acc: 0.9571, \n",
      "Epoch [7630/10000], Loss: 0.1955, Train Acc: 0.9571, \n",
      "Epoch [7640/10000], Loss: 0.1953, Train Acc: 0.9572, \n",
      "Epoch [7650/10000], Loss: 0.1952, Train Acc: 0.9572, \n",
      "Epoch [7660/10000], Loss: 0.1950, Train Acc: 0.9572, \n",
      "Epoch [7670/10000], Loss: 0.1949, Train Acc: 0.9573, \n",
      "Epoch [7680/10000], Loss: 0.1947, Train Acc: 0.9573, \n",
      "Epoch [7690/10000], Loss: 0.1946, Train Acc: 0.9574, \n",
      "Epoch [7700/10000], Loss: 0.1944, Train Acc: 0.9575, \n",
      "Epoch [7710/10000], Loss: 0.1943, Train Acc: 0.9576, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|█████████████████████      | 7801/10000 [00:17<00:04, 461.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7720/10000], Loss: 0.1941, Train Acc: 0.9576, \n",
      "Epoch [7730/10000], Loss: 0.1939, Train Acc: 0.9576, \n",
      "Epoch [7740/10000], Loss: 0.1938, Train Acc: 0.9577, \n",
      "Epoch [7750/10000], Loss: 0.1936, Train Acc: 0.9577, \n",
      "Epoch [7760/10000], Loss: 0.1935, Train Acc: 0.9579, \n",
      "Epoch [7770/10000], Loss: 0.1933, Train Acc: 0.9579, \n",
      "Epoch [7780/10000], Loss: 0.1932, Train Acc: 0.9580, \n",
      "Epoch [7790/10000], Loss: 0.1930, Train Acc: 0.9581, \n",
      "Epoch [7800/10000], Loss: 0.1929, Train Acc: 0.9581, \n",
      "Epoch [7810/10000], Loss: 0.1927, Train Acc: 0.9581, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|█████████████████████▎     | 7895/10000 [00:17<00:04, 460.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7820/10000], Loss: 0.1926, Train Acc: 0.9581, \n",
      "Epoch [7830/10000], Loss: 0.1924, Train Acc: 0.9583, \n",
      "Epoch [7840/10000], Loss: 0.1923, Train Acc: 0.9583, \n",
      "Epoch [7850/10000], Loss: 0.1921, Train Acc: 0.9584, \n",
      "Epoch [7860/10000], Loss: 0.1920, Train Acc: 0.9585, \n",
      "Epoch [7870/10000], Loss: 0.1918, Train Acc: 0.9586, \n",
      "Epoch [7880/10000], Loss: 0.1917, Train Acc: 0.9587, \n",
      "Epoch [7890/10000], Loss: 0.1915, Train Acc: 0.9588, \n",
      "Epoch [7900/10000], Loss: 0.1914, Train Acc: 0.9588, \n",
      "Epoch [7910/10000], Loss: 0.1912, Train Acc: 0.9589, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████████████▌     | 7989/10000 [00:17<00:04, 458.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7920/10000], Loss: 0.1911, Train Acc: 0.9590, \n",
      "Epoch [7930/10000], Loss: 0.1909, Train Acc: 0.9591, \n",
      "Epoch [7940/10000], Loss: 0.1908, Train Acc: 0.9592, \n",
      "Epoch [7950/10000], Loss: 0.1906, Train Acc: 0.9593, \n",
      "Epoch [7960/10000], Loss: 0.1905, Train Acc: 0.9593, \n",
      "Epoch [7970/10000], Loss: 0.1903, Train Acc: 0.9595, \n",
      "Epoch [7980/10000], Loss: 0.1902, Train Acc: 0.9595, \n",
      "Epoch [7990/10000], Loss: 0.1900, Train Acc: 0.9597, \n",
      "Epoch [8000/10000], Loss: 0.1899, Train Acc: 0.9598, \n",
      "Epoch [8010/10000], Loss: 0.1897, Train Acc: 0.9600, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|█████████████████████▊     | 8082/10000 [00:17<00:04, 460.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8020/10000], Loss: 0.1896, Train Acc: 0.9601, \n",
      "Epoch [8030/10000], Loss: 0.1894, Train Acc: 0.9601, \n",
      "Epoch [8040/10000], Loss: 0.1893, Train Acc: 0.9602, \n",
      "Epoch [8050/10000], Loss: 0.1891, Train Acc: 0.9602, \n",
      "Epoch [8060/10000], Loss: 0.1890, Train Acc: 0.9603, \n",
      "Epoch [8070/10000], Loss: 0.1889, Train Acc: 0.9605, \n",
      "Epoch [8080/10000], Loss: 0.1887, Train Acc: 0.9605, \n",
      "Epoch [8090/10000], Loss: 0.1886, Train Acc: 0.9606, \n",
      "Epoch [8100/10000], Loss: 0.1884, Train Acc: 0.9608, \n",
      "Epoch [8110/10000], Loss: 0.1883, Train Acc: 0.9608, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|██████████████████████     | 8175/10000 [00:18<00:04, 454.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8120/10000], Loss: 0.1881, Train Acc: 0.9609, \n",
      "Epoch [8130/10000], Loss: 0.1880, Train Acc: 0.9611, \n",
      "Epoch [8140/10000], Loss: 0.1878, Train Acc: 0.9612, \n",
      "Epoch [8150/10000], Loss: 0.1877, Train Acc: 0.9614, \n",
      "Epoch [8160/10000], Loss: 0.1875, Train Acc: 0.9614, \n",
      "Epoch [8170/10000], Loss: 0.1874, Train Acc: 0.9615, \n",
      "Epoch [8180/10000], Loss: 0.1872, Train Acc: 0.9616, \n",
      "Epoch [8190/10000], Loss: 0.1871, Train Acc: 0.9616, \n",
      "Epoch [8200/10000], Loss: 0.1870, Train Acc: 0.9616, \n",
      "Epoch [8210/10000], Loss: 0.1868, Train Acc: 0.9617, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|██████████████████████▎    | 8269/10000 [00:18<00:03, 458.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8220/10000], Loss: 0.1867, Train Acc: 0.9617, \n",
      "Epoch [8230/10000], Loss: 0.1865, Train Acc: 0.9618, \n",
      "Epoch [8240/10000], Loss: 0.1864, Train Acc: 0.9619, \n",
      "Epoch [8250/10000], Loss: 0.1862, Train Acc: 0.9619, \n",
      "Epoch [8260/10000], Loss: 0.1861, Train Acc: 0.9620, \n",
      "Epoch [8270/10000], Loss: 0.1860, Train Acc: 0.9621, \n",
      "Epoch [8280/10000], Loss: 0.1858, Train Acc: 0.9621, \n",
      "Epoch [8290/10000], Loss: 0.1857, Train Acc: 0.9621, \n",
      "Epoch [8300/10000], Loss: 0.1855, Train Acc: 0.9621, \n",
      "Epoch [8310/10000], Loss: 0.1854, Train Acc: 0.9622, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████████████▋    | 8411/10000 [00:18<00:03, 464.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8320/10000], Loss: 0.1852, Train Acc: 0.9623, \n",
      "Epoch [8330/10000], Loss: 0.1851, Train Acc: 0.9624, \n",
      "Epoch [8340/10000], Loss: 0.1850, Train Acc: 0.9624, \n",
      "Epoch [8350/10000], Loss: 0.1848, Train Acc: 0.9625, \n",
      "Epoch [8360/10000], Loss: 0.1847, Train Acc: 0.9626, \n",
      "Epoch [8370/10000], Loss: 0.1845, Train Acc: 0.9626, \n",
      "Epoch [8380/10000], Loss: 0.1844, Train Acc: 0.9627, \n",
      "Epoch [8390/10000], Loss: 0.1842, Train Acc: 0.9627, \n",
      "Epoch [8400/10000], Loss: 0.1841, Train Acc: 0.9627, \n",
      "Epoch [8410/10000], Loss: 0.1840, Train Acc: 0.9627, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|██████████████████████▉    | 8508/10000 [00:18<00:03, 472.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8420/10000], Loss: 0.1838, Train Acc: 0.9628, \n",
      "Epoch [8430/10000], Loss: 0.1837, Train Acc: 0.9628, \n",
      "Epoch [8440/10000], Loss: 0.1835, Train Acc: 0.9628, \n",
      "Epoch [8450/10000], Loss: 0.1834, Train Acc: 0.9629, \n",
      "Epoch [8460/10000], Loss: 0.1833, Train Acc: 0.9629, \n",
      "Epoch [8470/10000], Loss: 0.1831, Train Acc: 0.9629, \n",
      "Epoch [8480/10000], Loss: 0.1830, Train Acc: 0.9630, \n",
      "Epoch [8490/10000], Loss: 0.1828, Train Acc: 0.9630, \n",
      "Epoch [8500/10000], Loss: 0.1827, Train Acc: 0.9631, \n",
      "Epoch [8510/10000], Loss: 0.1826, Train Acc: 0.9632, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|███████████████████████▏   | 8605/10000 [00:19<00:02, 475.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8520/10000], Loss: 0.1824, Train Acc: 0.9633, \n",
      "Epoch [8530/10000], Loss: 0.1823, Train Acc: 0.9635, \n",
      "Epoch [8540/10000], Loss: 0.1821, Train Acc: 0.9635, \n",
      "Epoch [8550/10000], Loss: 0.1820, Train Acc: 0.9636, \n",
      "Epoch [8560/10000], Loss: 0.1819, Train Acc: 0.9637, \n",
      "Epoch [8570/10000], Loss: 0.1817, Train Acc: 0.9637, \n",
      "Epoch [8580/10000], Loss: 0.1816, Train Acc: 0.9637, \n",
      "Epoch [8590/10000], Loss: 0.1814, Train Acc: 0.9638, \n",
      "Epoch [8600/10000], Loss: 0.1813, Train Acc: 0.9639, \n",
      "Epoch [8610/10000], Loss: 0.1812, Train Acc: 0.9639, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|███████████████████████▍   | 8701/10000 [00:19<00:02, 469.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8620/10000], Loss: 0.1810, Train Acc: 0.9640, \n",
      "Epoch [8630/10000], Loss: 0.1809, Train Acc: 0.9642, \n",
      "Epoch [8640/10000], Loss: 0.1808, Train Acc: 0.9642, \n",
      "Epoch [8650/10000], Loss: 0.1806, Train Acc: 0.9642, \n",
      "Epoch [8660/10000], Loss: 0.1805, Train Acc: 0.9643, \n",
      "Epoch [8670/10000], Loss: 0.1803, Train Acc: 0.9643, \n",
      "Epoch [8680/10000], Loss: 0.1802, Train Acc: 0.9643, \n",
      "Epoch [8690/10000], Loss: 0.1801, Train Acc: 0.9644, \n",
      "Epoch [8700/10000], Loss: 0.1799, Train Acc: 0.9645, \n",
      "Epoch [8710/10000], Loss: 0.1798, Train Acc: 0.9645, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|███████████████████████▋   | 8796/10000 [00:19<00:02, 467.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8720/10000], Loss: 0.1797, Train Acc: 0.9646, \n",
      "Epoch [8730/10000], Loss: 0.1795, Train Acc: 0.9647, \n",
      "Epoch [8740/10000], Loss: 0.1794, Train Acc: 0.9647, \n",
      "Epoch [8750/10000], Loss: 0.1793, Train Acc: 0.9648, \n",
      "Epoch [8760/10000], Loss: 0.1791, Train Acc: 0.9649, \n",
      "Epoch [8770/10000], Loss: 0.1790, Train Acc: 0.9649, \n",
      "Epoch [8780/10000], Loss: 0.1788, Train Acc: 0.9650, \n",
      "Epoch [8790/10000], Loss: 0.1787, Train Acc: 0.9650, \n",
      "Epoch [8800/10000], Loss: 0.1786, Train Acc: 0.9651, \n",
      "Epoch [8810/10000], Loss: 0.1784, Train Acc: 0.9652, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████████████████████   | 8893/10000 [00:19<00:02, 474.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8820/10000], Loss: 0.1783, Train Acc: 0.9652, \n",
      "Epoch [8830/10000], Loss: 0.1782, Train Acc: 0.9652, \n",
      "Epoch [8840/10000], Loss: 0.1780, Train Acc: 0.9654, \n",
      "Epoch [8850/10000], Loss: 0.1779, Train Acc: 0.9654, \n",
      "Epoch [8860/10000], Loss: 0.1778, Train Acc: 0.9654, \n",
      "Epoch [8870/10000], Loss: 0.1776, Train Acc: 0.9654, \n",
      "Epoch [8880/10000], Loss: 0.1775, Train Acc: 0.9654, \n",
      "Epoch [8890/10000], Loss: 0.1774, Train Acc: 0.9655, \n",
      "Epoch [8900/10000], Loss: 0.1772, Train Acc: 0.9655, \n",
      "Epoch [8910/10000], Loss: 0.1771, Train Acc: 0.9656, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████████████████████▎  | 8989/10000 [00:19<00:02, 474.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8920/10000], Loss: 0.1770, Train Acc: 0.9656, \n",
      "Epoch [8930/10000], Loss: 0.1768, Train Acc: 0.9657, \n",
      "Epoch [8940/10000], Loss: 0.1767, Train Acc: 0.9657, \n",
      "Epoch [8950/10000], Loss: 0.1766, Train Acc: 0.9657, \n",
      "Epoch [8960/10000], Loss: 0.1764, Train Acc: 0.9659, \n",
      "Epoch [8970/10000], Loss: 0.1763, Train Acc: 0.9659, \n",
      "Epoch [8980/10000], Loss: 0.1762, Train Acc: 0.9660, \n",
      "Epoch [8990/10000], Loss: 0.1760, Train Acc: 0.9660, \n",
      "Epoch [9000/10000], Loss: 0.1759, Train Acc: 0.9661, \n",
      "Epoch [9010/10000], Loss: 0.1758, Train Acc: 0.9662, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|████████████████████████▌  | 9084/10000 [00:20<00:01, 465.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9020/10000], Loss: 0.1756, Train Acc: 0.9662, \n",
      "Epoch [9030/10000], Loss: 0.1755, Train Acc: 0.9663, \n",
      "Epoch [9040/10000], Loss: 0.1754, Train Acc: 0.9663, \n",
      "Epoch [9050/10000], Loss: 0.1752, Train Acc: 0.9664, \n",
      "Epoch [9060/10000], Loss: 0.1751, Train Acc: 0.9664, \n",
      "Epoch [9070/10000], Loss: 0.1750, Train Acc: 0.9665, \n",
      "Epoch [9080/10000], Loss: 0.1749, Train Acc: 0.9665, \n",
      "Epoch [9090/10000], Loss: 0.1747, Train Acc: 0.9665, \n",
      "Epoch [9100/10000], Loss: 0.1746, Train Acc: 0.9667, \n",
      "Epoch [9110/10000], Loss: 0.1745, Train Acc: 0.9667, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|████████████████████████▊  | 9180/10000 [00:20<00:01, 467.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9120/10000], Loss: 0.1743, Train Acc: 0.9667, \n",
      "Epoch [9130/10000], Loss: 0.1742, Train Acc: 0.9668, \n",
      "Epoch [9140/10000], Loss: 0.1741, Train Acc: 0.9669, \n",
      "Epoch [9150/10000], Loss: 0.1739, Train Acc: 0.9670, \n",
      "Epoch [9160/10000], Loss: 0.1738, Train Acc: 0.9670, \n",
      "Epoch [9170/10000], Loss: 0.1737, Train Acc: 0.9671, \n",
      "Epoch [9180/10000], Loss: 0.1736, Train Acc: 0.9671, \n",
      "Epoch [9190/10000], Loss: 0.1734, Train Acc: 0.9671, \n",
      "Epoch [9200/10000], Loss: 0.1733, Train Acc: 0.9672, \n",
      "Epoch [9210/10000], Loss: 0.1732, Train Acc: 0.9672, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████████████████████  | 9274/10000 [00:20<00:01, 466.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9220/10000], Loss: 0.1730, Train Acc: 0.9673, \n",
      "Epoch [9230/10000], Loss: 0.1729, Train Acc: 0.9674, \n",
      "Epoch [9240/10000], Loss: 0.1728, Train Acc: 0.9674, \n",
      "Epoch [9250/10000], Loss: 0.1726, Train Acc: 0.9675, \n",
      "Epoch [9260/10000], Loss: 0.1725, Train Acc: 0.9675, \n",
      "Epoch [9270/10000], Loss: 0.1724, Train Acc: 0.9676, \n",
      "Epoch [9280/10000], Loss: 0.1723, Train Acc: 0.9676, \n",
      "Epoch [9290/10000], Loss: 0.1721, Train Acc: 0.9677, \n",
      "Epoch [9300/10000], Loss: 0.1720, Train Acc: 0.9678, \n",
      "Epoch [9310/10000], Loss: 0.1719, Train Acc: 0.9678, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████████████████████▎ | 9369/10000 [00:20<00:01, 465.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9320/10000], Loss: 0.1718, Train Acc: 0.9679, \n",
      "Epoch [9330/10000], Loss: 0.1716, Train Acc: 0.9680, \n",
      "Epoch [9340/10000], Loss: 0.1715, Train Acc: 0.9680, \n",
      "Epoch [9350/10000], Loss: 0.1714, Train Acc: 0.9681, \n",
      "Epoch [9360/10000], Loss: 0.1712, Train Acc: 0.9681, \n",
      "Epoch [9370/10000], Loss: 0.1711, Train Acc: 0.9682, \n",
      "Epoch [9380/10000], Loss: 0.1710, Train Acc: 0.9682, \n",
      "Epoch [9390/10000], Loss: 0.1709, Train Acc: 0.9684, \n",
      "Epoch [9400/10000], Loss: 0.1707, Train Acc: 0.9685, \n",
      "Epoch [9410/10000], Loss: 0.1706, Train Acc: 0.9685, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████████████████████▋ | 9514/10000 [00:21<00:01, 475.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9420/10000], Loss: 0.1705, Train Acc: 0.9686, \n",
      "Epoch [9430/10000], Loss: 0.1704, Train Acc: 0.9686, \n",
      "Epoch [9440/10000], Loss: 0.1702, Train Acc: 0.9687, \n",
      "Epoch [9450/10000], Loss: 0.1701, Train Acc: 0.9687, \n",
      "Epoch [9460/10000], Loss: 0.1700, Train Acc: 0.9689, \n",
      "Epoch [9470/10000], Loss: 0.1699, Train Acc: 0.9689, \n",
      "Epoch [9480/10000], Loss: 0.1697, Train Acc: 0.9690, \n",
      "Epoch [9490/10000], Loss: 0.1696, Train Acc: 0.9690, \n",
      "Epoch [9500/10000], Loss: 0.1695, Train Acc: 0.9690, \n",
      "Epoch [9510/10000], Loss: 0.1694, Train Acc: 0.9691, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████████████████████▉ | 9611/10000 [00:21<00:00, 473.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9520/10000], Loss: 0.1692, Train Acc: 0.9691, \n",
      "Epoch [9530/10000], Loss: 0.1691, Train Acc: 0.9691, \n",
      "Epoch [9540/10000], Loss: 0.1690, Train Acc: 0.9692, \n",
      "Epoch [9550/10000], Loss: 0.1689, Train Acc: 0.9693, \n",
      "Epoch [9560/10000], Loss: 0.1687, Train Acc: 0.9693, \n",
      "Epoch [9570/10000], Loss: 0.1686, Train Acc: 0.9694, \n",
      "Epoch [9580/10000], Loss: 0.1685, Train Acc: 0.9694, \n",
      "Epoch [9590/10000], Loss: 0.1684, Train Acc: 0.9694, \n",
      "Epoch [9600/10000], Loss: 0.1682, Train Acc: 0.9695, \n",
      "Epoch [9610/10000], Loss: 0.1681, Train Acc: 0.9695, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|██████████████████████████▏| 9707/10000 [00:21<00:00, 473.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9620/10000], Loss: 0.1680, Train Acc: 0.9696, \n",
      "Epoch [9630/10000], Loss: 0.1679, Train Acc: 0.9697, \n",
      "Epoch [9640/10000], Loss: 0.1678, Train Acc: 0.9697, \n",
      "Epoch [9650/10000], Loss: 0.1676, Train Acc: 0.9698, \n",
      "Epoch [9660/10000], Loss: 0.1675, Train Acc: 0.9699, \n",
      "Epoch [9670/10000], Loss: 0.1674, Train Acc: 0.9699, \n",
      "Epoch [9680/10000], Loss: 0.1673, Train Acc: 0.9699, \n",
      "Epoch [9690/10000], Loss: 0.1671, Train Acc: 0.9700, \n",
      "Epoch [9700/10000], Loss: 0.1670, Train Acc: 0.9701, \n",
      "Epoch [9710/10000], Loss: 0.1669, Train Acc: 0.9701, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|██████████████████████████▍| 9803/10000 [00:21<00:00, 469.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9720/10000], Loss: 0.1668, Train Acc: 0.9703, \n",
      "Epoch [9730/10000], Loss: 0.1667, Train Acc: 0.9704, \n",
      "Epoch [9740/10000], Loss: 0.1665, Train Acc: 0.9704, \n",
      "Epoch [9750/10000], Loss: 0.1664, Train Acc: 0.9704, \n",
      "Epoch [9760/10000], Loss: 0.1663, Train Acc: 0.9705, \n",
      "Epoch [9770/10000], Loss: 0.1662, Train Acc: 0.9706, \n",
      "Epoch [9780/10000], Loss: 0.1661, Train Acc: 0.9706, \n",
      "Epoch [9790/10000], Loss: 0.1659, Train Acc: 0.9706, \n",
      "Epoch [9800/10000], Loss: 0.1658, Train Acc: 0.9706, \n",
      "Epoch [9810/10000], Loss: 0.1657, Train Acc: 0.9707, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|██████████████████████████▋| 9898/10000 [00:21<00:00, 465.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9820/10000], Loss: 0.1656, Train Acc: 0.9708, \n",
      "Epoch [9830/10000], Loss: 0.1655, Train Acc: 0.9709, \n",
      "Epoch [9840/10000], Loss: 0.1653, Train Acc: 0.9709, \n",
      "Epoch [9850/10000], Loss: 0.1652, Train Acc: 0.9709, \n",
      "Epoch [9860/10000], Loss: 0.1651, Train Acc: 0.9710, \n",
      "Epoch [9870/10000], Loss: 0.1650, Train Acc: 0.9710, \n",
      "Epoch [9880/10000], Loss: 0.1649, Train Acc: 0.9711, \n",
      "Epoch [9890/10000], Loss: 0.1647, Train Acc: 0.9711, \n",
      "Epoch [9900/10000], Loss: 0.1646, Train Acc: 0.9712, \n",
      "Epoch [9910/10000], Loss: 0.1645, Train Acc: 0.9713, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████| 10000/10000 [00:22<00:00, 453.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9920/10000], Loss: 0.1644, Train Acc: 0.9713, \n",
      "Epoch [9930/10000], Loss: 0.1643, Train Acc: 0.9714, \n",
      "Epoch [9940/10000], Loss: 0.1641, Train Acc: 0.9715, \n",
      "Epoch [9950/10000], Loss: 0.1640, Train Acc: 0.9715, \n",
      "Epoch [9960/10000], Loss: 0.1639, Train Acc: 0.9716, \n",
      "Epoch [9970/10000], Loss: 0.1638, Train Acc: 0.9716, \n",
      "Epoch [9980/10000], Loss: 0.1637, Train Acc: 0.9716, \n",
      "Epoch [9990/10000], Loss: 0.1635, Train Acc: 0.9718, \n",
      "Epoch [10000/10000], Loss: 0.1634, Train Acc: 0.9718, \n",
      "<generator object Module.parameters at 0x30ac79fc0>\n",
      "[0 1]\n",
      "[0. 1.]\n",
      "accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dqs, accs = {}, {}\n",
    "float_preds_list, preds_list = [], []\n",
    "# 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5,\n",
    "n = X_train.shape[0]\n",
    "# k = 1\n",
    "Lamb = 0.0001\n",
    "# [0.1, 0.3, 0.5, 0.7, 0.9, 1, 2, 3, 5, 10]\n",
    "# 0.1, 0.5, 1, 3, 5, 10, \n",
    "for eps_p in [np.inf]:\n",
    "    dq_list, acc_list = [], []\n",
    "    for _ in range(1):\n",
    "\n",
    "\n",
    "#         model = twostg_train_model(X_train, y_train, eps_p, \\\n",
    "#                                    lr=best_params['learning_rate'], \\\n",
    "#                                    weight_decay=best_params['weight_decay'], \\\n",
    "#                                    epochs=100)\n",
    "\n",
    "        model_trained = twostg_train_model(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                eps_p=eps_p,\n",
    "                lr=1e-3,\n",
    "                epochs=10000,\n",
    "                Lamb=Lamb,\n",
    "            )\n",
    "    \n",
    "        print(model_trained.parameters())\n",
    "    \n",
    "        X_test, y_test = X_test[:1000], y_test[:1000]\n",
    "        print(np.unique(y_test))\n",
    "\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            # predicted = model(X_test_tensor)\n",
    "            # predicted_labels = (predicted > 0.5).float() # Convert probabilities to binary predictions\n",
    "            # accuracy = (predicted_labels == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            # print(f'Eps_p = {eps_p} -- Accuracy on test set: {accuracy:.4f}')\n",
    "\n",
    "            y_predicted = model_trained(X_test_tensor)\n",
    "#                 X_test_tensor, theta_trained.float())\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            print(np.unique(y_predicted_cls))\n",
    "            acc = y_predicted_cls.eq(y_test_tensor).sum() / float(y_test_tensor.shape[0])\n",
    "            acc_list.append(acc)\n",
    "            print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "\n",
    "#         print(y_test_tensor)\n",
    "        # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "        preds_list.append(np.array(y_predicted_cls))\n",
    "        float_preds_list.append(np.array(y_predicted))\n",
    "        \n",
    "#         print(y_predicted_cls)\n",
    "\n",
    "#         z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "#         dq = np.dot(z_star.flatten(), y_test)\n",
    "#         dq_list.append(dq)\n",
    "        \n",
    "        \n",
    "#     dqs[f'{eps_p}'] = np.mean(dq_list)\n",
    "#     accs[f'{eps_p}'] = np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "def run_opt(y_hat):\n",
    "\n",
    "#     y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "    newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "    print(newn)\n",
    "\n",
    "    model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "    # Create variables z\n",
    "    z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "#     abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "\n",
    "    # Define the auxiliary variables t\n",
    "#     t = model.addMVar(newn, lb=0, ub=GRB.INFINITY, name=\"t\")\n",
    "\n",
    "#     # Add constraints to enforce t_i >= |z_i|\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(t[i] >= z[i], name=f\"abs_constr1_{i}\")\n",
    "#         model.addConstr(t[i] >= -z[i], name=f\"abs_constr2_{i}\")\n",
    "\n",
    "#     # Add the budget constraint\n",
    "    B = newn // 2  # Replace with your desired budget value\n",
    "#     model.addConstr(t.sum() <= B, name=\"L1_norm_constraint\")\n",
    "\n",
    "    model.addConstr(z.sum() <= B, name=\"sum_constraint\")\n",
    "    \n",
    "    \n",
    "    # h = h.flatten()\n",
    "    # print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "    # print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "    o, p = np.identity(newn), -np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    # h = np.ones(G.shape[0])\n",
    "    # h = h.reshape(-1,1)\n",
    "\n",
    "    h_1, h_2 = np.ones(newn), np.zeros(newn)\n",
    "    h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "    h = np.concatenate((h_1, h_2), axis=0)\n",
    "#     print(h.shape)\n",
    "\n",
    "\n",
    "    print(G.shape)\n",
    "    print(z.shape)\n",
    "    print(h.shape)\n",
    "    \n",
    "    z = z.reshape(-1, 1)\n",
    "    \n",
    "    model.addConstr(G @ z <= h)\n",
    "\n",
    "#     print(y_hat.shape)\n",
    "#     print(z.shape)\n",
    "    y_hat = y_hat.flatten()\n",
    "    \n",
    "    dot_product = gp.quicksum(y_hat[i] * z[i] for i in range(len(y_hat)))\n",
    "    \n",
    "\n",
    "    model.setObjective(dot_product, GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(abs_z[i] <= z[i], name=f\"abs_z_pos_{i}\")\n",
    "#         model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "#     model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "    # model.addConstr(G_1 @ z <= h_1)\n",
    "    # model.addConstr(G_2 @ z <= h_2)\n",
    "    # model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        z_star_test = z.X\n",
    "        \n",
    "    return z_star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([204.])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_predicted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([240.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(2000, 1000)\n",
      "(1000,)\n",
      "(2000, 1)\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 24.2.0 24C101)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 2001 rows, 1000 columns and 3000 nonzeros\n",
      "Model fingerprint: 0x028c7254\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 5e+02]\n",
      "Presolve removed 2001 rows and 1000 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    2.0400000e+02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  2.040000000e+02\n"
     ]
    }
   ],
   "source": [
    "z_star = run_opt(np.array(y_predicted_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq = np.dot(z_star.flatten(), y_test)\n",
    "dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_loss(linear_output, X_train, y_train, Lamb):\n",
    "\n",
    "    eps_p = np.inf\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    b = np.random.gamma(d, scale=1.0 / eta, size=(d, 1))\n",
    "    b = torch.Tensor(b.reshape(1, -1))\n",
    "    \n",
    "\n",
    "#     z = torch.dot(w.T,x_train.T)\n",
    "#     y_head = sigmoid(z)\n",
    "    #     print(y_head.shape)\n",
    "    pert = torch.dot(b.flatten(), linear_output.flatten())\n",
    "    \n",
    "    log_likelihood = - y_train * torch.log(sigmoid(linear_output)) - (1 - y_train) * torch.log(1 - sigmoid(linear_output))\n",
    "    l2_reg = (lambda_reg + delta_reg) * torch.sum(model.linear.weight ** 2)\n",
    "    \n",
    "    return log_likelihood + l2_reg + pert_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "Lamb = n\n",
    "\n",
    "model = LogisticRegression(d,1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    linear_output = model(X_train_tensor)\n",
    "\n",
    "\n",
    "    loss = two_stage_loss(linear_output, X_train_tensor, y_train_tensor, Lamb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(x_train,y_train,eps_p=np.inf):\n",
    "    \n",
    "    \n",
    "    n, d = x_train.shape[0], x_train.shape[1]\n",
    "    \n",
    "    w = torch.randn(size=(d,1))\n",
    "    \n",
    "    k = 1\n",
    "    Lamb = k * n\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "    b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "    # forward propagation\n",
    "#     b = torch.FloatTensor(b)\n",
    "    \n",
    "    z = torch.dot(w.T,x_train.T)\n",
    "    y_head = sigmoid(z)\n",
    "#     print(y_head.shape)\n",
    "    pert = np.dot(b.T, w)\n",
    "#     print(pert.shape)\n",
    "    \n",
    "#     print(y_head.shape)\n",
    "\n",
    "    loss_term = np.dot(y_train, np.log(y_head.T)) + np.dot((1-y_train), np.log(1-y_head.T))\n",
    "    print(loss_term)\n",
    "    \n",
    "    loss = loss_term \\\n",
    "            + pert \\\n",
    "            + ((Lamb + Delta) * (np.linalg.norm(w, ord=2)**2))\n",
    "            \n",
    "    cost = (np.sum(loss)) #     /x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    ce_grad = np.dot(x_train.T,((y_head-y_train)).T) / n\n",
    "#     print(ce_grad)\n",
    "    b_grad = b\n",
    "#     print(b_grad.shape)\n",
    "    reg_grad = 2 * (Lamb + Delta) * w\n",
    "\n",
    "#     print(reg_grad)\n",
    "#     print(reg_grad.shape)\n",
    "    total_grads = ce_grad + b_grad + reg_grad # x_train.shape[1]  is for scaling\n",
    "#     derivative_bias = np.sum(y_head-y_train)/x_train.shape[0]    # x_train.shape[1]  is for scaling\n",
    "#     print(grads)\n",
    "    \n",
    "    if np.any(total_grads):  # Check if any non-zero gradients\n",
    "        grad_min = total_grads.min()\n",
    "        grad_max = total_grads.max()\n",
    "\n",
    "        # Avoid division by zero in scaling\n",
    "        if grad_max != grad_min:\n",
    "            scaled_gradient = (total_grads - grad_min) / (grad_max - grad_min)\n",
    "        else:\n",
    "            scaled_gradient = np.zeros_like(total_grads)\n",
    "    else:\n",
    "        scaled_gradient = total_grads\n",
    "    \n",
    "#     total_grads = np.clip(total_grads, 0, 1)\n",
    "#     print(total_grads)\n",
    "    gradients = {\"gradients\": scaled_gradient}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,x_train,y_train,eps_p=np.inf):\n",
    "    \n",
    "    \n",
    "    n, d = x_train.shape[0], x_train.shape[1]\n",
    "    \n",
    "    k = 1\n",
    "    Lamb = k * n\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "    b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train.T)\n",
    "    y_head = sigmoid(z)\n",
    "#     print(y_head.shape)\n",
    "    pert = np.dot(b.T, w)\n",
    "#     print(pert.shape)\n",
    "    \n",
    "#     print(y_head.shape)\n",
    "\n",
    "    loss_term = np.dot(y_train, np.log(y_head.T)) + np.dot((1-y_train), np.log(1-y_head.T))\n",
    "    print(loss_term)\n",
    "    \n",
    "    loss = - loss_term \\\n",
    "            + pert \\\n",
    "            + ((Lamb + Delta) * (np.linalg.norm(w, ord=2)**2))\n",
    "            \n",
    "    cost = (np.sum(loss)) #     /x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    ce_grad = np.dot(x_train.T,((y_head-y_train)).T) / n\n",
    "#     print(ce_grad)\n",
    "    b_grad = b\n",
    "#     print(b_grad.shape)\n",
    "    reg_grad = 2 * (Lamb + Delta) * w\n",
    "\n",
    "#     print(reg_grad)\n",
    "#     print(reg_grad.shape)\n",
    "    total_grads = ce_grad + b_grad + reg_grad # x_train.shape[1]  is for scaling\n",
    "#     derivative_bias = np.sum(y_head-y_train)/x_train.shape[0]    # x_train.shape[1]  is for scaling\n",
    "#     print(grads)\n",
    "    \n",
    "    if np.any(total_grads):  # Check if any non-zero gradients\n",
    "        grad_min = total_grads.min()\n",
    "        grad_max = total_grads.max()\n",
    "\n",
    "        # Avoid division by zero in scaling\n",
    "        if grad_max != grad_min:\n",
    "            scaled_gradient = (total_grads - grad_min) / (grad_max - grad_min)\n",
    "        else:\n",
    "            scaled_gradient = np.zeros_like(total_grads)\n",
    "    else:\n",
    "        scaled_gradient = total_grads\n",
    "    \n",
    "#     total_grads = np.clip(total_grads, 0, 1)\n",
    "#     print(total_grads)\n",
    "    gradients = {\"gradients\": scaled_gradient}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w, x_train, y_train, learning_rate,number_of_iterations):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterations):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - (learning_rate * gradients[\"gradients\"])\n",
    "#         print(gradients['gradients'])\n",
    "#         print(learning_rate * gradients[\"gradients\"])\n",
    "#         b = b - (learning_rate * gradients[\"derivative_bias\"])\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test.T))\n",
    "    Y_prediction = np.zeros((1,x_test.shape[0]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "#     b = 0.0\n",
    "    return w\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[1] \n",
    "    w = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(X_train, y_train, X_test, y_test,learning_rate = 1e-2, num_iterations = 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def theta_closed_form(X_train, y_train):\n",
    "    \n",
    "#     k = 1\n",
    "#     Lamb = k * n\n",
    "    \n",
    "#     n, d = X_train.shape[0], X_train.shape[1]\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1/Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p/4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * n) + (8 * math.sqrt(n)))\n",
    "    \n",
    "    \n",
    "#     b = np.random.gamma(d, scale=1.0/eta, size=(w.shape[0],1))\n",
    "#     # forward propagation\n",
    "#     term = np.dot(y_train - , X_train)\n",
    "    \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # Compute sigmoid: 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def twostg_train_model(X_train, y_train, eps_p, lr, epochs, Lamb, batch_size=128):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    d = X_train.shape[1]\n",
    "    output_dim = 1  # Binary classification\n",
    "#     model = LogisticRegression(d, output_dim)\n",
    "    criterion = nn.BCELoss(reduction='sum')  # Binary Cross Entropy Loss\n",
    "    \n",
    "    \n",
    "    theta_init = torch.randn((d,1),requires_grad=True)\n",
    "    optimizer = optim.Adam([theta_init], lr=lr)\n",
    "    \n",
    "    eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "    if eps_dash_p > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "        eps_dash_p = eps_p / 2\n",
    "\n",
    "    eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "#     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    b = np.random.gamma(d, scale=1.0 / eta, size=(d, 1))\n",
    "    b = torch.Tensor(b.reshape(1, -1))\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc='Training'):\n",
    "#         model.train()\n",
    "        \n",
    "        for (X_batch, y_batch) in data_loader:\n",
    "        \n",
    "    #         for batch_X, batch_y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "    #             outputs = model(batch_X)\n",
    "\n",
    "            y_hat_init = torch.matmul(X_batch, theta_init.float())\n",
    "    # #             print(y_hat_init.shape)\n",
    "            outputs = sigmoid(y_hat_init.flatten())\n",
    "#             outputs = model(X_batch)\n",
    "\n",
    "    #         theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "#             theta = model.get_parameters().view(-1)  # Get current parameters\n",
    "            theta = theta_init\n",
    "\n",
    "    #         print(theta)\n",
    "\n",
    "\n",
    "\n",
    "            # Add perturbation\n",
    "            pert = torch.dot(b.flatten(), theta.flatten())\n",
    "    #         print(pert)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = (\n",
    "                criterion(outputs.flatten(), y_batch.flatten())\n",
    "                + pert\n",
    "                + ((Lamb + Delta) * (torch.norm(theta, p=2) ** 2))\n",
    "            )\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging training loss\n",
    "    #         if (epoch + 1) % 10 == 0:\n",
    "    #             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "    #                 model.eval()\n",
    "                    train_pred = (torch.matmul(X_batch, theta_init.float()) >= 0.5).float()\n",
    "    #                 test_pred = (model(X_test_tensor) >= 0.5).float()\n",
    "                    train_acc = (train_pred == y_batch).float().mean()\n",
    "    #                 test_acc = (test_pred == y_test_tensor).float().mean()\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                          f'Loss: {loss.item():.4f}, '\n",
    "                          f'Train Acc: {train_acc:.4f}, ')\n",
    "    #                       f'Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return theta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fC26Q-Eljl8E"
   },
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "def run_opt(y_hat):\n",
    "\n",
    "#     y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "    newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "    print(newn)\n",
    "\n",
    "    model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "    # Create variables z\n",
    "    z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "#     abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "\n",
    "    # Define the auxiliary variables t\n",
    "#     t = model.addMVar(newn, lb=0, ub=GRB.INFINITY, name=\"t\")\n",
    "\n",
    "#     # Add constraints to enforce t_i >= |z_i|\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(t[i] >= z[i], name=f\"abs_constr1_{i}\")\n",
    "#         model.addConstr(t[i] >= -z[i], name=f\"abs_constr2_{i}\")\n",
    "\n",
    "#     # Add the budget constraint\n",
    "    B = newn // 2  # Replace with your desired budget value\n",
    "#     model.addConstr(t.sum() <= B, name=\"L1_norm_constraint\")\n",
    "\n",
    "    model.addConstr(z.sum() <= B, name=\"sum_constraint\")\n",
    "    \n",
    "    \n",
    "    # h = h.flatten()\n",
    "    # print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "    # print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "    o, p = -np.identity(newn), np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    # h = np.ones(G.shape[0])\n",
    "    # h = h.reshape(-1,1)\n",
    "\n",
    "    h_1, h_2 = np.zeros(newn), np.ones(newn)\n",
    "    h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "    h = np.concatenate((h_1, h_2), axis=0)\n",
    "#     print(h.shape)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model.addConstr(G @ z <= h)\n",
    "\n",
    "#     print(y_hat.shape)\n",
    "#     print(z.shape)\n",
    "\n",
    "    z = z.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    model.setObjective(y_hat.T @ z, GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "#     for i in range(newn):\n",
    "#         model.addConstr(abs_z[i] <= z[i], name=f\"abs_z_pos_{i}\")\n",
    "#         model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "#     model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "    # model.addConstr(G_1 @ z <= h_1)\n",
    "    # model.addConstr(G_2 @ z <= h_2)\n",
    "    # model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        z_star_test = z.X\n",
    "        return z_star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150, fit_intercept=False)\n",
    "print(\"test accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HhZYotVrtNo",
    "outputId": "3fa1b047-0296-4b5f-b0c0-3bf19f047271"
   },
   "outputs": [],
   "source": [
    "dqs, accs = {}, {}\n",
    "float_preds_list, preds_list = [], []\n",
    "# 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5,\n",
    "n = X_train.shape[0]\n",
    "k = 1\n",
    "Lamb = k * n\n",
    "# [0.1, 0.3, 0.5, 0.7, 0.9, 1, 2, 3, 5, 10]\n",
    "# 0.1, 0.5, 1, 3, 5, 10, \n",
    "for eps_p in [np.inf]:\n",
    "    dq_list, acc_list = [], []\n",
    "    for _ in range(1):\n",
    "\n",
    "\n",
    "#         model = twostg_train_model(X_train, y_train, eps_p, \\\n",
    "#                                    lr=best_params['learning_rate'], \\\n",
    "#                                    weight_decay=best_params['weight_decay'], \\\n",
    "#                                    epochs=100)\n",
    "\n",
    "        theta_trained = twostg_train_model(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                eps_p=eps_p,\n",
    "                lr=1e-1,\n",
    "                epochs=150,\n",
    "                Lamb=Lamb,\n",
    "            )\n",
    "    \n",
    "        X_test, y_test = X_test[:1000], y_test[:1000]\n",
    "        print(np.unique(y_test))\n",
    "\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "#         g = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "#         print(g)\n",
    "#         print(g.shape)\n",
    "        \n",
    "#         model.eval()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            # predicted = model(X_test_tensor)\n",
    "            # predicted_labels = (predicted > 0.5).float() # Convert probabilities to binary predictions\n",
    "            # accuracy = (predicted_labels == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "            # print(f'Eps_p = {eps_p} -- Accuracy on test set: {accuracy:.4f}')\n",
    "\n",
    "            y_predicted = torch.matmul(X_test_tensor, theta_trained.float())\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            print(np.unique(y_predicted_cls))\n",
    "            acc = y_predicted_cls.eq(y_test_tensor).sum() / float(y_test_tensor.shape[0])\n",
    "            acc_list.append(acc)\n",
    "            print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "\n",
    "#         print(y_test_tensor)\n",
    "        # dictt[f\"{eps_p}\"] = predicted_labels\n",
    "        preds_list.append(np.array(y_predicted_cls))\n",
    "        float_preds_list.append(np.array(y_predicted))\n",
    "\n",
    "        z_star = run_opt(np.array(y_predicted))\n",
    "\n",
    "        dq = np.dot(z_star.flatten(), y_test)\n",
    "        dq_list.append(dq)\n",
    "        \n",
    "        \n",
    "    dqs[f'{eps_p}'] = np.mean(dq_list)\n",
    "    accs[f'{eps_p}'] = np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(dq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CmPeZ9cr-EQ",
    "outputId": "0e830129-9238-4232-dd23-f8f4c6bc2e02"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, preds_list[5])\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, preds_list[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-wQL9nJo5ji",
    "outputId": "c15e4eba-11cb-4480-9f03-ae339408b9db"
   },
   "outputs": [],
   "source": [
    "test_n = X_test.shape[0]\n",
    "test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJCKDEK9tX_x",
    "outputId": "6192f221-72b1-4f38-f126-d65026337f3b"
   },
   "outputs": [],
   "source": [
    "# o, p = np.identity(test_n), -np.identity(test_n)\n",
    "\n",
    "# G = np.concatenate((o, p), axis=0)\n",
    "# G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_1 = np.identity(test_n)\n",
    "G_2 = -np.identity(test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qk3GicOEu8HM",
    "outputId": "93701894-e5d2-4ce7-989a-abb8dbe6f4a4"
   },
   "outputs": [],
   "source": [
    "G_1.shape, G_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o, p = np.ones(test_n), np.ones(test_n)\n",
    "\n",
    "# h = np.concatenate((o, p), axis=0)\n",
    "# h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK4pYh-Rt9kf",
    "outputId": "320715bf-b336-48d1-ecb8-d383079ddbda"
   },
   "outputs": [],
   "source": [
    "# h_1, h_2 = np.ones(test_n), np.ones(test_n)\n",
    "# h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "# h_1.shape, h_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Optional for sparse matrices\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB  # Ensure GRB is imported\n",
    "\n",
    "\n",
    "# Example data\n",
    "# G = np.random.rand(100, 50)  # A 100x50 matrix\n",
    "# h = np.random.rand(100)      # RHS vector\n",
    "\n",
    "y_hat = float_preds_list[0]  # Predicted values (1D array or list)\n",
    "newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "\n",
    "model = gp.Model(\"matrix_constraints\")\n",
    "\n",
    "# Create variables z\n",
    "z = model.addMVar(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")  # MVar for vectorized variables\n",
    "abs_z = model.addVars(newn, lb=0, name=\"abs_z\")\n",
    "\n",
    "z = z.reshape(-1, 1)\n",
    "\n",
    "print(y_hat.shape)\n",
    "print(z.shape)\n",
    "\n",
    "model.setObjective(y_hat.T @ z, GRB.MAXIMIZE)\n",
    "\n",
    "# h = h.flatten()\n",
    "# print(\"G shape:\", G.shape)  # Should be (m, n)\n",
    "# print(\"h shape:\", h.shape)  # Should be (m,)\n",
    "\n",
    "o, p = np.identity(newn), -np.identity(newn)\n",
    "G = np.concatenate((o, p), axis=0)\n",
    "# h = np.ones(G.shape[0])\n",
    "# h = h.reshape(-1,1)\n",
    "\n",
    "h_1, h_2 = np.ones(test_n), np.zeros(test_n)\n",
    "h_1, h_2 = h_1.reshape(-1,1), h_2.reshape(-1,1)\n",
    "h = np.concatenate((h_1, h_2), axis=0)\n",
    "print(h.shape)\n",
    "\n",
    "B = newn // 2\n",
    "\n",
    "model.addConstr(G @ z <= h)\n",
    "\n",
    "\n",
    "for i in range(newn):\n",
    "    model.addConstr(abs_z[i] >= z[i], name=f\"abs_z_pos_{i}\")\n",
    "    model.addConstr(abs_z[i] >= -z[i], name=f\"abs_z_neg_{i}\")\n",
    "\n",
    "model.addConstr(gp.quicksum(abs_z[i] for i in range(newn)) <= B, name=\"budget_constraint\")\n",
    "\n",
    "# model.addConstr(G_1 @ z <= h_1)\n",
    "# model.addConstr(G_2 @ z <= h_2)\n",
    "# model.addRange(z, -1.0, 1.0, \"range0\")\n",
    "\n",
    "model.optimize()\n",
    "\n",
    "# Check if the model has a feasible solution\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Extract the optimal solution for z\n",
    "    z_star_test = z.X\n",
    "    print(\"Optimal solution z_star:\", z_star_test)\n",
    "else:\n",
    "    print(\"No optimal solution found. Status:\", model.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = np.dot(z_star_test.flatten(), y_test)\n",
    "dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(z_star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test) # no. of label=\"1\" in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(abs(z_star.flatten() - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "# Assuming G, h, preds_list, and y_test are already defined.\n",
    "y_hat = preds_list[0]  # Predicted values (1D array or list)\n",
    "newn = y_hat.shape[0]  # Number of variables (columns in G)\n",
    "\n",
    "# Convert y_hat to a NumPy array for consistency and then to a list\n",
    "if isinstance(y_hat, np.ndarray):\n",
    "    y_hat = y_hat.astype(float).tolist()  # Ensure all elements are floats\n",
    "\n",
    "# Create a Gurobi model\n",
    "model = gp.Model(\"robust_optimization\")\n",
    "\n",
    "# Create variables z with no bounds (default is unbounded)\n",
    "z = model.addVars(newn, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"z\")\n",
    "\n",
    "# Set the objective function: Minimize y_hat^T * z\n",
    "objective = gp.quicksum(y_hat[i] * z[i] for i in range(newn))  # y_hat[i] is numeric, z[i] is a Gurobi variable\n",
    "model.setObjective(objective, GRB.MINIMIZE)\n",
    "\n",
    "# Debug: Print shapes of G and h\n",
    "print(\"G shape:\", G.shape)\n",
    "print(\"z dimensions:\", len(z))\n",
    "print(\"h shape:\", h.shape if hasattr(h, 'shape') else len(h))\n",
    "\n",
    "# Ensure h is a 1D array with the correct length\n",
    "if isinstance(h, np.ndarray):\n",
    "    if len(h.shape) == 2 and h.shape[1] == 1:\n",
    "        h = h.flatten()  # Convert h from (n, 1) to (n,)\n",
    "    h = h.tolist()  # Convert h to a Python list for compatibility\n",
    "\n",
    "# Ensure h's length matches the number of rows in G\n",
    "if len(h) != G.shape[0]:\n",
    "    raise ValueError(f\"The length of h ({len(h)}) does not match the number of rows in G ({G.shape[0]}).\")\n",
    "\n",
    "# Add inequality constraints: Gz ≤ h\n",
    "for i in range(G.shape[0]):\n",
    "    model.addConstr(\n",
    "        gp.quicksum(G[i, j] * z[j] for j in range(G.shape[1])) <= h[i],\n",
    "        name=f\"ineq_constr_{i}\"\n",
    "    )\n",
    "\n",
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# Check if the model has a feasible solution\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Extract the optimal solution for z\n",
    "    z_star = np.array([z[i].X for i in range(newn)])  # Convert to a NumPy array\n",
    "    print(\"Optimal solution z_star:\", z_star)\n",
    "\n",
    "    # Optional: Compute and append result to TWOSTG_SOLQ_LIST\n",
    "    TWOSTG_SOLQ_LIST = []\n",
    "    TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))  # Dot product with y_test\n",
    "    print(\"TWOSTG_SOLQ_LIST:\", TWOSTG_SOLQ_LIST)\n",
    "else:\n",
    "    print(\"No optimal solution found. Status:\", model.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-w37BV6pS1E"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define Q as an identity matrix\n",
    "# Q = np.eye(n)\n",
    "\n",
    "TWOSTG_SOLQ_LIST = []\n",
    "\n",
    "y_hat = preds_list[0]\n",
    "\n",
    "# Number of variables : batch_size\n",
    "\n",
    "# Create a variable for z\n",
    "newn = y_hat.shape[0]\n",
    "print(newn)\n",
    "z = cp.Variable(newn)\n",
    "\n",
    "# Set the objective function\n",
    "# objective = (0.5 * cp.quad_form(z, Q)) + (y_hat.T @ z)\n",
    "objective = (y_hat.T @ z)\n",
    "\n",
    "# Add inequality constraints -1 <= z <= 1\n",
    "# o, p = np.identity(newn), -np.identity(newn)\n",
    "# G = np.concatenate((o, p), axis=0)\n",
    "# h = np.ones(newn)\n",
    "# h = h.reshape(-1,1)\n",
    "\n",
    "print(h.shape)\n",
    "constraints = [G @ z <= h]\n",
    "\n",
    "problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "problem.solve()\n",
    "\n",
    "z_star = z.value\n",
    "\n",
    "print(\"Optimal solution z_star:\", z_star)\n",
    "print(z_star.shape)\n",
    "\n",
    "TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZuwUTJSsYvo"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define Q as an identity matrix\n",
    "# Q = np.eye(n)\n",
    "\n",
    "TWOSTG_SOLQ_LIST = []\n",
    "\n",
    "batch_size = 100\n",
    "B = h.shape[0] // 100\n",
    "\n",
    "for j in range(B):\n",
    "\n",
    "  for i in range(len(preds_list)):\n",
    "    y_hat = preds_list[i]\n",
    "\n",
    "    # Number of variables : batch_size\n",
    "\n",
    "    # Create a variable for z\n",
    "    newn = y_hat.shape[0]\n",
    "    print(newn)\n",
    "    z = cp.Variable(newn)\n",
    "\n",
    "    # Set the objective function\n",
    "    # objective = (0.5 * cp.quad_form(z, Q)) + (y_hat.T @ z)\n",
    "    objective = (y_hat.T @ z)\n",
    "\n",
    "    # Add inequality constraints -1 <= z <= 1\n",
    "    o, p = np.identity(newn), -np.identity(newn)\n",
    "    G = np.concatenate((o, p), axis=0)\n",
    "    h = np.ones(newn)\n",
    "    h = h.reshape(-1,1)\n",
    "    print(h.shape)\n",
    "    constraints = [G @ z <= h]\n",
    "\n",
    "    problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    z_star = z.value\n",
    "\n",
    "    print(\"Optimal solution z_star:\", z_star)\n",
    "    print(z_star.shape)\n",
    "\n",
    "    TWOSTG_SOLQ_LIST.append(np.dot(z_star, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ZcgQDbwkvGx6",
    "outputId": "99828f11-856f-4700-f799-ff9eae810e00"
   },
   "outputs": [],
   "source": [
    "# # z_star = z_star.reshape(-1, 1)\n",
    "\n",
    "# # Decision quality\n",
    "np.dot(TWOSTG_SOLQ_LIST[5].reshape(-1,1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.gamma((12,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
