{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca522f8-054c-4a87-968a-c2660ae49d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 STG ADULT (UNUSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc436c-4c34-43a3-bafd-8642f2011128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, input_dim = X_train.shape[0], X_train.shape[1]\n",
    "# output_dim = 1\n",
    "\n",
    "# #logistic regression class\n",
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "#         self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "#         self.sigmoid = torch.nn.Sigmoid()\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "\n",
    "#     #sigmoid transformation of the input\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.linear2(x)\n",
    "# #         x = self.relu(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc0870-0e36-4d94-93d9-e991b5d05fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0642359-661b-49dd-b8e5-85f0b193ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# ### Needs a check again\n",
    "\n",
    "# def sample_l2lap(eta:float, d:int) -> np.array:\n",
    "#     \"\"\"\n",
    "#         Returns\n",
    "#           d dimensional noise sampled from `L2 laplace'\n",
    "#           https://math.stackexchange.com/questions/3801271/sampling-from-a-exponentiated-multivariate-distribution-with-l2-norm\n",
    "#     \"\"\"\n",
    "#     R = np.random.gamma(d, scale = 1.0/eta)\n",
    "#     Z = np.random.normal(0, 1, size = d)\n",
    "#     return R  * (Z / np.linalg.norm(Z)) #shape is (d,) one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f806c0-dbd4-4f8d-b38f-86e004de8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Define the linear regression model\n",
    "# # class LinearRegression(nn.Module):\n",
    "# #     def __init__(self, input_dim):\n",
    "# #         super(LinearRegression, self).__init__()\n",
    "# #         self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# def train_model(X_train, y_train, eps_p, epochs=1000):\n",
    "\n",
    "#     # Convert data to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#     # Initialize the model, loss function, and optimizer\n",
    "# #     model = LogisticRegression(input_dim, output_dim)\n",
    "#     criterion = nn.BCELoss(reduction='sum') # Binary Cross Entropy Loss for binary classification\n",
    "    \n",
    "#     d = X_train.shape[1]\n",
    "#     lr = 1e-4\n",
    "    \n",
    "    \n",
    "#     theta_init = torch.randn((d,1),requires_grad=True)\n",
    "#     optimizer = optim.Adam([theta_init], lr=lr)\n",
    "    \n",
    "#     eps_dash_p = eps_p - (2 * np.log(1 + (1 / Lamb)))\n",
    "\n",
    "#     if eps_dash_p > 0:\n",
    "#         Delta = 0\n",
    "#     else:\n",
    "#         Delta = (1 / (np.exp(eps_p / 4) - 1)) - Lamb\n",
    "#         eps_dash_p = eps_p / 2\n",
    "\n",
    "#     eta = eps_dash_p / ((4 * X_train.shape[0]) + (8 * math.sqrt(X_train.shape[0])))\n",
    "\n",
    "# #     theta_d = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     b = np.random.gamma(d, scale=1.0 / eta, size=(theta_init, 1))\n",
    "#     b = torch.Tensor(b.reshape(1, -1))\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in tqdm(range(epochs), desc='Training'):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "\n",
    "#         for batch_X, batch_y in data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "# #             outputs = model(batch_X)\n",
    "# #             theta = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "            \n",
    "#             y_hat_init = torch.matmul(batch_X, theta_init.float())\n",
    "#             outputs = torch.nn.Sigmoid(y_hat_init)\n",
    "            \n",
    "            \n",
    "\n",
    "#             # Add perturbation\n",
    "#             pert = torch.dot(b.flatten(), theta.flatten())\n",
    "\n",
    "#             # Calculate the loss\n",
    "#             loss = (\n",
    "#                 criterion(outputs, batch_y)\n",
    "#                 + pert\n",
    "#                 + ((Lamb + Delta) * (torch.norm(theta_init, p=2) ** 2))\n",
    "#             )\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#             # Backward and optimize\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Logging training loss\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4db204-9c5a-4a79-8105-a6024fdd7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial, X, y):\n",
    "#     # Define the hyperparameter search space\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "# #     eta = trial.suggest_float('eta', 0.1, 10.0, log=True)\n",
    "#     epochs = trial.suggest_int('epochs', 1, 1000)\n",
    "    \n",
    "#     # Split data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "    \n",
    "#     try:\n",
    "#         # Train the model with suggested hyperparameters\n",
    "#         model = train_model(\n",
    "#             X_train=X_train,\n",
    "#             y_train=y_train,\n",
    "#             eps_p=np.inf,\n",
    "#             epochs=epochs\n",
    "#         )\n",
    "        \n",
    "#         # Update the model's optimizer with suggested learning rate and weight decay\n",
    "#         model.optimizer = torch.optim.Adam(\n",
    "#             model.parameters(),\n",
    "#             lr=learning_rate\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         with torch.no_grad():\n",
    "#             X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "#             val_outputs = model(X_val_tensor)\n",
    "#             val_predictions = (val_outputs >= 0.5).float().numpy()\n",
    "#             accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "#         return accuracy\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         # Return a very low score if training fails\n",
    "#         print(f\"Trial failed with error: {str(e)}\")\n",
    "#         return float('-inf')\n",
    "\n",
    "# def optimize_hyperparameters(X, y, n_trials=100):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "#     # Create a partial function with fixed X and y\n",
    "#     objective_with_data = lambda trial: objective(trial, X, y)\n",
    "    \n",
    "#     # Run the optimization\n",
    "#     study.optimize(objective_with_data, n_trials=n_trials)\n",
    "    \n",
    "#     print(\"Best hyperparameters:\", study.best_params)\n",
    "#     print(\"Best accuracy:\", study.best_value)\n",
    "    \n",
    "#     return study.best_params, study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69dc52-1893-4bd9-b4b6-d4f249fbbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial, X, y):\n",
    "#     # Define the hyperparameter search space\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "# #     eta = trial.suggest_float('eta', 0.1, 10.0, log=True)\n",
    "#     epochs = trial.suggest_int('epochs', 1, 1000)\n",
    "    \n",
    "#     # Split data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "    \n",
    "#     try:\n",
    "#         # Train the model with suggested hyperparameters\n",
    "#         model = train_model(\n",
    "#             X_train=X_train,\n",
    "#             y_train=y_train,\n",
    "#             eps_p=np.inf,\n",
    "#             epochs=epochs\n",
    "#         )\n",
    "        \n",
    "#         # Update the model's optimizer with suggested learning rate and weight decay\n",
    "#         model.optimizer = torch.optim.Adam(\n",
    "#             model.parameters(),\n",
    "#             lr=learning_rate\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         with torch.no_grad():\n",
    "#             X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "#             val_outputs = model(X_val_tensor)\n",
    "#             val_predictions = (val_outputs >= 0.5).float().numpy()\n",
    "#             accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "#         return accuracy\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         # Return a very low score if training fails\n",
    "#         print(f\"Trial failed with error: {str(e)}\")\n",
    "#         return float('-inf')\n",
    "\n",
    "# def optimize_hyperparameters(X, y, n_trials=100):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "#     # Create a partial function with fixed X and y\n",
    "#     objective_with_data = lambda trial: objective(trial, X, y)\n",
    "    \n",
    "#     # Run the optimization\n",
    "#     study.optimize(objective_with_data, n_trials=n_trials)\n",
    "    \n",
    "#     print(\"Best hyperparameters:\", study.best_params)\n",
    "#     print(\"Best accuracy:\", study.best_value)\n",
    "    \n",
    "#     return study.best_params, study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7e94d-a830-4b33-ad1c-508c1b1e3d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9014e0-8678-4f1b-b4f9-ae6f325d6447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b424a-2d1e-42bb-9f32-4a6b59ad5b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd83b137-0689-43d6-bf75-2af72a49c374",
   "metadata": {},
   "source": [
    " # DFL (UNUSED CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3b211-7013-4cbd-ada4-a3429640138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def theta_closed_form(X_train_tensor, y_train_tensor, Lamb, b, c):\n",
    "    \n",
    "#     n, d = X_train_tensor.shape[0], X_train_tensor.shape[1]\n",
    "    \n",
    "#     Q = c * torch.eye(n)\n",
    "\n",
    "#     numerator = torch.matmul(X_train_tensor, torch.matmul(torch.transpose(torch.linalg.inv(Q), 0, 1), y_train_tensor)) \\\n",
    "#                 + b\n",
    "#     print(numerator.shape)\n",
    "    \n",
    "#     return - (numerator / (2 * Lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5dd14-2abe-4bc5-9783-b31da6086c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# def objective(z, Q, y_hat, gamma, G, h):\n",
    "    \n",
    "#     y_hat_np = y_hat.detach().numpy()\n",
    "    \n",
    "#     term1 = 0.5 * z.T @ Q @ z\n",
    "#     term2 = -(y_hat_np).T @ z\n",
    "    \n",
    "#     # print(type(G), type(h), type(gamma))\n",
    "#     # print(G.shape, z.shape, h.shape)\n",
    "\n",
    "#     term3 = gamma @ ((G @ z) - h.reshape(-1))\n",
    "#     # print(gamma.shape)\n",
    "\n",
    "\n",
    "#     # print(term1.shape)\n",
    "#     # print(term2.shape)\n",
    "#     # print(term3.shape)\n",
    "    \n",
    "#     return term1 + term2[0] + term3\n",
    "\n",
    "# def solve_opt(X_train_tensor, y_hat, gamma, G, h):\n",
    "    \n",
    "#     n, d = X_train_tensor.shape[0], X_train_tensor.shape[1]\n",
    "#     c = 1\n",
    "#     Q = c * np.eye(n)\n",
    "    \n",
    "#     initial_z = np.zeros(n)\n",
    "    \n",
    "#     result = minimize(\n",
    "#         lambda z: objective(z, Q, y_hat, gamma, G, h),\n",
    "#         initial_z,\n",
    "#         method='BFGS'\n",
    "#     )\n",
    "    \n",
    "#     return result.x, result.fun\n",
    "\n",
    "\n",
    "# def bin_search_gamma(X_train_tensor, y_hat, G, h, gamma_low=0, gamma_high=10**3, iters=10):\n",
    "\n",
    "#     k = G.shape[0]\n",
    "#     gamma_low_v, gamma_high_v = np.zeros(k), gamma_high * np.ones(k)\n",
    "\n",
    "#     opt_gamma, opt_z = None, None\n",
    "#     i = 0\n",
    "#     while i <= iters:\n",
    "\n",
    "#         gamma_mid = (gamma_low_v + gamma_high_v) / 2\n",
    "\n",
    "#         gamma_left, gamma_right = (gamma_low_v + gamma_mid) / 2, (gamma_mid + gamma_high_v) / 2\n",
    "\n",
    "#         left_opt_z, left_soln = solve_opt(X_train_tensor, y_hat, gamma_left, G, h)\n",
    "#         right_opt_z, right_soln = solve_opt(X_train_tensor, y_hat, gamma_right, G, h)\n",
    "\n",
    "#         if left_soln < right_soln:\n",
    "#             opt_gamma = gamma_left\n",
    "#             gamma_high_v = gamma_mid\n",
    "            \n",
    "#         elif left_soln > right_soln:\n",
    "#             opt_gamma = gamma_right\n",
    "#             gamma_low_v = gamma_mid\n",
    "#         else:\n",
    "#             return gamma_left\n",
    "            \n",
    "            \n",
    "#         i += 1\n",
    "\n",
    "#     return opt_gamma\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149f2bc-71c9-4853-8f4e-886f9bd6d7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde0286-1cfb-4734-8640-529b90eb0da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7b4dc-479e-4966-afb8-2ef92891a467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
